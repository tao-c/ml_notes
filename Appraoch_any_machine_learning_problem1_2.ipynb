{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Appraoch any machine learning problem1-2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NojrJxRc8brD",
        "XnVUze4k8l1l",
        "tSFjVIv2Qner",
        "2-J2fUsNQ07U",
        "iE-fqvTFZnhV",
        "t3TV1NJVZ_Zp",
        "j78I-1ggaZTb",
        "VmyDs4Qwci9O",
        "k_HLXh4UdNXi",
        "cNJfqh9MhPMp",
        "f3RQAguriw61",
        "2aHo-yRQkF5a",
        "W5AN13jVl_7e",
        "oo0UMj_nnfBE",
        "cRjTwFOOoeKB",
        "t_F3IwOUoxP1",
        "GSMhHRs4o7-2",
        "qJTI8niKO1bk",
        "6SpKNAZNRD19"
      ],
      "authorship_tag": "ABX9TyMZCzAtZ8Yp246irs4uK7L1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tao-c/ml_notes/blob/master/Appraoch_any_machine_learning_problem1_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UDElblG1xb6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XNBn_Zogg3K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oJG6lIa4EqX",
        "outputId": "258c16ec-c73a-452a-e6aa-e41699fb4fa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ks027zB4Rg_",
        "outputId": "56db3831-519c-4482-c405-28aac37362a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /gdrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0JRQomC4ZG2",
        "outputId": "d9384429-7f11-43e1-d140-6d4a35bebd4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/gdrive'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEKP4VbD4Zus",
        "outputId": "93d095c3-f938-4b51-8a25-da01e3d15f8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34m'My Drive'\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hFNHtOj4bAE",
        "outputId": "9efcf335-7f08-4a01-c20b-192748395e76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd 'My Drive'/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGD4lKDv4eKA",
        "outputId": "ced67b9a-2722-46bd-cc5e-49db73080536",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd notes/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive/notes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10DT4mzS4gff",
        "outputId": "2c8325b1-b03d-45d0-d843-efebe4f60c0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "!git clone https://github.com/tao-c/scikit-learn/tree/master/doc/tutorial/text_analytics"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'text_analytics'...\n",
            "fatal: repository 'https://github.com/tao-c/scikit-learn/tree/master/doc/tutorial/text_analytics/' not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4UpI_pZ5BnA"
      },
      "source": [
        "categories = ['alt.atheism', 'soc.religion.christian',\n",
        "...               'comp.graphics', 'sci.med']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiKqmH6M5qmP",
        "outputId": "f039ccb1-73cb-47d4-bb30-ea724f607c5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "twenty_train = fetch_20newsgroups(subset='train',categories=categories, shuffle=True, random_state=42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vpE86yj5tsk",
        "outputId": "824fe765-3c6c-41f0-df25-65cf251c3b29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " ch02.ipynb\n",
            "'NLP in action.gdoc'\n",
            " nlp.ipynb\n",
            "'PEP 274 -- Dict Comprehensions.gdoc'\n",
            " statistics_notes.gdoc\n",
            "'Text Preprocessing in Python: Steps, Tools, and Examples.ipynb'\n",
            "'Untitled document.gdoc'\n",
            " 房东互助：从雷丁恐怖袭击，分析政府房及其区域的危害.gdoc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GMdzHYy6VTe"
      },
      "source": [
        "#The returned dataset is a scikit-learn “bunch”: a simple holder object with fields that can be both accessed as python dict keys or object attributes for convenience, for instance the target_names holds the list of the requested category names:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlgJ1wxR5zgW",
        "outputId": "6249db79-24de-4f0b-d84d-b92f5c4c3ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "twenty_train.target_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_irFX4dk6kad"
      },
      "source": [
        "#The files themselves are loaded in memory in the data attribute. For reference the filenames are also available:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lC8KveDK56d3",
        "outputId": "ffb2c1ad-697d-43cc-fc62-f3172d2f36b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(twenty_train.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2257"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpjBFDnH588i",
        "outputId": "f7720113-340c-4b2e-f6b2-951bcb77735d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(twenty_train.filenames)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2257"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnECL7PI6nBY"
      },
      "source": [
        "#Let’s print the first lines of the first loaded file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6DeFipD6BF6",
        "outputId": "f938c289-0b5f-4339-cc04-0e1db169d847",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From: sd345@city.ac.uk (Michael Collier)\n",
            "Subject: Converting images to HP LaserJet III?\n",
            "Nntp-Posting-Host: hampton\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujq2J2fR6qsL",
        "outputId": "234a2868-57dc-4648-f424-e51d9e51b622",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "print(\"\\n\".join(twenty_train.data[2].split(\"\\n\")[:10]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From: djohnson@cs.ucsd.edu (Darin Johnson)\n",
            "Subject: Re: harrassed at work, could use some prayers\n",
            "Organization: =CSE Dept., U.C. San Diego\n",
            "Lines: 63\n",
            "\n",
            "(Well, I'll email also, but this may apply to other people, so\n",
            "I'll post also.)\n",
            "\n",
            ">I've been working at this company for eight years in various\n",
            ">engineering jobs.  I'm female.  Yesterday I counted and realized that\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6DrMYrO6u6h",
        "outputId": "a3b807c3-0077-4ff0-c87c-4f22af408178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(twenty_train.target_names[twenty_train.target[0]]),print(twenty_train.target_names[twenty_train.target[2]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "comp.graphics\n",
            "soc.religion.christian\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMT-UbDx64_M",
        "outputId": "45143317-af8e-4132-d0f0-bb0167e28ee2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(twenty_train.target_names[twenty_train.target[2]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "soc.religion.christian\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSLPG4587hFS"
      },
      "source": [
        "#Supervised learning algorithms will require a category label for each document in the training set. In this case the category is the name of the newsgroup which also happens to be the name of the folder holding the individual documents.\n",
        "\n",
        "#For speed and space efficiency reasons scikit-learn loads the target attribute as an array of integers that corresponds to the index of the category name in the target_names list. The category integer id of each sample is stored in the target attribute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jf53h8eJ7AG1",
        "outputId": "a038b22d-f618-43a1-87b7-11225d6a4baf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "twenty_train.target[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 3, 3, 3, 3, 3, 2, 2, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcUqb4SB7pUH"
      },
      "source": [
        "#It is possible to get back the category names as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3nJTI-W7kFx",
        "outputId": "5f481a9f-0d4d-4bb9-cab1-7d3d39b26b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "for t in twenty_train.target[:10]:\n",
        "...     print(twenty_train.target_names[t])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "comp.graphics\n",
            "comp.graphics\n",
            "soc.religion.christian\n",
            "soc.religion.christian\n",
            "soc.religion.christian\n",
            "soc.religion.christian\n",
            "soc.religion.christian\n",
            "sci.med\n",
            "sci.med\n",
            "sci.med\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF8zMjyn7rkF",
        "outputId": "ddd83b58-52f2-45d4-925e-1e60ec863cb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(twenty_train.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXhaQHsg8LmB",
        "outputId": "bd3d0c44-7d82-45ac-e8e6-e7c5bae47360",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
        "X_train_counts.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2257, 35788)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NojrJxRc8brD"
      },
      "source": [
        "#CountVectorizer supports counts of N-grams of words or consecutive characters. Once fitted, the vectorizer has built a dictionary of feature indices:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtRRUKOo8USA",
        "outputId": "1367ce04-0833-4b3e-ba9b-2c4e6006a198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "count_vect.vocabulary_.get(u'algorithm')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4690"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06pheXGu-gxq",
        "outputId": "a7ff4e12-bd20-454b-9b04-2381bf44da35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "count_vect.vocabulary_['algorithm']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4690"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnVUze4k8l1l"
      },
      "source": [
        "#The index value of a word in the vocabulary is linked to its frequency in the whole training corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlTXJkRL8e3v",
        "outputId": "7264ebe9-7b43-4dd0-c029-b296e03951b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(count_vect.vocabulary_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35788"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2zypF1C9ccj",
        "outputId": "a923aa0a-0c03-4fe9-a875-3b28eaa2cf12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for word, frequency in count_vect.vocabulary_.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
        "    if frequency == 1:\n",
        "        print(word, frequency)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "000 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9BI8D3i8qtG",
        "outputId": "5665392a-6e48-4de1-ce5f-79eb48704a52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "for i in range(10):\n",
        "  print(f\"=========== Frequency {i} =============\")\n",
        "  for word, frequency in count_vect.vocabulary_.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
        "    if frequency == 1:\n",
        "        print(word, frequency)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========== Frequency 0 =============\n",
            "000 1\n",
            "=========== Frequency 1 =============\n",
            "000 1\n",
            "=========== Frequency 2 =============\n",
            "000 1\n",
            "=========== Frequency 3 =============\n",
            "000 1\n",
            "=========== Frequency 4 =============\n",
            "000 1\n",
            "=========== Frequency 5 =============\n",
            "000 1\n",
            "=========== Frequency 6 =============\n",
            "000 1\n",
            "=========== Frequency 7 =============\n",
            "000 1\n",
            "=========== Frequency 8 =============\n",
            "000 1\n",
            "=========== Frequency 9 =============\n",
            "000 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0MwT4bt826f",
        "outputId": "d8d827b7-a03e-47fc-fe69-16e932ddf8e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "for i in range(10):\n",
        "  print(f\"=========== Frequency {i} =============\")\n",
        "  for word, frequency in count_vect.vocabulary_.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
        "    if frequency == i:\n",
        "        print(word, frequency)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========== Frequency 0 =============\n",
            "00 0\n",
            "=========== Frequency 1 =============\n",
            "000 1\n",
            "=========== Frequency 2 =============\n",
            "0000 2\n",
            "=========== Frequency 3 =============\n",
            "0000001200 3\n",
            "=========== Frequency 4 =============\n",
            "000005102000 4\n",
            "=========== Frequency 5 =============\n",
            "0001 5\n",
            "=========== Frequency 6 =============\n",
            "000100255pixel 6\n",
            "=========== Frequency 7 =============\n",
            "00014 7\n",
            "=========== Frequency 8 =============\n",
            "000406 8\n",
            "=========== Frequency 9 =============\n",
            "0007 9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCRaCjsx-TCC",
        "outputId": "7181a189-e3ff-446a-8837-d113b6ffb7fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "for i in range(10):\n",
        "  print(f\"=========== Frequency {i+50} =============\")\n",
        "  for word, frequency in count_vect.vocabulary_.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
        "    if frequency == i+50:\n",
        "        print(word, frequency)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========== Frequency 50 =============\n",
            "0131 50\n",
            "=========== Frequency 51 =============\n",
            "013423tan102 51\n",
            "=========== Frequency 52 =============\n",
            "013657 52\n",
            "=========== Frequency 53 =============\n",
            "0138 53\n",
            "=========== Frequency 54 =============\n",
            "013846 54\n",
            "=========== Frequency 55 =============\n",
            "0150 55\n",
            "=========== Frequency 56 =============\n",
            "015518 56\n",
            "=========== Frequency 57 =============\n",
            "01580 57\n",
            "=========== Frequency 58 =============\n",
            "015931 58\n",
            "=========== Frequency 59 =============\n",
            "01720 59\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSFLMPpB-yD6",
        "outputId": "fc96ee3b-c700-4805-fc4e-11440db88468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "for i in range(10):\n",
        "  print(f\"=========== Frequency {i+4690} =============\")\n",
        "  for word, frequency in count_vect.vocabulary_.items():  # for name, age in dictionary.iteritems():  (for Python 2.x)\n",
        "    if frequency == i+4690:\n",
        "        print(word, frequency)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=========== Frequency 4690 =============\n",
            "algorithm 4690\n",
            "=========== Frequency 4691 =============\n",
            "algorithmic 4691\n",
            "=========== Frequency 4692 =============\n",
            "algorithmically 4692\n",
            "=========== Frequency 4693 =============\n",
            "algorithms 4693\n",
            "=========== Frequency 4694 =============\n",
            "algoritmic 4694\n",
            "=========== Frequency 4695 =============\n",
            "ali 4695\n",
            "=========== Frequency 4696 =============\n",
            "alia 4696\n",
            "=========== Frequency 4697 =============\n",
            "alias 4697\n",
            "=========== Frequency 4698 =============\n",
            "aliased 4698\n",
            "=========== Frequency 4699 =============\n",
            "aliases 4699\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLoCag53_AXj",
        "outputId": "699cd761-79de-4ee7-d480-31b6d44d2a88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(X_train_counts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scipy.sparse.csr.csr_matrix"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggaE0s8XA6MG",
        "outputId": "a84a9e44-dd34-412e-9355-b646a01f9023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X_train_counts[1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x35788 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 102 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-tNtPdWA8Vg",
        "outputId": "b51bdbb5-4f00-4822-f5c2-dc9c507b5dca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X_train_counts.toarray"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method _cs_matrix.toarray of <2257x35788 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 365886 stored elements in Compressed Sparse Row format>>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vWCjNKTA_xx",
        "outputId": "be897158-7432-45e0-b337-66547d1d77c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "X_train_counts.toarray()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om600NITBDx9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwmVSMUsBK4i",
        "outputId": "7d3c3605-3410-4af3-ca7c-54d388ce45ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X_train_tfidf.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2257, 35788)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h_t4QMgDGSp",
        "outputId": "526cd619-f95e-4adc-fa48-129e286fa395",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "X_train_tfidf.toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hBw4WkUDX4S"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqlNnctTDgGn"
      },
      "source": [
        "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
        "X_new_counts = count_vect.transform(docs_new)\n",
        "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
        "\n",
        "predicted = clf.predict(X_new_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLgJcCiKDnci",
        "outputId": "d116e38d-a491-4880-ee09-cffdb57041e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for doc, category in zip(docs_new, predicted):\n",
        "...     print('%r => %s' % (doc, twenty_train.target_names[category]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'God is love' => soc.religion.christian\n",
            "'OpenGL on the GPU is fast' => comp.graphics\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvW4Vwc7DsCX"
      },
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "text_clf = Pipeline([\n",
        "     ('vect', CountVectorizer()),\n",
        "     ('tfidf', TfidfTransformer()),\n",
        "     ('clf', MultinomialNB()),\n",
        " ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pugzBaE0D4OG",
        "outputId": "476ab37e-66f3-4669-8710-ae150253ef5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "text_clf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('tfidf',\n",
              "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
              "                                  sublinear_tf=False, use_idf=True)),\n",
              "                ('clf',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3R20jVPEAVt",
        "outputId": "9d18b6ef-e781-4281-bfff-b7065a245085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "text_clf.fit(twenty_train.data, twenty_train.target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=None)),\n",
              "                ('tfidf',\n",
              "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
              "                                  sublinear_tf=False, use_idf=True)),\n",
              "                ('clf',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejn1AVaREHJD",
        "outputId": "ba3a691d-c3a3-4c14-f379-0a439b523e2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "twenty_test = fetch_20newsgroups(subset='test',\n",
        "     categories=categories, shuffle=True, random_state=42)\n",
        "docs_test = twenty_test.data\n",
        "predicted = text_clf.predict(docs_test)\n",
        "np.mean(predicted == twenty_test.target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8348868175765646"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6jesyo8EQaM",
        "outputId": "0529bd20-e955-4731-b90f-2a741ab02a1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "text_clf = Pipeline([\n",
        "     ('vect', CountVectorizer()),\n",
        "     ('tfidf', TfidfTransformer()),\n",
        "     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
        "                           alpha=1e-3, random_state=42,\n",
        "                           max_iter=5, tol=None)),\n",
        " ])\n",
        "\n",
        "text_clf.fit(twenty_train.data, twenty_train.target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vect',\n",
              "                 CountVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
              "                                 input='content', lowercase=True, max_df=1.0,\n",
              "                                 max_features=None, min_df=1,\n",
              "                                 ngram_range=(1, 1), preprocessor=None,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, vocabulary=Non...\n",
              "                ('clf',\n",
              "                 SGDClassifier(alpha=0.001, average=False, class_weight=None,\n",
              "                               early_stopping=False, epsilon=0.1, eta0=0.0,\n",
              "                               fit_intercept=True, l1_ratio=0.15,\n",
              "                               learning_rate='optimal', loss='hinge',\n",
              "                               max_iter=5, n_iter_no_change=5, n_jobs=None,\n",
              "                               penalty='l2', power_t=0.5, random_state=42,\n",
              "                               shuffle=True, tol=None, validation_fraction=0.1,\n",
              "                               verbose=0, warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXzqEF6EEc8I",
        "outputId": "885b29e1-1be8-43f6-9424-fce43eae380a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "predicted = text_clf.predict(docs_test)\n",
        "np.mean(predicted == twenty_test.target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9101198402130493"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0cLNfP5EjS4",
        "outputId": "6faa110d-e135-4800-d48c-cb18d9e20d90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(twenty_test.target, predicted,\n",
        "    target_names=twenty_test.target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                        precision    recall  f1-score   support\n",
            "\n",
            "           alt.atheism       0.95      0.80      0.87       319\n",
            "         comp.graphics       0.87      0.98      0.92       389\n",
            "               sci.med       0.94      0.89      0.91       396\n",
            "soc.religion.christian       0.90      0.95      0.93       398\n",
            "\n",
            "              accuracy                           0.91      1502\n",
            "             macro avg       0.91      0.91      0.91      1502\n",
            "          weighted avg       0.91      0.91      0.91      1502\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSFjVIv2Qner"
      },
      "source": [
        "#There is a red winequality dataset2 which is quite famous. This dataset has 11 different attributes that decide the quality of red wine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKF9cWWDEmzd"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"winequality-red.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn15xhUzQJxt",
        "outputId": "aa84e027-b77a-4942-8f58-c6bb34b2cfae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.880</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.098</td>\n",
              "      <td>25.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>0.99680</td>\n",
              "      <td>3.20</td>\n",
              "      <td>0.68</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.8</td>\n",
              "      <td>0.760</td>\n",
              "      <td>0.04</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.092</td>\n",
              "      <td>15.0</td>\n",
              "      <td>54.0</td>\n",
              "      <td>0.99700</td>\n",
              "      <td>3.26</td>\n",
              "      <td>0.65</td>\n",
              "      <td>9.8</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.2</td>\n",
              "      <td>0.280</td>\n",
              "      <td>0.56</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.075</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.99800</td>\n",
              "      <td>3.16</td>\n",
              "      <td>0.58</td>\n",
              "      <td>9.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.9</td>\n",
              "      <td>0.076</td>\n",
              "      <td>11.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99780</td>\n",
              "      <td>3.51</td>\n",
              "      <td>0.56</td>\n",
              "      <td>9.4</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1594</th>\n",
              "      <td>6.2</td>\n",
              "      <td>0.600</td>\n",
              "      <td>0.08</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.090</td>\n",
              "      <td>32.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.99490</td>\n",
              "      <td>3.45</td>\n",
              "      <td>0.58</td>\n",
              "      <td>10.5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1595</th>\n",
              "      <td>5.9</td>\n",
              "      <td>0.550</td>\n",
              "      <td>0.10</td>\n",
              "      <td>2.2</td>\n",
              "      <td>0.062</td>\n",
              "      <td>39.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.99512</td>\n",
              "      <td>3.52</td>\n",
              "      <td>0.76</td>\n",
              "      <td>11.2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1596</th>\n",
              "      <td>6.3</td>\n",
              "      <td>0.510</td>\n",
              "      <td>0.13</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.076</td>\n",
              "      <td>29.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.99574</td>\n",
              "      <td>3.42</td>\n",
              "      <td>0.75</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1597</th>\n",
              "      <td>5.9</td>\n",
              "      <td>0.645</td>\n",
              "      <td>0.12</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.075</td>\n",
              "      <td>32.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.99547</td>\n",
              "      <td>3.57</td>\n",
              "      <td>0.71</td>\n",
              "      <td>10.2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1598</th>\n",
              "      <td>6.0</td>\n",
              "      <td>0.310</td>\n",
              "      <td>0.47</td>\n",
              "      <td>3.6</td>\n",
              "      <td>0.067</td>\n",
              "      <td>18.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>0.99549</td>\n",
              "      <td>3.39</td>\n",
              "      <td>0.66</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1599 rows × 12 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n",
              "0               7.4             0.700         0.00  ...       0.56      9.4        5\n",
              "1               7.8             0.880         0.00  ...       0.68      9.8        5\n",
              "2               7.8             0.760         0.04  ...       0.65      9.8        5\n",
              "3              11.2             0.280         0.56  ...       0.58      9.8        6\n",
              "4               7.4             0.700         0.00  ...       0.56      9.4        5\n",
              "...             ...               ...          ...  ...        ...      ...      ...\n",
              "1594            6.2             0.600         0.08  ...       0.58     10.5        5\n",
              "1595            5.9             0.550         0.10  ...       0.76     11.2        6\n",
              "1596            6.3             0.510         0.13  ...       0.75     11.0        6\n",
              "1597            5.9             0.645         0.12  ...       0.71     10.2        5\n",
              "1598            6.0             0.310         0.47  ...       0.66     11.0        6\n",
              "\n",
              "[1599 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-J2fUsNQ07U"
      },
      "source": [
        "#We can treat this problem either as a classification problem or as a regression problem since wine quality is nothing but a real number between 0 and 10. For simplicity, let’s choose classification. This dataset, however, consists of only six types of quality values. We will thus map all quality values from 0 to 5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0uxY6LoQtjM",
        "outputId": "28a58f85-b0eb-4dcc-9b07-bcac262740e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "df.quality.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5    681\n",
              "6    638\n",
              "7    199\n",
              "4     53\n",
              "8     18\n",
              "3     10\n",
              "Name: quality, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJl_B3htQ6QU"
      },
      "source": [
        "# a mapping dictionary that maps the quality values from 0 to 5\n",
        "quality_mapping = {\n",
        "3: 0,\n",
        "4: 1,\n",
        "5: 2,\n",
        "6: 3,\n",
        "7: 4,\n",
        "8: 5\n",
        "}\n",
        "# you can use the map function of pandas with\n",
        "# any dictionary to convert the values in a given\n",
        "# column to values in the dictionary\n",
        "df.loc[:, \"quality\"] = df.quality.map(quality_mapping)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hACV0P4yRPm3",
        "outputId": "ab1d46cd-ca67-4277-8681-86722922c432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "df.quality.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    681\n",
              "3    638\n",
              "4    199\n",
              "1     53\n",
              "5     18\n",
              "0     10\n",
              "Name: quality, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE-fqvTFZnhV"
      },
      "source": [
        "#Before we begin to understand what overfitting is, let’s divide the data into two parts. This dataset has 1599 samples. We keep 1000 samples for training and 599 as a separate set.\n",
        "#Splitting can be done easily by the following chunk of code:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCcFxCqRRTBW"
      },
      "source": [
        "# use sample with frac=1 to shuffle the dataframe\n",
        "# we reset the indices since they change after\n",
        "# shuffling the dataframe\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "# top 1000 rows are selected\n",
        "# for training\n",
        "df_train = df.head(1000)\n",
        "# bottom 599 values are selected\n",
        "# for testing/validation\n",
        "df_test = df.tail(599)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ubXVsBwZ0hj",
        "outputId": "48fdd075-486f-45f2-e5b0-d13a3d694d87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.1</td>\n",
              "      <td>0.470</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.20</td>\n",
              "      <td>0.067</td>\n",
              "      <td>7.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.99517</td>\n",
              "      <td>3.40</td>\n",
              "      <td>0.58</td>\n",
              "      <td>10.9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.7</td>\n",
              "      <td>0.580</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.088</td>\n",
              "      <td>12.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.99568</td>\n",
              "      <td>3.32</td>\n",
              "      <td>0.56</td>\n",
              "      <td>10.5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.3</td>\n",
              "      <td>0.490</td>\n",
              "      <td>0.36</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.222</td>\n",
              "      <td>6.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.99800</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.60</td>\n",
              "      <td>9.5</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.1</td>\n",
              "      <td>0.590</td>\n",
              "      <td>0.01</td>\n",
              "      <td>2.30</td>\n",
              "      <td>0.080</td>\n",
              "      <td>27.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>0.99550</td>\n",
              "      <td>3.42</td>\n",
              "      <td>0.58</td>\n",
              "      <td>10.7</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "      <td>1.185</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.25</td>\n",
              "      <td>0.097</td>\n",
              "      <td>5.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.99660</td>\n",
              "      <td>3.63</td>\n",
              "      <td>0.54</td>\n",
              "      <td>10.7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n",
              "0            7.1             0.470         0.00  ...       0.58     10.9        1\n",
              "1            7.7             0.580         0.01  ...       0.56     10.5        4\n",
              "2            8.3             0.490         0.36  ...       0.60      9.5        3\n",
              "3            7.1             0.590         0.01  ...       0.58     10.7        3\n",
              "4            7.4             1.185         0.00  ...       0.54     10.7        0\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3TV1NJVZ_Zp"
      },
      "source": [
        "#We will now train a decision tree model on the training set. For the decision tree model, I am going to use scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4q3yhSxXZ3OQ",
        "outputId": "515e3b4a-8621-4db7-ee19-80ed5c8c8b3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# import from scikit-learn\n",
        "from sklearn import tree\n",
        "from sklearn import metrics\n",
        "# initialize decision tree classifier class\n",
        "# with a max_depth of 3\n",
        "clf = tree.DecisionTreeClassifier(max_depth=3)\n",
        "# choose the columns you want to train on\n",
        "# these are the features for the model\n",
        "cols = ['fixed acidity',\n",
        "'volatile acidity',\n",
        "'citric acid','residual sugar',\n",
        "'chlorides',\n",
        "'free sulfur dioxide',\n",
        "'total sulfur dioxide',\n",
        "'density',\n",
        "'pH',\n",
        "'sulphates',\n",
        "'alcohol']\n",
        "# train the model on the provided features\n",
        "# and mapped quality from before\n",
        "clf.fit(df_train[cols], df_train.quality)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=3, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j78I-1ggaZTb"
      },
      "source": [
        "#Note that I have used a max_depth of 3 for the decision tree classifier. I have left all other parameters of this model to its default value.\n",
        "#Now, we test the accuracy of this model on the training set and the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1ph2RuoaTVO"
      },
      "source": [
        "# generate predictions on the training set\n",
        "train_predictions = clf.predict(df_train[cols])\n",
        "# generate predictions on the test set\n",
        "test_predictions = clf.predict(df_test[cols])\n",
        "# calculate the accuracy of predictions on\n",
        "# training data set\n",
        "train_accuracy = metrics.accuracy_score(\n",
        "df_train.quality, train_predictions\n",
        ")\n",
        "# calculate the accuracy of predictions on\n",
        "# test data set\n",
        "test_accuracy = metrics.accuracy_score(\n",
        "df_test.quality, test_predictions\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tbf_fGVIalVR",
        "outputId": "d194625f-a158-4e88-fc84-2821ec20e82a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_accuracy,test_accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.592, 0.5425709515859767)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCYMpGpba6lW"
      },
      "source": [
        "#The training and test accuracies are found to be 58.9% and 54.25%. Now we increase the max_depth to 7 and repeat the process. This gives training accuracy of 76.6% and test accuracy of 57.3%. \n",
        "#Here, we have used accuracy, mainly because it is the most straightforward metric. It might not be the best metric for this problem.\n",
        "#What about we calculate these accuracies for different values of max_depth and make a plot?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AXxiQviaveV",
        "outputId": "20a30451-b9be-4498-b96a-6c5c9299fe58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "# NOTE: this code is written in a jupyter notebook\n",
        "# import scikit-learn tree and metrics\n",
        "from sklearn import tree\n",
        "from sklearn import metrics\n",
        "# import matplotlib and seaborn\n",
        "# for plotting\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# this is our global size of label text\n",
        "# on the plots\n",
        "matplotlib.rc('xtick', labelsize=20)\n",
        "matplotlib.rc('ytick', labelsize=20)\n",
        "# This line ensures that the plot is displayed\n",
        "# inside the notebook\n",
        "%matplotlib inline\n",
        "# initialize lists to store accuracies\n",
        "# for training and test data\n",
        "# we start with 50% accuracy\n",
        "train_accuracies = [0.5]\n",
        "test_accuracies = [0.5]\n",
        "# iterate over a few depth values\n",
        "for depth in range(1, 25):\n",
        "# init the model\n",
        "  clf = tree.DecisionTreeClassifier(max_depth=depth)\n",
        "  # columns/features for training\n",
        "  # note that, this can be done outside\n",
        "  # the loop\n",
        "  cols = [\n",
        "  'fixed acidity',\n",
        "  'volatile acidity',\n",
        "  'citric acid',\n",
        "  'residual sugar',\n",
        "  'chlorides',\n",
        "  'free sulfur dioxide',\n",
        "  'total sulfur dioxide',\n",
        "  'density',\n",
        "  'pH',\n",
        "  'sulphates','alcohol'\n",
        "  ]\n",
        "  # fit the model on given features\n",
        "  clf.fit(df_train[cols], df_train.quality)\n",
        "  # create training & test predictions\n",
        "  train_predictions = clf.predict(df_train[cols])\n",
        "  test_predictions = clf.predict(df_test[cols])\n",
        "  # calculate training & test accuracies\n",
        "  train_accuracy = metrics.accuracy_score(\n",
        "  df_train.quality, train_predictions\n",
        "  )\n",
        "  test_accuracy = metrics.accuracy_score(\n",
        "  df_test.quality, test_predictions\n",
        "  )\n",
        "  # append accuracies\n",
        "  train_accuracies.append(train_accuracy)\n",
        "  test_accuracies.append(test_accuracy)\n",
        "# create two plots using matplotlib\n",
        "# and seaborn\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.plot(train_accuracies, label=\"train accuracy\")\n",
        "plt.plot(test_accuracies, label=\"test accuracy\")\n",
        "plt.legend(loc=\"upper left\", prop={'size': 15})\n",
        "plt.xticks(range(0, 26, 5))\n",
        "plt.xlabel(\"max_depth\", size=20)\n",
        "plt.ylabel(\"accuracy\", size=20)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAoIAAAFQCAYAAAAr7h+sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3hUVf7H8XcSUggJhJbQOxyQ3psdLLtYWMWGomsBlGLDguwqgqvuT9YGisvaUGmiC+zaEMGuCwihSjj0FloEEghJJm1+f9xJSCKBTAozST6v55nnTu69c+bL3JB8cu495wa43W5EREREpPIJ9HUBIiIiIuIbCoIiIiIilZSCoIiIiEglpSAoIiIiUkkpCIqIiIhUUgqCIiIiIpVUFV8X4I/Wrl3rDg0NLdP3cLlclPV7SMnpOJUPOk7+T8eofNBxKh8KHqeUlJTfunfvXrc4bSkInkZoaCjt2rUr0/eIi4sr8/eQktNxKh90nPyfjlH5oONUPhQ8TqtXr95d3LZ0alhERESkklIQFBEREamkFARFREREKikFQREREZFKyueDRYwxQ4CLgC5AZyASmG2tva0YbTUCJgNXArWBA8AiYJK19lipFS0iIiJSAfhDj+BfgTE4QTC+uI0YY1oCq4E7gZXAy8AO4AHgf8aY2iUvVURERKTi8HmPIPAQsA/YhtMz+E0x25kORAP3W2un5aw0xrzkeY9ngXtLVuopx48f5/Dhw2RkZBTr9RkZGcTFxZVWOVJG/O04BQcHEx0dTfXq1X1dioiIVAA+D4LW2tzgZ4wpVhue3sDLgV3A6wU2TwRGAMOMMeOstSeLV+kpx48f59ChQzRs2JCqVasSEBDgdRupqalUrVq1pKVIGfOn4+R2u0lNTSU+3uk4VxgUEZGS8odTw6XhEs9yibU2O+8Ga+0J4CcgHOhTGm92+PBhGjZsSHh4eLFCoEhxBAQEEB4eTsOGDTl8+LCvyxERkQrA5z2CpSSnK3FLIdu34vQYtgGWlfTNMjIy/KaXSCqfqlWrFvuSBBEpfzKzsklKzSAxNYPElAySUtNJSs0gLSP77C/20oEDx1l7Yk+pt1tZBQcF8ocO9agW6r9xy38r804NzzKpkO0566OK0pjL5TrjdWEZGRmkpaUVvbrTyDnNJ/7NX4+Tv1276GtpaWn6PPycjhGkZ7lJdmVxwpXNiXTPMt/zLJLTs3Ofn0h3likZ7nNc6W/n+P0qthNHDtKncbVSbbM0/z9VlCBYqs52r+G4uLgS9wj607VnUjh/PU7BwcG6H2geuj+q/ytvxygr201KeiYp6VmcdBVYpmeS4vIs86xPSc/kZHoWKS7PMs9+x1MzSc3IKvT9ggIDiKoaTI3wYKKqhtKkRkier0OICg8mKjyYGlWDiQp3toUFB5X6v3vr1q20bt261NutrKoEBVAnIrTU2z3NvYaL3VZFCYI5PX41Ctmesz7xHNRSLnz++eekpaVx3XXXlVqbK1as4Pbbb+eTTz6hTZs2pdauiEhZOHYyna2Hk9l6+ARbDyWz7XAy2xOSOZaS7tVp16DAAKqFBFEttArheZYxkWGE16lCeHAQNTwhzglyp8JdztcRoVX84przY9WqUK9GmK/LkHOoogRB61kWlj5y/rwp7BrCSmfx4sUcO3asVINg+/bt+fDDD2nSpEmptSkiUhJut5uEZBfbDiXnC33bE5L5LTk9d7/wkCBaR0fQt0Vt6kSGOoEupArhoZ7laYJezvaQoEC/CHEixVFRgmDOFDSXG2MC844cNsZEAv2BFGC5L4orzzIyMggMDCQo6OynICIiIujSpcs5qMr3XC4XoaGl390vIsXjdrs5kJTGtsNO4NvmCXxbDyeTlHpqcFVkWBVaR0cwoG0MrWMiaBUdQeuYSOpXDyMwUGFOKp9yFQSNMcFASyDDWrs9Z721drsxZgnOyODRwLQ8L5sEVANmlMYcghXB+PHj+fLLL4FTczeOGTOGsWPHMmzYMGrWrEn//v158803iY+P5+uvvyYlJYXXXnuN2NhYEhMTadSoETfccAO33347gYHOLESnOzVsjGHChAkcOXKE+fPnExAQwJVXXskTTzxBSEhIoTWuWbOGGTNmsHHjRpKTk2natCl3330311xzTb794uPjefHFF/npp59IS0ujSZMmjBgxgquvvhpwLqidOnUqX3zxBQkJCURHRzNo0CDGjRuXW9+TTz7JbbeduqPhtGnTmDVrFitWrABgwYIFPPHEE3z00UdMmTKFdevWMXLkSEaPHs0//vEPvvvuO/bt20dkZCQ9e/Zk/Pjx1K1bN1+d8+fP54MPPmDXrl1ERkbSo0cPnn32WWJjYxk5ciRfffUVjRs3zt1/7969XHbZZbz22msMHDjQ+4MsUo653W5SM7I46coiNeeavPRMTrqy8i2Pp2Wy87eTbD2czPbDySS7MnPbqFUthFbREVzVqT6tPWGvdXQEdSND1XsnkofPg6AxZjAw2PNlPc+yrzFmpuf5b9baRzzPGwJxwG6gWYGmRgE/A1ONMQM8+/XGmWNwC/CXsqi/PBo1ahT79+/nxIkTTJw4EYB69erlbo+NjWXPnj088sgjVK1alcjISHbt2kXz5s25+uqrqVatGnFxcUybNg2Xy8XIkSPP+H7vvvsuffr0YcqUKVhreemll2jQoAHDhw8v9DX79++nW7du3HLLLYSEhBAbG8uECRMIDAzkqquuAuDIkSPcdNNNVK1alccff5z69euzZcsWDhw4ADi/TEaNGsWaNWsYNWoUHTp04NChQ6xatapYn9vDDz/M0KFDGT16dO5kzkeOHGHkyJFER0dz9OhR3n33Xe644w4+/fTT3IA8ffp0pk6dytChQ3n00UdJS0vj22+/JSUlhfPPP5/o6GgWLVrE2LFjc99r4cKF1K5dm4svvrhYtYr4UnpmNkdOukg4ceoRt/MY4bs35xtIUTDY5Qy0SMnIwl3EgbLRkaG0jolgSPdGTu9etNPLV7sMLtAXqYh8HgRx7jF8R4F1LTwPcELfI5yFp1ewBzAZuBL4I3AAeBWYZK09VmoVn8a/V+9j/qq9Rd4/Ozs7NyiU1I09GnN990ZF3r9JkyZERUXhdrtPeyr3+PHjLFq0iDp16uSu69u3L3379gWcgNW9e3fS0tKYP3/+WYNgw4YN+fvf/w7ABRdcQGxsLF999dUZg+CgQYNyn7vdbnr27MmhQ4eYP39+bhCcOXMmycnJLFiwgOjo6Nw6c/z444/89NNPTJ8+nQEDBuSuHzx4MMUxbNgw7rgj/7fq888/n/s8KyuLrl27cuGFF7J69Wp69uzJ8ePHmTFjBnfccQdPPPFE7r6XX3557vM//elPLFy4kDFjxhAQEIDb7WbRokVcc801VKniD/9FRSA7282xlHQSkvMHvIQTrnzrfkt2cSzl9PNchgQl5bvmLjy0CtVCgmgQFUK10CBnXYizLmdbeEgVz7Y8yzzX7lUNKf2RsyKVic9/y1hrnwaeLuK+u4BC+/SttXuBO0ujrsqsffv2+UIgONfEzZgxg08++YQDBw7km9A4MzPzjIGlf//++b5u1aoVGzduPGMNSUlJTJs2jWXLlnHo0CGyspxpF2JiYnL3Wb58ORdccEFuCCxo+fLlREVF5QuBJXG63rnvvvuON954g61bt5KcnJy7fteuXfTs2ZM1a9acdXT2kCFDmDFjBitWrKBPnz4sX76c+Pj4Uh3II3I2brebfcdSWbs3EXvwBIdPpOULeb8lp5OV/ftuurDgQKIjw6gbGUrLuhH0aVGbupGhziMiNPd5wr6ddO5wng/+ZSJyJj4PghXF9d0bedUr56/z0wG/C4EAU6ZM4eOPP2b06NG0b9+eyMhIli1bxhtvvIHL5TpjECx4T9zg4GBcLtcZaxg/fjzr1q1j1KhRtGzZkoiICObOncuyZaduDJOYmEjHjh0LbSMxMfF31+qVRO3atfN9vX79ekaNGsXAgQMZPnw4tWvXJiAggBtvvDH335eY6MxYdKY6GjduTK9evViwYAF9+vRhwYIFdOrUSXN5SZk6kuxi/b4k1u5NZN2+RNbvS+LoSWcUbVBgAHUiQnLD3Hn1q+cJdmGngl5kKNVCgop0zV3SAV2XJ+KPFATld073Q33x4sXcdttt+U7nfvfdd2Xy/i6Xi2+//ZannnqKW265JXf9nDlz8u0XFRVFQkJCoe2cbTtASEjI727XlpR0+hvUFPxcli5dSs2aNXnllVdyt8XHx/+uBoCEhARq1apVaB033HADTz75JOPGjeOrr77i8ccfP2PdIt5ISc/k1/3HWbc3MTf47T3q3DEnIADaREcysF00nRtH0blRFKZeJMFBFeVW9CJyJgqClVRReuXycrlc+Ub5ZmVl8dlnn5VFaaSnp5OdnZ3v/ZKTk/n666/z7de3b18++OADfvvtt9P2Yvbt25e33nqLb775hksuueS071WvXj22b88dgE52djbLlxdtlqG0tDSCg4PzBcRPPvkk3z5du3YlLCyMRYsWnTHcXX755UyePJmHHnqI7OzsfNdIingjMyubLYeSWbcvMTf4bT2cnHtat2FUVbo0jmJYn6Z0bhRFh4Y1/Po+qCJStvS/v5Jq3rw5y5YtY+nSpcTExBAdHZ3v+ruC+vXrx+zZs3MHmsyePZv09PRC9y+JyMhIOnbsyOuvv05ERASBgYH861//IiIiIt91eH/+859ZtGgRt956K/feey/16tVjx44dpKSkMHz4cPr378/555/PuHHjGD16NOeddx4JCQmsWrWKyZMnAzBw4EDmzJlDu3btaNy4MR9//HG+9ziT/v3789577/Hss89y6aWXEhsby3//+998+1SvXp1Ro0bx8ssvk5GRwYUXXkh6ejrfffcdY8aMyf3MQ0NDufrqq5k9ezZXXXXV706ni5xO3uv61nl6+jbEJ+XeFSMqPJjOjaK4/LwYOjeOolOjKOpGajStiJyiIFhJDR06lLi4OCZMmEBSUlLuPIKFefLJJ5k4cSKTJ08mLCyMwYMHc9lll/Hkk0+WSX0vvvgiTz31FI8//jhRUVHceuutpKWlMWvWrNx9atWqxdy5c5kyZQrPPfcc6enpNG3aNHcUc0BAAK+//jqvvvoq7733HkePHiU6Ojp3jkFw5k88evQor776KsHBwdx66620atWK2bNnn7XGiy66iEceeYRZs2bx0Ucf0aVLF2bMmMEVV1yRb7+RI0dSo0YN3n//febNm0eNGjXo0aMH1arlvwn5wIEDmT17Ntdff31JPjqpBNxuN0vjDjPt662s3+dcyhBaJZAODWswtFdTOjeuQZfGUTSpFa4580TkjALcRZ2sqRKJi4tzn+nm6KVx83R/Hiwip5zL4/TCCy+wePFili5detaphUrje7AiqSyfR3a2myWbDjJ12TY2HThOk1rh3Nm/Gb2a16JNjH9f11dZjlF5p+NUPhQ8TqtXr17dvXv3HsVpSz2CIj62Y8cOtm/fzty5cxkzZkypzS8pFUdWtpvPNxzgta+3YQ+doHmdarx4Q2eu7dKAKn4c/kTE/ykIivjYxIkTWbduHZdeeinDhg3zdTniRzKzsvl0/QGmfb2V7QknaRUdwas3d+GqTg0I0n1xRaQUKAiK+NgHH3zg6xLEz2RkZbNoTTzTv93Ozt9O0rZeJK8N7cofOtRXABSRUqUgKCLiJ9Izs1kQu4/Xv93G3qOptG9QnX/e1p3Lz4shUAFQRMqAgqCIiI+5MrOYv2of//x2O/GJqXRuVIOnr27PpW2jNepXRMqUgqCIiI+kZWQxb+Ue/vndDg4eT6Nbkyie/VMHLmpTVwFQRM4JBUERkXMsJT2TOSv2MOP7HSSccNGrWS1evLEz/VrWVgAUkXNKQVBE5BxJdmXywf9289YPOzhyMp1+LWsz7Zau9GlR29eliUglpSAoIlLGMrKyefennUz/djuJKRlc2KYu91/aih7Navm6NBGp5BQEK6nPP/+ctLQ0rrvuunLVtkh5s3zHEZ5ctJGth5O5qE1dHhzYmq5Navq6LBERQEGw0lq8eDHHjh0rk7BWlm2LlBcJJ1w8/3kcC9bE0zCqKm/e3oPLzovxdVkiIvkoCIoUIiMjg6ysLF+XIeVMVrab2St2M+VLS1pGFmMuacXoS1pRNSTI16WJiPyOblJZCY0fP54vv/ySlStXYozBGMO0adNyty9dupTrrruOjh070r9/f1544QUyMjJytx88eJAHHniAvn370qlTJwYOHMgrr7xSpLYLeuedd7j++uvp3r07/fr1495772X37t2/2++rr75iyJAhdOrUid69ezN8+HDi4+Nzt2/evJl7772XHj160LVrV4YMGcJPP/0EwIIFCzDGcPLkyXxtXnrppfzf//1f7tfDhg3j/vvv58MPP2TgwIF06tSJhIQEtm/fzkMPPcRFF11E586dGTRoEDNnziQ7Oztfe8eOHeOpp57i/PPPp2PHjlxxxRXMnDkTgAceeOC0t4+bNm0a/fr1y/f5Svm1dm8i177+I0/951c6NarB4gcv5JErjEKgiPgt9QhWQqNGjWL//v2cOHGCiRMnAlCvXj3Aub5v3Lhx3HTTTTz88MPs2bOHl156CbfbzeOPPw7AY489hsvl4plnniEyMpK9e/eyY8eOs7Z9OgcPHuS2226jQYMGJCcnM2/ePG6++WaWLFlCZGQkAIsWLeLxxx9n0KBBjBo1CrfbzfLlyzl69CgNGzZk+/bt3HLLLTRv3pxJkyYRFRXFxo0bOXDggNefTWxsLHv27OGRRx6hatWqREREsG3bNpo3b87VV19NtWrViIuLY9q0abhcLkaOHAlAWloat99+O0eOHGH06NG0aNGCPXv25IbaIUOGMHz4cPbu3Uvjxo0BcLvdLFy4kGuuuYbg4GCvaxX/kZiSzgtfWuau3EPdiFCm3dKVqzrV11QwIuL3FAQroSZNmhAVFYXb7aZLly65691uN1OmTGHw4ME8/fTTuetDQkKYPHkyI0aMoGbNmmzYsIEXX3yRSy+9FIDevXufte3CTJgwIfd5VlYW/fv3p2/fvixbtozBgweTnZ3Niy++yGWXXcZLL72Uu++AAQNyn7/++utERkYyZ84cwsLCAOjfv7/3Hwxw/PhxFi1aRJ06dQBITU2lb9++9O3bF3A+o+7du5OWlsb8+fNzg+CiRYvYunUrCxcupF27dgC5r8mpp169eixcuJD7778fgOXLlxMfH69rKcux7Gw3H8fu4+9fbCYpNYO7+jfnwYGtiQxTsBeR8kFBsLSsnQtrZhV595DsLAgspdNFXW+DLreUuJmdO3eyf/9+rrzySjIzM3PX9+nTB5fLxdatW+nVqxdt27blpZdeIjExkT59+tCgQYNiv+fatWt59dVX2bRpE4mJiflqyVkePnz4jGFp+fLlXHPNNbkhsCTat2+fGwJzuFwuZsyYwSeffMKBAwfyncbNzMykSpUqLF++nPPOOy83BBYUGBjIddddx6JFixg7diwBAQEsXLiQDh060KZNmxLXLede3IHjPLloI6t2H6N705r8bXAH2tWv7uuyRES8oiAouY4dOwbAiBEjTrs951TrK6+8wssvv8zzzz/P8ePHadu2LePHj8/XA1YU+/fv56677qJTp05MmjSJ6OhogoODGTlyJOnp6flqqlu3bqHtJCYmnnG7NwqGQIApU6bw8ccfM3r0aNq3b09kZCTLli3jjTfewOVyUaVKlSLVcN111zF9+nSWL19Ox44dWbJkCY899lip1C3nTrIrk5e/2sLMn3dRo2owLwzpxJBujQgM1GlgESl/FARLS5dbvOqVS09NpWrVqmVYkPeioqIAeOaZZ07bs9WoUSMAYmJi+Pvf/052djbr169n2rRp3HfffXzzzTfUrFn0+dF++OEH0tLSmD59OuHh4YDTw5aUlJS7T057CQkJZ6z7TNtDQ0MBfjcgI+/75DjdNV2LFy/mtttuY/jw4bnrvvvuu9/VsGfPnkJrAOfz69evHwsXLmTfvn1kZ2dz1VVXnfE14j/cbjefrj/A3z7bxOETLm7p1YTHrjBEhYf4ujQRkWLTqOFKKjg4GJfLlW9d8+bNiYmJIT4+no4dO/7uUTDkBQYG0qVLF8aMGUNqair79+8vtO3TSUtLIzAwkCpVTv098sUXX+Q7LZ1T06JFiwptp2/fvnzxxReFvmdMjDN32/bt23PXrVu3juTk5LPWCM6p4ZCQU7/ss7Ky+Oyzz35Xw6ZNm9i8efMZ2xoyZAhLlixh7ty5DBw4kOrVdSqxPNiekMywt1cydu4a6kaGsnBUf577U0eFQBEp99QjWEk1b96cZcuWsXTpUmJiYoiOjiYmJobx48fz2GOPkZyczIUXXkhwcDB79+5l6dKlTJ06lczMTO6++26uvfZamjdvTnp6Ou+88w5169alZcuWZ2y7oD59+pCVlcUTTzzBkCFD2Lp1K++8806+cBQYGMijjz7KI488wrhx47jqqqsICAhg+fLlDBo0iI4dOzJ69GiGDBnCrbfeyl133UVUVBSbNm0iKioqd8qZmJgYnn32WR544AESExN56623iIiIKNJn1a9fP2bPnp07EGb27Nm5p65zDB48mNmzZ3P33XczZswYmjdvzr59+9i1axePPPJI7n4DBw5k0qRJ/Prrrzz88MPFOXRyDqWmZ/H6N9uY8f12woKDmHxte27t3ZQgnQYWkQpCQbCSGjp0KHFxcUyYMIGkpCTGjBnD2LFj+eMf/0i1atWYMWMG//73vwkMDKRx48ZcfPHFBAcHExQURJs2bXj//fc5ePAgYWFhdOnShbfffjt3sEZhbRdkjOH555/ntdde46uvvqJt27a8+uqrPPTQQ/n2u/rqqwkNDeWf//wn999/P+Hh4XTu3JlatZz7tLZo0YI5c+bw4osv8pe//AWAVq1a5QatkJAQXnvtNSZNmsT9999P8+bNefrpp3n00UeL9Fk9+eSTTJw4kcmTJxMWFsbgwYO57LLLePLJJ3P3CQ0N5b333uPFF19k6tSpJCcn07BhQ4YOHZqvrZCQEC644AJWrVpFv379ini0xBeWbjrE05/8yr5jqVzXtSFP/LEddSNDfV2WiEipCnC73b6uwe/ExcW5Cxv96dle6OjQokr1w2sE5fdK+zhlZmZyySWXcP311/Pggw8Wu53S+B6sSErz89iekMxzn8WxbPNhWkdH8MzgDvRpUbtU2q7M9D1bPug4lQ8Fj9Pq1atXd+/evUdx2lKPoMg5kJ6ezubNm/n0009JTEzk5ptv9nVJUsDRk+lMXbaVWct3ExYcxBN/aMtd5zcnOEiXUotIxaUgKHIOHD58mBtuuIHatWszefLkM95tRc4tV2YW7/+8m6lfb+WkK5OhvZvw4MA21InQaWARqfgUBEXOgUaNGmGt9XUZkofb7eaLjQf5+xeb2XM0hYtNXSb8sR1tYiJ9XZqIyDmjICgilc7avYn87dNNrNp9DBMTyft39eLCNqUzKbmISHmiICgilUZ8YiovLN7Mf9bup05EKM9f15EbezTWdDAiUmkpCBaT2+0+7V0oRMqaRvp770RaBm98u523f3TuYT3mklbce3FLIkL1I1BEKje/+SlojGkETAauBGoDB4BFwCRr7TEv2rkeGAt0BUKAHcAs4EVrbfqZXltUwcHBpKam5t4WTeRcSk1NJTg42NdllAuZWdnMX7WPl76y/Jaczp+6NuTRKwwNojR1k4gI+EkQNMa0BH4GooH/AJuBXsADwJXGmP7W2iNFaOc54AkgGfg3cBS4AHgOGGCM+YO1NuMMTRRJdHQ08fHxNGzYkKpVq6pnUM4Jt9tNamoq8fHxp71Ti+T33ZYEnv1sE1sOJdOrWS3evqMdnRtH+bosERG/4hdBEJiOEwLvt9ZOy1lpjHkJeAh4Frj3TA0YY7rhhMBEoLu1dodnfYCn/XtxegpfKmmxObdA279/PxkZxcuVGRkZ6tUpB/ztOAUHBxMTE6N7FJ+BPXiCZz+P4/stCTStHc4/b+vGFe3r6Q82EZHT8HkQ9PQGXg7sAl4vsHkiMAIYZowZZ609eYamBnuWb+WEQABrrdsYMwEnCI6mFIIgOGGwJL+MNXt7+aDjVH4knHDx8tItzFu5h4jQKvx1UDuG9W1KaJUgX5cmIuK3/GHK/Es8yyXW2uy8G6y1J4CfgHCgz1nayZmhd0fBDZ5rDI8BLYwxzUtWroj4k7SMLD7ccIxL/vEt83/Zy+19m/Hdo5dwzwUtFAJFRM7C5z2CgPEstxSyfStOj2EbYNkZ2vnNs/xd0DPGRAE187zfTu/LFBF/syMhmbtm/sKuIylcdl4MT/yhLS3qRvi6LBGRcsMfegRreJZJhWzPWX+2q7w/8yyHG2Oa5az0XCP4bJ79aiIi5V7snmNc/8bPnEjL5LnL6/Pm7T0UAkVEvOQPPYKlwlr7kzHmbeBuYL0xJu+o4U44I5HbAtmFt+JwuVzExcWVZbmkpaWV+XtIyek4+afle07y9+8PUys8iL8NrEetkCwdJz+n/0vlg45T+VCax8kfgmBOj1+NQrbnrE8sQlvDgZWe5Y2AG1gOXAz8FScIHj5bI6GhoWU+QECDEMoHHSf/M3vFbp75dgcdG9bg7T/3pE5EqI5TOaBjVD7oOJUPBY/T6tWri92WPwRB61m2KWR7a8+ysGsITzVkrRv4l+eRjzGmI05vYGwxahQRH3O73bz01Ramfb2NS0xdXr+1G+Eh/vAjTESk/PKHawS/8SwvN8bkq8cYEwn0B1JwevaKxRhzMdAE+MxaW9i1iCLipzKysnn04/VM+3obN/dszJu391AIFBEpBT4Pgtba7cASoBnOPH95TQKqAR/knUPQGNPWGNO2YFvGmN9N7GeMaQq8BaTjnB4WkXLkpCuTu99bxcer9/HgwNY8f11HqgT5/EeXiEiF4C9/Uo/CucXcVGPMACAO6I0zx+AW4C8F9s+5QrLgrQLe9gS/WJyBIs2Ba4BgYJi1dn3ZlC8iZeHwiTTumvkLcQdO8H/Xd+Smnk18XZKISIXiF39We3oFewAzcQLgOKAl8CrQpyj3Gfb4FMgAbgAeAc4HPgY6W68UYmIAACAASURBVGs/LOWyRaQM7UhI5vo3fmb74ZO8dXsPhUARkTLgLz2CWGv3AncWcd/T3jTUWvse8F5p1iUi517snmPcPfMXAgMCmDeiD50bn20aURERKQ6/CYIiIgBfbTrE2Lmx1Ksexnt39aJp7Wq+LklEpMJSEBQRvzF7xW6eXLSRjo2ieOeOHtSOCPV1SSIiFZqCoIj4XN45Ai9tG81rQ7tqehgRkXNAP2lFxKcysrJ5YsEGPl69j1t6NeaZaztoehgRkXNEQVBEfCbZlcmo2bF8vyWBhwa24f4BrQgIOO1YMBERKQMKgiLiE3nnCHzh+k7c2LOxr0sSEal0FARF5JzbnpDMHe+s5EhyOm/d0YNLTLSvSxIRqZQUBEXknFq9+xj3vPcLQYEBfDiyD50aaY5AERFfURAUkXNmya8HGTt3DfVraI5AERF/oCAoIufEf9ft58F5azRHoIiIH1EQFJEy94knBPZsVot37+ypOQJFRPyEJusSkTL16fr9PPjhWnooBIqI+B0FQREpM5+tP8AD89bSvUlN3v2zQqCIiL9REBSRMvH5hgPcP28N3ZpE8e6dPakWqhAoIuJvFARFpNR9seEAY+euoWvjKN69s5dCoIiIn1IQFJFStXijEwK7NI5i5l29iFAIFBHxWwqCIlJqFm88yJg5a+jUqAYz7+ypECgi4ucUBEWkVHz560HGzImlY6MavHdXLyLDgn1dkoiInIWCoIiU2JJfDzJ6diwdGioEioiUJwqCIlIiSzcdYvQcJwS+f3cvqisEioiUGwqCIlJsSzcd4r7ZqzmvgUKgiEh5pCAoIsWyLM4TAutX5/27FAJFRMojBUER8do3mw9z36xY2tWvzvt396ZGVYVAEZHyyKsgaIzRT3uRSu6bzYcZ+cFqTL1IPrhLIVBEpDzztkcw3hjzf8aYVmVSjYj4tW+tEwLb1Itg1t29qRGuECgiUp55GwQDgUcBa4z5yhhzvTEmqAzqEhE/892WBEZ8sJrWMQqBIiIVhbdBsAFwG/ADMACYD+wzxjxrjGlWyrWJiJ/4bksCw99fRau6Ecy+pzdR4SG+LklEREqBV0HQWpturZ1jrb0YaAu8AlQBngC2GWM+N8Zca4zRIBSRCuJ7TwhsqRAoIlLhFDuwWWu3WGvHAQ051Ut4JbAA2GOMedoY06B0yhQRX/hhqxMCW9Spxux7elOzmkKgiEhFUuKeO2ttOvAZsBDYDwTgnEJ+CthpjHnFGBNa0vcRkXPrx62/cc97q2hepxpzhvehlkKgiEiFU6UkLzbG9AFGAjcCYcBxYCrwDtANeBgYC4QC95WoUhE5ZzbsS+Lu936huacnUCFQRKRi8joIGmMigWE4AbADTg/gGmA6MMdam+rZdb0x5gNgMTAEBUGRcuFEWgZj5sZSq1oIs+7pTe0IdeiLiFRUXgVBY8zbOL1/4YAL+ACYbq1debr9rbVZxphvgUtLWKeInANut5sJCzey71gq80b0oY5CoIhIheZtj+CdwHbgn8C71tqjRXjNt8BkL99HRHxg/qq9fLJuP49c3oaezWr5uhwRESlj3gbBK621S7x5gbX2J+AnL99HRM6xLYdOMPG/v9KvZW3uu1g3DxIRqQy8CoLehkBvGGMa4fQcXgnUBg4Ai4BJ1tpjXrRzPs7dTzoD9YDDwEZgqrV2cWnXLVIRpKZnMWZOLNVCqvDKTV0ICgzwdUkiInIOeDV9jDFmgDHmncLmBzTGNPBsv9jLdlsCq3FOPa8EXgZ2AA8A/zPG1C5iO/dx6q4nP3ja+Q64CPjCGPMXb+oSqSwmf/orWw4l89JNXYiuHubrckRE5Bzx9tTwWKCttXb/6TZaa/cbY/oCNXCuDSyq6UA0cL+1dlrOSmPMS8BDwLPAvWdqwBgTDDwPpAHdrbU2z7bncEY2/8UY8w9rrcuL2kQqtE/W7Wfuyr3ce1FLLmpT19fliIjIOeTthNLdgJ/Pss+PQI+iNujpDbwc2AW8XmDzROAkMMwYU+0sTdXCCaBb8oZAAGttHLAFqApEFLU2kYpuz5EUJizYQNcmUYy7vI2vyxERkXPM2yAYjXP3kDM55NmvqC7xLJdYa7PzbrDWnsAZaBIO9DlLO4eBBKCNMaZ13g3GmDZAa2CttfaIF7WJVFjpmdmMmRtLQABMvbkrwUG6RbiISGXj7anhJKDxWfZpjNOLV1TGs9xSyPatOD2GbYBlhTVirXUbY0YDs4DVxpicW941BP4E/Arc7EVdIhXaC4s3s35fEv+8rRuNa4X7uhwREfEBb4PgSmCwMaaetfZgwY2eQSSD8W66mBqeZVIh23PWR52tIWvtR8aY/cBc4PY8mw4B7+IMQDkrl8tFXFxcUXYttrS0tDJ/Dym5inqcVu5L4a0fD3KVqU7ToETi4hJ9XVKJVNTjVJHoGJUPOk7lQ2keJ2+D4DRgEPCDMWYc8KW11mWMCcWZ9uVFnGvwppZKdV4yxtwGvAksAJ4BdgNNgSeB13BGD994tnZCQ0Np165dGVYKcXFxZf4eUnIV8TgdTErjlY++p1396vzjtn6EBQf5uqQSq4jHqaLRMSofdJzKh4LHafXq1cVuy6uLgjzzCD4DtAQWAieNMQk4p4IXAC2Av3k5X19Oj1+NQrbnrD9jl4XnOsB3cE4BD7PWbrbWplprN+PcG3k1cIO3U9uIVCRZ2W4emLcGV2Y2rw3tWiFCoIiIFJ/XV4dbayfi9P59DhzFCWpHgc+AKzzbvWrSsyxsyGLOwI/CriHMcTkQDHx3mkEn2cD3ni+7e1mfSIUxddlWVuw8yuRrO9CyrgbQi4hUdt6eGgZyewZL6y4j33iWlxtjAvOGOGNMJNAfSAGWn6WdUM+ysInQctanF7dQkfLsf9uPMO3rrVzXtSFDujfydTkiIuIHfD5fhLV2O06obAaMLrB5ElAN+MBamzsS2RjT1hjTtsC+P3iWQ4wxnfJuMMZ0AYYAbuDr0qtepHw4kuzigXlraFa7Gs8M7uDrckRExE8Uq0ewDIzCmah6qjFmABAH9MaZY3ALUPDWcDlDZXJviGqtXWmMeRfnNnW/eKaP2Y0TMAcDIcAr1tpfy/DfIeJ3srPdjPtoHYmpGbx7Z0+qhfrLf3sREfE1r38jGGPqA38FrsCZoy/kNLu5rbVFbttau90Y0wOYjHP94R+BA8CrwCRr7bEiNnU3zrWAf/bUFwkcx7nbyZvW2nlFrUmkonj7x518axOYfG172jcobEyWiIhURl4FQWNMQ5y5BGNwRueG4vS6uXBGDFcB1lL4nICFstbuxenNK8q+AYWsdwMzPQ+RSm/t3kT+b/Fmrmgfw7A+TX1djoiI+BlvrxF8CqgHXGmt7exZ9661ti1OEPwS536+15VeiSJSHMfTMhg7N5aY6mG8cH1nAgJO+/eTiIhUYt4GwSuAxdbapQU3WGv3ATfgBMFJpVCbiBST2+3miX9vYH9iGlNv6UKN8GBflyQiIn7I2yBYD+eUcI4snOAHgLU2GfgKuLbkpYlIcc1duZfPNhxg3OVt6N60lq/LERERP+VtEDxO/sEhx3AGjOSVROFz+YlIGdt88DiTPvmVC1rX4d4LW/q6HBER8WPeBsHdQOM8X68DLjXGhAMYYwJx7vCxr3TKExFvpKRnMmbOGiLDgnnpxi4EBuq6QBERKZy3QXAZcIkxJueCo/eABsDPxpgpwE9Ae+DD0itRRIrq6f/+yvaEZF65qQt1I0PP/gIREanUvJ1H8G2c08F1gAPW2lnGmO7AWCDnbh7zgGdLr0QRKYr/rI1n/qp9jL6kJee3ruPrckREpBzwKghaa7cC/1dg3UPGmOdwpo/ZZa09VIr1iUgR7PrtJBMWbKBH05o8NLCNr8sREZFywtsJpW8HDllrv8y73lqbACSUZmEiUjSuzCzGzI2lSlAgr97SlSpBPr+FuIiIlBPe/sZ4B+cWcCLiB7Ky3Tw8fx0b44/zwpBONIyqevYXiYiIeHgbBA8W4zUiUgays92M//d6Plt/gAl/bMsV7ev5uiQRESlnvA11i3FGDSsMiviQ2+1m8qeb+Gj1Pu4f0JoRmi9QRESKwdtA9xcgEnjbGKNhiSI+8o8llpk/7+Lu85vz0MDWvi5HRETKKW+nj5mLc+eQ24GbjTG7cE4Xuwvs57bWDihxdSLyO69/s43Xv9nOLb2a8NdB7QgI0KTRIiJSPN4GwYvzPA8FjOdRUMFgKCKlYOZPO5nypeXaLg342+AOCoEiIlIi3s4jqGsDRXxk/qq9PP3JJi47L4Z/3NCZIN0+TkRESkjBTqQc+HT9fsb/ez0XtK7Da0O7Eqy5AkVEpBTot4mIn1sWd4gH562le9Oa/GtYD0KrBPm6JBERqSC8vbPIhUXd11r7vffliEheP2/7jftmx9KufnXe/nNPqoYoBIqISOnxdrDItxR9IIh+Y4mUwOrdx7jn/VU0qx3O+3f1onpYsK9LEhGRCsbbIDiZ0wfBKKAn0A/4BIgtYV0ildrG+CT+/O5KoiNDmXV3b2pWC/F1SSIiUgF5O2r46TNtN8b8GZiGM/G0iBTDtsMnuP2dlUSGVmHWPb2Jrh7m65JERKSCKtXBItbamcBy4LnSbFeksthzJIVb31pBYEAAs4f3oVHNcF+XJCIiFVhZjBpeCxR5UImIOA4kpTL0reW4MrOZdU8vmtep5uuSRESkgiuLINgY7689FKnUfkt2cetbK0hMyeD9u3rRtl51X5ckIiKVQKkFNmNMEHAnMAT4sbTaFanoklIyGPb2SvYnpvL+Xb3p1CjK1yWJiEgl4e08gjvO0E6MZ5kOTChhXSKVQrIrkz/PXMn2w8m8eUcPejWv5euSRESkEvG2RzCQ008fkwFsAFYC06y1cSUtTKSiS8vI4p73fmH9viSm39qNi9rU9XVJIiJSyXg7fUyzMqpDpFJJz8zmvlmrWbHzKC/f2IUr2tfzdUkiIlIJ6V7DIudYZlY2D324lm9sAs8O7sjgrg19XZKIiFRS3l4jWBWoCxy01qafZnsozrWCh621aaVTokjFkZ3t5vF/b+CzDQf466B2DO3dxNcliYhIJeZtj+BTgAUiCtleDdiMBouInNY/llj+HbuPBwe25p4LWvi6HBERqeS8DYJ/AJZaa4+ebqNn/VLgqpIWJlLRLIs7xPRvt3Nzz8Y8MKC1r8sRERHxOgg2A7acZZ8tnv1ExGPv0RQe+nAt7RtU5+lr2hMQEODrkkRERLyePiYYyD7LPm4gzNtCjDGNgMnAlUBt4ACwCJhkrT1WhNdfDHxThLdqYq3d6219IsXlysxi9JxY3MD0W7sRFhzk65JEREQA74PgDuCis+xzMbDbm0aNMS2Bn4Fo4D841xn2Ah4ArjTG9LfWHjlLM7uASYVs6whcB2xUCJRz7W+fxrF+XxIzhnWnaW3dP1hERPyHt0Hwv8B4Y8xj1toXCm40xowHugG/23YW03FC4P3W2ml52nsJeAh4Frj3TA1Ya3cBT59umzFmrufpm17WJVIi/1kbzwfLdzPiwhaaK1BERPyOt0HwH8CtwPPGmBuBJUA80BC4AugC7MGLIOjpDbwcp0fv9QKbJwIjgGHGmHHW2pNe1osxpg7wJyAVeN/b14sU17bDJ3hiwQZ6NqvJo1cYX5cjIiLyO14NFvFcq3cxsAKn5288MNWz7Ar8D7ikKNf05XGJZ7nEWpvv+kNr7QngJyAc6ONNrXncAYQCH1lrE4vZhohXUtIzuW9WLFWDg5h2SzeCgzR3u4iI+B9vewRzTsH2M8Z0wwlnUUAisNxaG1uMGnK6SgobjbwVp8ewDbCsGO0P9yxnFOO1Il5zu91MWLCBbQnJfHBXb+rV8HrslIiIyDnhdRDM4Ql9xQl+BdXwLJMK2Z6zPsrbho0xF+EEzY3W2p+LUZuI1+as3MOitft5+LI2nN+6jq/LERERKVRFv8XcCM/yX968yOVyERcXVwblnJKWllbm7yEl5+1x2nrExcTP4+neoCoD6mfoGJ8j+v/k/3SMygcdp/KhNI+Ttz2CTwEP4gwOOd3dRXJuMfcPz75FkdPjV6OQ7Tnrvbq+zxhTC7geZ5DIB968NjQ0lHbt2nnzEq/FxcWV+XtIyXlznJJSMhjxyQ/UjQzjzbvPp1a1kDKuTnLo/5P/0zEqH3ScyoeCx2n16tXFbssfbjFnPcs2hWzPuRfX2e5oUlDOIJH5GiQiZc3tdjPuo3UcSEzjtaHdFAJFRKRc8IdbzOXcDeRyY0y+eowxkUB/IAVY7kWbcGqQiFenhUWK41/f72Bp3CEm/LEd3ZvW9HU5IiIiReJtECz1W8xZa7fjzEfYDBhdYPMknNPNH+SdQ9AY09YY07awNo0xFwDt0CAROQdW7jzKC19a/tChHnf2b+brckRERIrML24xB4zCucXcVGPMACAO6I0zx+AW4C8F9s+5QjKgkPaKNUhExFsJJ1yMmRNLk1rhvDCkEwEBhX1LioiI+B9vewT/C3Q3xjx2uo15bjG3yJtGPb2CPYCZOAFwHNASeBXoU4T7DOetoSYwhGIMEhHxRla2mwfmrSEpNYPpt3YjMizY1yWJiIh4xee3mMthrd0L3FnEfQvtdvHc1aSqt+8v4q1Xlm7h5+1HeGFIJ9rVr+7rckRERLzmVRC01h4zxlwMzMG5q0g3nGsCc4LZz8BtXt5iTqTc+cYeZtrX27ixRyNu7NHY1+WIiIgUiz/cYk6kXIlPTOWhD9fStl4kk6/t4OtyREREis0fbjEnUm6kZ2YzenYsmVlu3ritO2HBQb4uSUREpNiKFQSNMfWBATjXBoaeZhe3tfaZkhQm4o+e+zyOtXsTeePWbjSvU83X5YiIiJSI10HQGDMJGF/gtQE41wrmfa4gKBXKZ+sPMPPnXdzVvzl/6Fjf1+WIiIiUmFfTxxhjbgWeBH7AmaIlAHgPGAq8iTPZ9Dzg0tItU8S3diQk8/i/19O1SRTj/1DoXOYiIiLlirfzCN4H7AOutNYu9KzbZa2dZ629F+cewzcCmktDKozU9CxGzY4lOCiA14d2I6SKt/9tRERE/JO3v9E6Ap9bazPzrMu9Wt5a+yXwJfBoKdQm4hee/M9G7KETvHJzVxpEaYpKERGpOIpzr+G8d/lIBWoU2Gcj0LkkRYn4iy+3Hufj1fsYe2lrLmpT19fliIiIlCpvg+ABIO9V8nuATgX2aQBkIlLO/bo/iekrjnB+qzo8MKC1r8sREREpdd6OGl4D5J1B92tghDFmGLAAuBhnEMlPpVKdiI/8vP037psVS2RoIK/c3IWgwELvaigiIlJuedsj+CnQwRjT3PP134EkYCZwHPgvzkjiv5ZWgSLn2ryVe7j97ZVER4Yy5coG1Ik43VSZIiIi5Z+39xqeiRP6cr7ea4zpCYwDWgK7gOnW2g2lV6LIuZGV7ebvX8Tx5g87uahNXaYN7Ur8zm2+LktERKTMFPsWczmstTuBMaVQi4jPJLsyeWDuGpZtPsyf+zXjr4PaUSUokHhfFyYiIlKGShwERcq7fcdSuOe9VWw9nMwz17ZnWN9mvi5JRETknFAQlEotds8xRry/CldmNjPv7MkFrTVFjIiIVB4KglJp/Xfdfh75aB31qocxb0QPWkVH+rokERGRc0pBUCodt9vNK0u38uqyrfRqVot/DutOrWohvi5LRETknFMQlEolLSOLRz9ezyfr9jOkeyOe/VMHQqsEnf2FIiIiFZCCoFQah0+kMeL91azbl8j4P7Rl5IUtCAjQRNEiIlJ5KQhKpbBp/3Huee8XjqVk8Mat3bmyQz1flyQiIuJzCoJS4S3ddIj7562helgwH93blw4Na/i6JBEREb+gICgVltvt5q0fdvLcF3F0bFiDN2/vQUz1MF+XJSIi4jcUBKVCSs/M5slFG/lw1V7+2LEeL97QhaohGhQiIiKSl4KgVDiJKencO2s1y3ccZeylrXhoYBsCAzUoREREpCAFQalQtickc/fMX9ifmMbLN3XmT10b+bokERERv6UgKBXGT9t+475ZqwkOCmTO8N70aFbL1yWJiIj4NQVBqRBmLd/NxP/+Ssu61Xj7jp40rhXu65JERET8noKglGuuzCwm/udX5v2yl4tNXabd0pXIsGBflyUiIlIuKAhKuXUgKZX7ZsWydm8ioy9pycOXGYI0KERERKTIFASlXFqx4wij58SSmp7FP2/rxpUd6vu6JBERkXJHQVDKFbfbzXs/7+Jvn8XRpFY4c4f3oXVMpK/LEhERKZcUBKXcSMvIYsLCDSyIjWdgu2heuqkL1XU9oIiISLEpCEq5sO9YCvfOWs3G+OM8NLANYy9tpUmiRURESkhBUPzez9t+Y/ScWDKz3Lx9Rw8GtIvxdUkiIiJnlpUJJw9DZH0I8N+OC78JgsaYRsBk4EqgNnAAWARMstYe87KtbsAjwIVAXSAR2Ay8ba19vzTrlrLjdrt564edPP9FHC3rRjBjWHda1I3wdVkiIiKnpCXBb9vgty15Hlvh6A7IzoAb3oP2g31dZaH8IggaY1oCPwPRwH9wQlsv4AHgSmNMf2vtkSK2NQZ4FTgGfAbEA7WADsAfAQXBciAlPZPH/72BT9bt5w8d6jHlhs5EhPrFt6uIiFQ22dlwPP5UyMsb+JIPntovsArUagG1W4P5A9RtC+aPvqu7CPzlN+t0nBB4v7V2Ws5KY8xLwEPAs8C9Z2vEGHM5MBX4ChhirT1RYLtGFpQDu4+cZOQHq7GHTvDYlYb7LmpJgB93q4uISAWRkQpHtv8+8B3ZBhkpp/YLrQF120CrAVCnNdRp4zxqNoOg8hU1fB4EPb2BlwO7gNcLbJ4IjACGGWPGWWtPnqW5KUAqMLRgCASw1maUvGIpS9/aw9w/dw0BAQHMvLMXF7Wp6+uSRMQfnDxy+lNvAYEQXBWCwz3LvM8LLr3YFuTzX49S1pIPw4H1cGAtHFgHB9fDsd2A27NDAEQ1dgJes/PzB75qdf36uj9v+MN3+iWe5RJrbXbeDdbaE8aYn3CCYh9gWWGNGGM6AJ1wris8aoy5BOiOc0TXAt8UbF/8h9vtZvq32/nHEouJieRfw3rQpLbuF+wzWZlwbCfUagmBgb6uRiqLrExI3P37wPfbFkjNc6l4lTDn1Ft0OycIZqQ6vTVpSXDioPM8I/XU+uxi9AEEBhcSEqueIVSeYV3t1hChP2x9wu12TuseWJf/ceLAqX1qtYD6XaDzLacCX62WEFLxfw/5QxA0nuWWQrZvxQmCbThDEAR6epaHgW9xBorktcEYc521dlsx65QykuzK5JH561j860Gu6dyAv1/fkfAQf/jWrGTSU2D712A/B/sFpB6Fms2h1wjoeiuE1fB1heVXdjbsXeGEmca9oFodX1fkW2nH4cjW319rdWR7/tBWLdr5hXze4FM9MXVaQ43G3v2BkpWRPxj+7nneZQpkpBWyzfO65MOnbyO3J6kQ9TpCy0udR+M+EBxWrI+v3HC74dCvTk9bWBSE1/Y8ajlfl8UfmW6380dswdCX4hlmEBAIdQw0vwjqd3Ye9TpU6p9v/vDbNufTTypke876qLO0E+1Z3o0zQGQQ8CMQAzwF3AZ8ZozpaK1NP1NDLpeLuLi4s9VdImlpaWX+HuXBvqR0Jn9ziPjjGYzoUYvB54Wye/tWX5eVq6IfpyBXIhH7fyQy/nuqHVxBYJaLrOBIkhv0I7VWe6rvXUr4l0+QvWwyic0Gcaz1ENKrN/N12b/jl8fJ7SY0cRvV93xJjT1fEZxyKHeTK7IpqXU6kVK3Myl1OpER0bjCnGYqKCAzjWqHfqFO/M+c/GYvISd2E5yakLvdHRBEekQj0qs3xdWmp7OMbEZ69SZkh1TP31gmcDAFDtqSVgWEex5AIBDqeRSX201AdjoBmWkEZrkIzErzPE8jMDOVsKObqXZoBeH/e52An14lOyiUlLpdOVmvN8n1epNevblffA+U+P9Sdibhv60nIv57IuO/I+TkgdPu5g4IJCukBlmhNcgKjSIzxFk6jxpkhUSRmfPcsz67Snj+zyg7i5ATewg7tpmwY9Z5JG4lKCPZeY/AKriqtyAtph9pNQ2pNdviimqFu0qeAJ4K7NwP7C/+v9kHSvNnnj8EwdKS86dFEHCztfZ/nq+PG2NuB9oCPYDrgblnaig0NJR27dqVWaEAcXFxZf4e/m7ppkM8/MVagqsEMuue3vRr6X+9JBXyOB3bBZs/h82fwZ6fwZ0N1RtC9zug7SCCmvanRlCw5y+0ibB/DYEr/kWtjR9Ta9vH0HIA9L4XWg30m9PGfnWcju6EjR/Dho8hYbMzirDlAOh4g3O90d4VhO5ZTuieH4na+Ynzmmp1oUkfaNLXWdbrVO4uOM8n5ShsXQKbP4VtyyAjhawq4QTFnAdtLst3rVVAzWaEVgkhFKgUN4t0JcPunwjc/jUR278mYu2rxIAz11xOb2GLi33Wa1ys/0vpKbDjG+dnSs7ZhKAQaHEJtH3C+b5OT3a+L1KOQMoRAlKOUMXzIPUYpCRA0mZne3bm6d8nMPhUr2KVEEiwpwZwVAmDmA7Q+abcnr6A6HaEVQmlIva7FjxOq1evLnZb/hAEc3r8CuuXzVmfeJZ2crYfzBMCAbDWuo0x/8EJgr04SxCUspWd7Wbq11t5ZelWOjaswT+HdadhVFVfl1Vxud1wcIPzQ3rzZ3Bog7M++jy4YBy0HeRcG1NYb0SDrvCnN+CyybB6JvzyFsy5wbmmptcI6DK0Up9WAZxThRsXwIaPIH6Vs65pf7jqZWh3LVSrfWrfJn2g/wPO6eIjW2HP/2D3/5xlnCcYBodDox6ngmGjnhDq5zEpca9zWcHmT2HXT+DOcsJN51ug7SC2pNWlXYdOvq7S90IjoM0VzgOcz23HN6cuy1g721lfv3Oe08i9oUpJuivLQMpR2LLY+ZmybRlkpjojadtc4fxMaTWgeN+zQxB5awAAHshJREFUbje4jnsC46ngmP9xFNJPQrc7Tp3erdNGA3yKyR8+tZw+/jaFbG/tWRZ2DWHBdgoLjDlXGitx+JArM4uxc9awZNMhru/WiGf/1IGw4CBfl1XxZGU6vX05PX9Je4AAJ1Rc/jdnXqvaLb1rM6IuXPQonP8gbPoPrPwXLB4PX//NCYO9Rjg9PZVFWhLEfeqEv53fOT2r9To6gbn9dU7v35kEBkJd4zy6/9lZd/wA7F0Oe5Y7wfD7KU67AYFO2znBsHEfqF6/zP+JZ+R2w+FNnj8wPnWuwwLn+qv+D/x/e/cdJkWV9n38O8RBHJIkQZHoDQYkSFBUQF3AnFBxRQV0TaxiWMOqPIo5rQl91F0FFFFU9BF9XRUVFAExjCKi4wFBQAwoUSQz0+8fp4Zpmu5hQqeZ/n2uq6+iq6qrTndN0XefcB9of7z/EVFYa5xuTffpot7e0OVc/yjI9yNYF06FhdNg1miY8aD/YdDysKC2sK//m0lFM/LqJUGw/yYsmRUE+82g82Af/LU8rPw12VlZ/odldl3/Y1MSLh0CwWnBsp+ZVQkf2WtmOUAvYAMwexfHmQ2sB1qaWe0oqWYOCJY/xKHMUgZb8wsYPuFL3stbzsjj92NYr5bKDxhPW9b7L5Dv3vS/1Deuhqo1/ZdH72tg32PiM2qxanU4cKB//PSFDwhzx/ll26N9s3Gbo9Km2Tiutm7yTZ5fvwzz34H8zT5v2OFXwwEDoXH78h2/zp6w/yn+AbB5HSz7rCgw/OJZ+OQJv63ePj4wbN4F6rWAunv5R3a9xAUJBfl+0Eth8Ld6MZDlB8AcPcoHA5n0YyDeqlSF5l3944hr/PVfPCMIDKf6vz3wwVebI6FZJ9+EvH0Qxh5Qq4FvNo2HUAiWzyu63r8GrQmNOsBhV/rr3axzWvRtlLJLeSDonFtoZlPwI4OHA6PDNo8CagNPhgd2ZtY+eO13YcfZYGZPA5cDt5vZVc65ULD/gcAQfFfjSYl9RxLNtvwCRkz0QeCtJ+3PuYe0THWRKr4Nq4pGxC2d7ZuXtm3ygcC+A/x/0m2O9E1RidK8C5zyRFiz8dMwYaBPu9DjIt8smF1nl4cpsfytPuXD2mXB40dY+xNN1v4By9v50YjhX4rh/YnKqiAffpju+/zlve6brWo3hoOH+n5/zbsm7ouwZk5R8yD49//r3KLAcOH7MHfijq+psXtRULj9sXfRv3Oale7z2LoRFn3gAwH3lm+aq1rD92M77Er/AyNH838nRM0cPzuFHeOfr15S1Iz83Rsw57kYr6sT416IcX/Uqu+DUPD98xbPKAr+1sShNUHSWsoDwcCl+CnmHjGzo4A8oAc+x+B84MaI/QvbGCL/9x2JTxtzBXBIkIOwCXAqkA1c4ZxbmJB3IDHlF4S4+uWv+O/Xv3LTcR0UBJbFuuVhqRDm+CSoa5cWba/f0veXaX8c7HNo8gca7N4Yel8Lva7wwdInT8Bb18L7t4U1G7ct/hihEGxaExbkFQZ6Yc/X/eKbSsNl16Nu/jZY8HLsY5f2izG7nv+sv34ZvnkV/lzuj9HhBF8T2vKI1PRHqlq9qMbokOH+M/tzOaz9KeKzCv798xzYsCLiIFmQ0zR2oFg3aNKOGOzh+3/1C/p/HZ3+fRYro/r7+G4EXYf4HyjrV0TvPxf+/M/f4LdgEMbWWHMyZEEtn96l3brfYcvaoDWhr6+ZjFdrgqSltAgEg1rBg4FbgQH4OYF/wc8ZPMo5t7q414cd5w8zOxz4J3A68Hf84PAZwP3OuSmJKH/aWPerbx6MpypVfRNUGWs8CgpCXPfKXCbP+Zlr+hsXHK4+H8UKhfwXeGQOrPC5LPdoC3t3g+4XBDmwOvqAJh1UqxHWbJwLn/wbPh8Dnz4Jbf8C3S7wNZTRgry1y/zIwnBVa/gRzXX38nm/dgpemkON2szPy6NDuzbB6MN4fDEWnr+m7/x+4OnQrl/65X3LCoK6nKawV9fo+2zdGCVQDD7/X+b6fqT5m6O/NmdPH8i3Pw72OSx+TY5SflWq+prY0tTGbt0YZQDGjs//zNlEve6DfPeORLYmSNpIi0AQwDn3IzC0hPvGjEqcc3/iaxAjaxErt2Wfw9N/2bm2JB72OwlO/U+pR62FQiFumjyPSbnLGHFUO4b33UWNUHmsXuI7rheX2b9qjfTqy1JQED3x6cZVfntWFT9heZu+RSPjmhwQ36bWRGreFU59sqjZ+POn4YUzd9yndiMf1DVs55s/wwO9Onv57SXta1itRny/GOu1gA7HV/wR0dVr+drYWDWyoZCvWQoPFLeu99djz86Vs69npqpey/94qts85i6/5OVRL11SMUlSpE0gKOX0/q2+k3D/O+Mb7Pz+HXz0L99p+cznoEbtEr0sFAox6o1vef6TpVzSpw1XHJ3ADuQLp8KL5+xcmxQpq0rJpoIKWzZctRaWNy7+uKW1aY2vifl1ru9vBj4/VpP9fOCx50E+nUvj/SrH9EY5TaDPdb4/2aIPfMBWd2+o08x/1qlUgi/GSi8ryzf77d7I9/kUkYyiQLAy+GG6T1/R/y6fTDPeGrSG1y+D8afAX1/yfUmKEQqFuPut7xg3azHnH9aKa/tb4kYHz30JXrvE15wde5+v3Yg5dVSsaaWCWqEo+zUK5ce/zNVq+SmNOp5RVNPXqEPlb3arVsP3MRMRkbShQLCiC4V8HrecZnDwsMSco/Ng3zF80vnwzPEw+FU/OCCGB96dz5PTF3FOz3246bgOiQsCZ42GKTdBy8Nh0ISENOHlffsNHdrHuZkkKyu9mqhFRCRjqfNHRff9ez6vV+9rEtuRfb+T4K8T/aTwYwb4bPhRjH5/AaOnfs+gbnsz6sT9ExMEFhTA2zf4IHC/k2HwK4nrx5VVxfeRiudDQaCIiKQJBYIVWSgEU2/zo3o7DU78+doeDee85juWjxkAKxbssPnJDxfyr3fnc2rn5tx5yoFUqZKAgGfbZnj1bzD7MZ+4eODY9Jt6SUREpIJQIFiR5b3hR5n2+Wfy+pe16AFD/p9PNzFmgB/0AIyd+QN3vfUdx3fck3sHdkxMELjpD5hwOsyb5GcxGHC3RjSKiIiUg75FK6qCfJh2h59ou+MZyT33nh1h6NtQLRvGHc+Ut19j1Bvf0n//Jjx4ZieqVU3An9W65TDuWFgyE05+ws93qyZWERGRclEgWFHNe8Wndul7Q9HUQMnUsC0Me5s/qtXn8I//xuX7LGH0WV2onoggcMX3PkfiykVw1ovQ6az4n0NERCQDKRCsiPK3wrQ7ocmB0OGklBXjtR+qcOSq61heY2+u/P1/qOFej/9JluXCmH5+xpQhb0C7o+N/DhERkQylQLAimvO8n5HiyJtS1kfuzbm/cNVLc2jXqjVNLnuPrOZdYdJQ+GJ8/E4yf4pPV1MzB86f4meqEBERkbhRIFjRbNsMH94LzQ/2c6CmwJRvfmXExC/p0qI+T513MLXqNIBzXoXWfeH1v8PHj5X/JF9OgBcG+anHzn8X9mhT/mOKiIjIDhQIVjS54+CPZXDUyJQMlpjmfmP4819wQPO6jB3ajdo1g5zkNWrDWRN9vsF3bvBN16FQ6U8QCvkp7SZfCq2OgCFvFpu8WkRERMpOgWBFsmU9TL/fz6TRqnfSTz9jwQouGp+LNc3hmWHdycmuvuMO1Wr4vH6dB8OH98Db1/vkzyVVkA9vXevnTT7wDD+dXc2c+L4JERER2U5TzFUkn/4H1v8GZ45Pem3g7EUrueDZz2jdsDbjh/Wgbq3q0XesUhVOfBSy68HHj/rcfyeOhqq7+FPbugn+70L4djIcehkcfatyBIqIiCSYAsGKYtNamPkQtP0LtOiZ1FPnLlnFsHGfsXf93Xjugh7Ur72L5NVZWdDvdh8MTrsdNv8BA8fEngFk4xqYeDYsmQH974RDhsf/TYiIiMhOFAhWFLMfh42r4cgbY+6SXxAiv6AM/fKK8c3Paxky5jOa1MlmwgU9aLh7Cadzy8ry8x9n1/HNvc+fAWdOgJq777jfHz/Dc6f56epOexoOHBjX8ouIiEhsCgQrgg2rYNaj0OEEaNY56i6zvvf999Zt3hb30+/doBbP/60Hjetkl/7FPS6CmnX84I/xJ8PZL0Ot+n7b7w7Gn+prOwdPgtZ94llsERER2QUFghXBzIdhy5/QN3pt4Lc//8FF43NpWjebizs3j+upq1bJ4uROzWlatwxBYKFOZ/mawEnDYNzxMPhVWL3Y1xJWrQFD34Q9D4pbmUVERKRkFAimu3XL4ZMn4cDToXGHnTYvW72BIWM/Zffsajx7fnf2rFsrBYUsgQ4n+FHAE8+Gp472g17qNPf5B+u3THXpREREMpKGZaa7GQ9A/hboc/1Om9Zs2MKQsZ+xcWs+44amcRBYqE1fOHeyHzzSZH8/W4iCQBERkZRRjWA6W/MjfD4GOp+908wam7bmc8Ezn7N05QaePb871rSC5NvbuxtcOQ+q1dp1ShkRERFJKH0Tp7Pp9/nlEdfusDq/IMSIiV+Su3Q1o8/qTM/We6SgcOWgJNEiIiJpQU3D6WrlQvjyOeg6FOrtvX11KBTilte/4Z1vljPyuP04vmOzFBZSREREKjIFgunqw3v8iNrDr95h9eMfLmT87CVceERrhh3WKkWFExERkcpAgWA6+i0P5r4EPS6EnCbbV7+Su4x733ac1KkZ1w9on8ICioiISGWgQDAdTbsTauwOva7Yvmr6/N+57pW59Gq7B/cNPIgqVZI717CIiIhUPgoE083PcyDvdT/f7m4NAJj301oueS6Xdk1yeGJwV2pU02UTERGR8lNEkW6m3eGnYDvkUgB+XLWBIWM/o95uNRg3tBs52dVTXEARERGpLBQIppOln8CCKdBrBGTXZdX6LZw75lO25hfwzLBuNCnLXL8iIiIiMSgQTCdTb4PajaH7hWzcks+wcZ/x85qNPH3ewbRtrNx7IiIiEl8KBNPFog9g8Udw+NVsq1qLy174gq+WreHhQZ05uGWDVJdOREREKiEFgukgFIKpt0OdvQh1HcLIyfN4L+83Rp24PwMOaJrq0omIiEglpUAwHcx/B5Z9Br2vYfT0H3nh0x+5tE8bzj2kZapLJiIiIpWYAsFUKyiAabdD/Va8vO0IHnh3Pqd2ac41/S3VJRMREZFKrlqqC1DIzPYCbgUGAHsAvwCvAaOcc6tLeIwPgN7F7FLLObepnEWNr7zJ8OvXfNvzfq6f/B2Ht2vIPad1JCtLCaNFREQksdIiEDSzNsAsoDEwGfgO6A6MAAaYWS/n3MpSHHJUjPXbylXQeCvIh2l3sqleO06f2ZwOe+bw+OCuVK+qiloRERFJvLQIBIH/xQeBlzvnRheuNLMHgCuBO4CLS3ow59wt8S5gQsx9CVbMZ2SVf9AgJ5sxQ7qxe810uSQiIiJS2aW86imoDewHLAYei9h8M7AeOMfMaie5aIlVsI38aXcyP6s174e68czQ7jTOUcJoERERSZ50qH7qGyynOOcKwjc459aZ2Ux8oNgTeL8kBzSzM4FWwBYgD5jqnNscvyKXX+0Fr1N17VLuz7+Op/7Wg9aNdk91kURERCTDpLxGECgcHjs/xvYFwXLfUhxzInAX8C/gv8BSMxtYtuLF37bNG6g992lyC9pxxqBhdGlRP9VFEhERkQyUDjWCdYPl2hjbC9fXK8GxJgP3A18CK4F9gPOAq4EXzew459zbuzrI5s2bycvLK8Hpymblkm/omb+a7/e9no5VVpOXV6JB0ZICmzZtSujfgsSHrlP60zWqGHSdKoZ4Xqd0CATjxjn3YOQq4AYz+xkYja8l3GUgWLNmTTp06JCAEgY6dCC3QTPO7H5o4s4hcZGXl5fYvwWJC12n9KdrVDHoOlUMkdcpNze3zMdKh6bhwhq/ujG2F65fU45zPIVPHdPJzHLKcZy42S1HzcEiIiKSWukQCLpgGasPYLtgGasP4a5P4JNIrwueVq7RxyIiIiJllA6B4LRg2c/MdihPUHvXC9gAzC7rCczMgPr4YHBFWY8jIiIiUpmkPBB0zi0EpgAtgeERm0fha/DGO+fWF640s/Zm1j58RzNrZWYNIo9vZo2AscHTic659JpdRERERCRF0mWwyKX4KeYeMbOj8Ln/euBzDM4HbozYv3CoTPiEvL2BJ8xsBrAIWAW0AI7F9zP8HLg2UW9AREREpKJJeY0gbK8VPBgYhw8ArwbaAA8DPUs4z3AuPn9gE+C04BgDgK+By4FezrnyDDgRERERqVTSpUYQ59yPwNAS7psVZd3XwJA4F0tERESk0kqLGkERERERST4FgiIiIiIZSoGgiIiISIZSICgiIiKSobJCoVCqy5B2cnNzfweWpLocIiIiIiWwT9euXRuV5YUKBEVEREQylJqGRURERDKUAkERERGRDKVAUERERCRDKRAUERERyVAKBEVEREQylAJBERERkQxVLdUFyCRmthdwKzAA2AP4BXgNGOWcW53KsolnZouBfWJsXu6ca5q80mQ2MxsI9AY6AQcBOcAE59zgYl5zKHAT0BOoBSwAxgCjnXP5CS90BirNdTKzlsAPxRzuRefcoESUM5OZ2R7AKcBxwIFAc2AL8DUwFhjrnCuI8jrdT0lU2usUr/tJgWCSmFkbYBbQGJgMfAd0B0YAA8ysl3NuZQqLKEXWAg9FWf9nsguS4W7CBxZ/AsuA9sXtbGYnAa8Am4AXgVXACcCDQC/g9EQWNoOV6joFvsL/CI40L47lkiKnA4/jKx+mAUuBJsCpwFPAMWZ2unNue2Jh3U8pUerrFCjX/aRAMHn+Fx8EXu6cG1240sweAK4E7gAuTlHZZEdrnHO3pLoQwpX4wOJ7fI3TtFg7mlkd4D9APtDHOfd5sH4kMBUYaGaDnHMTE17qzFPi6xRmju6xpJoPnAi8GVGjdAPwKXAaPth4JViv+yk1SnWdwpTrflIfwSQIagP7AYuBxyI23wysB84xs9pJLppI2nLOTXPOLYjy6zeagUAjYGLhl1ZwjE34GiuASxJQzIxXyuskKeCcm+qceyOy+dc59yvwRPC0T9gm3U8pUIbrFBeqEUyOvsFySpQLvM7MZuIDxZ7A+8kunOykppkNBlrgg/S5wHT1iUlrRwbLt6Nsmw5sAA41s5rOuc3JK5bE0MzMLsL3lV4JfOycm5viMmWqrcFyW9g63U/pJ9p1KlSu+0k1gslhwXJ+jO0LguW+SSiL7FpTYDy+uf4hfFPIAjPrndJSSXFi3mPOuW34DtXVgNbJLJTE9Bd8DccdwfIrM5tmZi1SW6zMYmbVgHODp+FBn+6nNFLMdSpUrvtJgWBy1A2Wa2NsL1xfLwllkeKNBY7CB4O18SO3ngRaAm+Z2UGpK5oUQ/dYxbABuA3oCtQPHoX9CvsA76uLTFLdDRwA/Nc5907Yet1P6SXWdYrL/aSmYZEwzrlREavmAReb2Z/A1cAt+OH9IlJKzrnfgP+JWD3dzPoBM4AewAXAw8kuW6Yxs8vx/6d9B5yT4uJIDMVdp3jdT6oRTI7CX091Y2wvXL8mCWWRsinsqHtESkshsegeq8CC5sangqe6xxLMzP6ODw6+Bfo651ZF7KL7KQ2U4DpFVdr7SYFgcrhgGasPYLtgGasPoaTe78FSzVbpKeY9FvSvaYXvZL0omYWSUtE9lgRmdgUwGt/a0TcYkRpJ91OKlfA6FafE95MCweQozKvVz8x2+MzNLAefnHMDMDvZBZMS6xks9R9fepoaLAdE2XYEsBswSyMc05rusQQzs+vwCaHn4IOL32LsqvsphUpxnYpT4vtJgWASOOcWAlPwAw6GR2wehY/Yxzvn1ie5aBLGzDpE61gbTOPzaPD0uaQWSkpqErACGGRmBxeuNLNs4Pbg6eOpKJgUMbMukT+Gg/VH4RNTg+6xhAiSQd8N5AJHOedWFLO77qcUKc11itf9lBUKKQdoMkSZYi4P35GzL75J+FBNMZdaZnYLvlPudGAJsA5og5/3MRv4L3CKc25LqsqYSczsZODk4GlToD/+1+1HwboVzrl/ROw/CT8l1kT8lFgn4lNhTALOUNLj+CvNdTKzD/BdYWbhZyMB6EhR3rqRzrnCQEPixMzOA8bhZwoZTfTRwIudc+PCXqP7KclKe53idT9p1HCSOOcWBr+sbsVXtx+Ln0/wYWCUc251KssngG/CN6Azvrm+Nr4z9Ax8XsHx+o8vqToB50Wsa01R7rIlwPZA0Dn3WpDr8Ub8VEzZ+GnPrgIe0bVLmNJcp/H4UffdgGOA6sBy4CXgUefcR0gitAqWVYErYuzzIT4IAXQ/pUhpr1Nc7ifVCIqIiIhkKPURFBEREclQCgRFREREMpQCQREREZEMpUBQREREJEMpEBQRERHJUAoERURERDKUAkERERGRDKVAUEQkDZjZYjNbnOpyRDKzIWYWMrMhqS6LiMSfAkERkQxmZn2CQO+WVJdFRJJPgaCIiIhIhlIgKCIiIpKhqqW6ACIikcysJfAD8AxwG3AP0BeoAXwMXOWcm2dmjYA7gBOA+sDXwLXOuWlhx2oGXAD0B9oADYAVwAfA7c65byPO/RpwEjDCOfdIxLbbgJuAMc6588vwvrKA4cAlQVlWAv8H3LiL150FXAh0BrLxn80E4D7n3OaIfUP4ien/iv/c+gM5wLfAv5xzz4ftOw44L3h6s5ndHHaovs65DyKO3Re4GegKhICPgH845/JK9AGISNpRjaCIpLOWwCdAE2AcMAU4GvjAzNoBs4FuwIvAS8BBwFtm1iLsGEcA1wNrgFeAB4PXDQQ+NbODIs45DFgK3GtmnQtXmtlRwA34gOqyMr6fh4DR+KD138BEYADwHj7I3YmZjQGeB9oG5X8MWIUPkN82s2g/6OsDs4ADgbHAs0BrYIKZXRO232v4YBt88Dgq7LE44pjH4z//P4An8EHgscCHZtawJG9eRNKPagRFJJ31Bm5yzt1RuMLMRgK34gPEl4BLnXMFwbZ38UHPlcEDYCrQxDm3LvzAQQA4E7gbOKZwvXNuVVAD9yHwopl1AWoDzwGbgTOccxtK+0bM7FDgcmAh0N05typYfyMwDdgTWBLxmiHAUHyt4dnOuY1h227B184NBx6OOF1H4GVgUNhnczeQC9xhZq845xY5514zszX4WsEPnHO3FPMWTgb6O+feDyvDXfggexhwb4k/DBFJG6oRFJF0thgfqIUrrMGqCVxTGOgEnge2AZ0KVzjnfosMAoP1X+GDxL5mVj1i2yxgJNAOeBIYDzQFLnfOfVPG9zI0WN5RGAQG59oE/DPGa0YE72dYeBAYuA3ftHx2lNflA9eFfzbOuR+AR4DqwDllKP/E8CAw8O9g2b0MxxORNKAaQRFJZ3Occ/kR634OlvMjAzznXL6ZLQf2Cl9vZscBFwMHAw3Z+f++hsAvEesK+yX+NXj+gnPuqTK9C69LsPwwyrYZ+OAtvMy74Zu6VwBXmFm0Y24GOkRZvzQI/CJ9gK9F7Bxl2658HmXdj8GyfhmOJyJpQIGgiKSztZErnHPbgqBop22BbfhaLwDMbAS+b95q4F18/78N+MEOJ+ODrZpRzhMys1eBfsGqh8r8Lry6wXJ5lHNtM7MVEavrA1lAI3zwVho7nSPwa0RZSmNN5Iqwa1G1DMcTkTSgQFBEKq1gIMUt+ACoi3Pul4jthxTz2nbA/fgAsi7wlJl1D5pyy6IwcG0CLIpSzobAsij7f+mc60LpNImxvmnEsUUkw6mPoIhUZg2BesCsKEHg7hQ11xKxrSZ+JHJt4EzgLvwI3PLUCn4RLHtH2XYYEbVqzrk/gW+A/c2sQSnP1SJIwROpT7D8MmxdYZO0avVEMpACQRGpzH7DNwN3DQI/AILBIQ/jA8Vo7sf3o7vXOfcuvml2JnCRmZ1exrKMC5Y3hgd2ZpaNDzSjeQCfVmaMmdWL3Ghm9YNRzZGqAveYWZWwfVvhRy1vw4+ALrQyWIan3BGRDKGmYRGptJxzBWb2CD7FyddmNhkfWPXFJ5aeFvx7OzM7Bfg7Pj3NTcFx8oOUMnPwTcS5zrkdmndLUJaZZjYan4NwnplNArbik1evZufBKjjnxphZV+BSYKGZvYPv49gAaIXPkTgWPxAm3FygB5BrZlPwtaJnBMtrnXMLw08D/AQMMrOt+BQ2IWC8c24JIlKpqUZQRCq7kcDVwEbgIuBU/AjY7vigarsgEfXT+D50g5xz2wq3Oed+xOfLqwNMNLOoCaB3YQQ+EFwblOUs4B18kuwt0V7gnBuOnznl42C/q4AT8f0W7yN6c/Vq4FB80/JQfJ7AH/C5CO+LOH4+cAp+5PLp+GTSt+EDTRGp5LJCoVCqyyAiInFSOMWcc65PqssiIulPNYIiIiIiGUqBoIiIiEiG0mAREZEyCNKzDCnh7g8553ZKyCwikmoKBEVEyqYlJZ/xYxxRZuZIBOdcVjLOIyKVgwaLiIiIiGQo9REUERERyVAKBEVEREQylAJBERERkQylQFBEREQkQykQFBEREclQCgRFREREMtT/B/j0QWoWfvi8AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ5tg9i_c2Bz",
        "outputId": "c98f1c6c-8a0d-4a64-caca-58485e117a36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "[depth for depth in range(1, 25)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1,\n",
              " 2,\n",
              " 3,\n",
              " 4,\n",
              " 5,\n",
              " 6,\n",
              " 7,\n",
              " 8,\n",
              " 9,\n",
              " 10,\n",
              " 11,\n",
              " 12,\n",
              " 13,\n",
              " 14,\n",
              " 15,\n",
              " 16,\n",
              " 17,\n",
              " 18,\n",
              " 19,\n",
              " 20,\n",
              " 21,\n",
              " 22,\n",
              " 23,\n",
              " 24]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmyDs4Qwci9O"
      },
      "source": [
        "#We see that the best score for test data is obtained when max_depth has a value of 14(not, should be 24). \n",
        "#From max_depth from 1 to 24, as we keep increasing the value of this parameter, test accuracy remains the same or gets worse, but the training accuracy keeps increasing. It means that our simple decision tree model keeps learning about the training data better and better with an increase in max_depth, but the performance on test data does not improve at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_HLXh4UdNXi"
      },
      "source": [
        "#This is called overfitting.\n",
        "## The model fits perfectly on the training set and performs poorly when it comes to the test set. This means that the model will learn the training data well but will not generalize on unseen samples. In the dataset above, one can build a model with very high max_depth which will have outstanding results on training data, but that kind of model is not useful as it will not provide a similar result on the real-world samples or live data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlaxNimsd1Db"
      },
      "source": [
        "#One might argue that this approach isn’t overfitting as the accuracy of the test set more or less remains the same. Another definition of overfitting would be when the\n",
        "test loss increases as we keep improving training loss. This is very common when\n",
        "it comes to neural networks.\n",
        "Whenever we train a neural network, we must monitor loss during the training time\n",
        "for both training and test set. If we have a very large network for a dataset which is\n",
        "quite small (i.e. very less number of samples), we will observe that the loss for both\n",
        "training and test set will decrease as we keep training. However, at some point, test\n",
        "loss will reach its minima, and after that, it will start increasing even though training\n",
        "loss decreases further. We must stop training where the validation loss reaches its\n",
        "minimum value.\n",
        "This is the most common explanation of overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZR8TAnzcJye",
        "outputId": "98bf7121-cc3e-4b9d-89b1-5256024e0dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_accuracies.index(max(test_accuracies))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FRnGLC2cP9X",
        "outputId": "62b8845d-2866-43d8-de21-3b41a64b131d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_accuracies[24],test_accuracies[15]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6110183639398998, 0.5976627712854758)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "838DE1NKgGGG",
        "outputId": "998a4c38-2954-4fee-ace0-44282713821d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7.1</td>\n",
              "      <td>0.470</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.20</td>\n",
              "      <td>0.067</td>\n",
              "      <td>7.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.99517</td>\n",
              "      <td>3.40</td>\n",
              "      <td>0.58</td>\n",
              "      <td>10.9</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.7</td>\n",
              "      <td>0.580</td>\n",
              "      <td>0.01</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.088</td>\n",
              "      <td>12.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>0.99568</td>\n",
              "      <td>3.32</td>\n",
              "      <td>0.56</td>\n",
              "      <td>10.5</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.3</td>\n",
              "      <td>0.490</td>\n",
              "      <td>0.36</td>\n",
              "      <td>1.80</td>\n",
              "      <td>0.222</td>\n",
              "      <td>6.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>0.99800</td>\n",
              "      <td>3.18</td>\n",
              "      <td>0.60</td>\n",
              "      <td>9.5</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.1</td>\n",
              "      <td>0.590</td>\n",
              "      <td>0.01</td>\n",
              "      <td>2.30</td>\n",
              "      <td>0.080</td>\n",
              "      <td>27.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>0.99550</td>\n",
              "      <td>3.42</td>\n",
              "      <td>0.58</td>\n",
              "      <td>10.7</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "      <td>1.185</td>\n",
              "      <td>0.00</td>\n",
              "      <td>4.25</td>\n",
              "      <td>0.097</td>\n",
              "      <td>5.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.99660</td>\n",
              "      <td>3.63</td>\n",
              "      <td>0.54</td>\n",
              "      <td>10.7</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   fixed acidity  volatile acidity  citric acid  ...  sulphates  alcohol  quality\n",
              "0            7.1             0.470         0.00  ...       0.58     10.9        1\n",
              "1            7.7             0.580         0.01  ...       0.56     10.5        4\n",
              "2            8.3             0.490         0.36  ...       0.60      9.5        3\n",
              "3            7.1             0.590         0.01  ...       0.58     10.7        3\n",
              "4            7.4             1.185         0.00  ...       0.54     10.7        0\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs944yc_goX6",
        "outputId": "4bd14244-305a-4674-8960-8c0a0dd2dd3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/gdrive/My Drive/notes'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlfzGZTwglbN"
      },
      "source": [
        "df_train.to_csv(\"train.csv\",index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfLnQIwTfe6B"
      },
      "source": [
        "#We can split any data into k-equal parts using KFold from scikit-learn. Each sample is assigned a value from 0 to k-1 when using k-fold cross validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpT24FrucWmA"
      },
      "source": [
        "# import pandas and model_selection module of scikit-learn\n",
        "import pandas as pd\n",
        "from sklearn import model_selection\n",
        "if __name__ == \"__main__\":\n",
        "  # Training data is in a CSV file called train.csv\n",
        "  df = pd.read_csv(\"train.csv\")\n",
        "  # we create a new column called kfold and fill it with -1\n",
        "  df[\"kfold\"] = -1\n",
        "  # the next step is to randomize the rows of the data\n",
        "  df = df.sample(frac=1).reset_index(drop=True)\n",
        "  # initiate the kfold class from model_selection module\n",
        "  kf = model_selection.KFold(n_splits=5)\n",
        "  # fill the new kfold column\n",
        "  for fold, (trn_, val_) in enumerate(kf.split(X=df)):\n",
        "    df.loc[val_, 'kfold'] = fold\n",
        "    # save the new csv with kfold column\n",
        "    df.to_csv(\"train_folds.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hnESSRagAn-",
        "outputId": "51761bb4-e2d0-443e-b4f1-e6b4a35988cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>fixed acidity</th>\n",
              "      <th>volatile acidity</th>\n",
              "      <th>citric acid</th>\n",
              "      <th>residual sugar</th>\n",
              "      <th>chlorides</th>\n",
              "      <th>free sulfur dioxide</th>\n",
              "      <th>total sulfur dioxide</th>\n",
              "      <th>density</th>\n",
              "      <th>pH</th>\n",
              "      <th>sulphates</th>\n",
              "      <th>alcohol</th>\n",
              "      <th>quality</th>\n",
              "      <th>kfold</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>6.5</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.06</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.063</td>\n",
              "      <td>29.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.99489</td>\n",
              "      <td>3.38</td>\n",
              "      <td>0.83</td>\n",
              "      <td>10.3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8.8</td>\n",
              "      <td>0.590</td>\n",
              "      <td>0.18</td>\n",
              "      <td>2.9</td>\n",
              "      <td>0.089</td>\n",
              "      <td>12.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>0.99738</td>\n",
              "      <td>3.14</td>\n",
              "      <td>0.54</td>\n",
              "      <td>9.4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6.8</td>\n",
              "      <td>0.490</td>\n",
              "      <td>0.22</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.071</td>\n",
              "      <td>13.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.99438</td>\n",
              "      <td>3.41</td>\n",
              "      <td>0.83</td>\n",
              "      <td>11.3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10.0</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.33</td>\n",
              "      <td>2.7</td>\n",
              "      <td>0.095</td>\n",
              "      <td>28.0</td>\n",
              "      <td>89.0</td>\n",
              "      <td>0.99840</td>\n",
              "      <td>3.22</td>\n",
              "      <td>0.68</td>\n",
              "      <td>10.0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6.6</td>\n",
              "      <td>0.840</td>\n",
              "      <td>0.03</td>\n",
              "      <td>2.3</td>\n",
              "      <td>0.059</td>\n",
              "      <td>32.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>0.99520</td>\n",
              "      <td>3.52</td>\n",
              "      <td>0.56</td>\n",
              "      <td>12.3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>9.5</td>\n",
              "      <td>0.460</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.7</td>\n",
              "      <td>0.092</td>\n",
              "      <td>14.0</td>\n",
              "      <td>44.0</td>\n",
              "      <td>0.99800</td>\n",
              "      <td>3.12</td>\n",
              "      <td>0.74</td>\n",
              "      <td>10.0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>8.3</td>\n",
              "      <td>0.430</td>\n",
              "      <td>0.30</td>\n",
              "      <td>3.4</td>\n",
              "      <td>0.079</td>\n",
              "      <td>7.0</td>\n",
              "      <td>34.0</td>\n",
              "      <td>0.99788</td>\n",
              "      <td>3.36</td>\n",
              "      <td>0.61</td>\n",
              "      <td>10.5</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>11.4</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.66</td>\n",
              "      <td>6.2</td>\n",
              "      <td>0.088</td>\n",
              "      <td>6.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.99880</td>\n",
              "      <td>3.11</td>\n",
              "      <td>0.99</td>\n",
              "      <td>13.3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>7.7</td>\n",
              "      <td>0.835</td>\n",
              "      <td>0.00</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0.081</td>\n",
              "      <td>6.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>0.99750</td>\n",
              "      <td>3.30</td>\n",
              "      <td>0.52</td>\n",
              "      <td>9.3</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>7.9</td>\n",
              "      <td>0.320</td>\n",
              "      <td>0.51</td>\n",
              "      <td>1.8</td>\n",
              "      <td>0.341</td>\n",
              "      <td>17.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>0.99690</td>\n",
              "      <td>3.04</td>\n",
              "      <td>1.08</td>\n",
              "      <td>9.2</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     fixed acidity  volatile acidity  citric acid  ...  alcohol  quality  kfold\n",
              "0              6.5             0.530         0.06  ...     10.3        3      0\n",
              "1              8.8             0.590         0.18  ...      9.4        2      0\n",
              "2              6.8             0.490         0.22  ...     11.3        3      0\n",
              "3             10.0             0.430         0.33  ...     10.0        2      0\n",
              "4              6.6             0.840         0.03  ...     12.3        4      0\n",
              "..             ...               ...          ...  ...      ...      ...    ...\n",
              "995            9.5             0.460         0.24  ...     10.0        3      4\n",
              "996            8.3             0.430         0.30  ...     10.5        2      4\n",
              "997           11.4             0.625         0.66  ...     13.3        3      4\n",
              "998            7.7             0.835         0.00  ...      9.3        2      4\n",
              "999            7.9             0.320         0.51  ...      9.2        3      4\n",
              "\n",
              "[1000 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4fCerPugxSJ",
        "outputId": "8c47515f-fede-4b29-c5af-c0d684d122dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "df.kfold.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    200\n",
              "3    200\n",
              "2    200\n",
              "1    200\n",
              "0    200\n",
              "Name: kfold, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cNJfqh9MhPMp"
      },
      "source": [
        "#You can use this process with almost all kinds of datasets. For example, when you have images, you can create a CSV with image id, image location and image labeland use the process above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3RQAguriw61"
      },
      "source": [
        "#Stratified k-fold cross-validation keeps the ratio of labels in each fold constant. So,in each fold, you will have the same 90% positive and 10% negative samples. Thus,whatever metric you choose to evaluate, it will give similar results across all folds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aczb-XUUjc2S"
      },
      "source": [
        "#It’s easy to modify the code for creating k-fold cross-validation to create stratified k-folds. We are only changing from model_selection.KFold to model_selection.StratifiedKFold and in the kf.split(...) function, we specify the target column on which we want to stratify. We assume that our CSV dataset has a column called “target” and it is a classification problem!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7LCRhunjwF3"
      },
      "source": [
        "# import pandas and model_selection module of scikit-learn\n",
        "import pandas as pd\n",
        "from sklearn import model_selection\n",
        "if __name__ == \"__main__\":\n",
        "  # Training data is in a csv file called train.csv\n",
        "  df = pd.read_csv(\"train.csv\")\n",
        "  # we create a new column called kfold and fill it with -1\n",
        "  df[\"kfold\"] = -1\n",
        "  # the next step is to randomize the rows of the data\n",
        "  df = df.sample(frac=1).reset_index(drop=True)\n",
        "  # fetch targets\n",
        "  y = df.quality.values\n",
        "  # initiate the kfold class from model_selection module\n",
        "  kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "  # fill the new kfold column\n",
        "  for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
        "    df.loc[v_, 'kfold'] = f\n",
        "    # save the new csv with kfold column\n",
        "    df.to_csv(\"train_folds.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBtpDyxUjLbr"
      },
      "source": [
        "#For the wine dataset, let’s look at the distribution of labels.\n",
        "═"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYwTKyqjg1zw",
        "outputId": "6772b72c-dbf2-4e7e-da9e-53e58c3db493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "b = sns.countplot(x='quality', data=df)\n",
        "b.set_xlabel(\"quality\", fontsize=20)\n",
        "b.set_ylabel(\"count\", fontsize=20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'count')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaEAAAEYCAYAAAD1bUl/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de1xUZf4H8A9yExYESrwACgTCklkRq2LeEjETV1QupbKDK23pvrxkbSn+0q0UF9m8vBRvtNus3JREQitbLBQTLZQVpVzG8GUojIGIeBkCh4D5/eGP+TXNoAwMPMzweb9evTbPeZ5zvk878PE858xzzFQqlQpEREQC9BFdABER9V4MISIiEoYhREREwjCEiIhIGIYQEREJYyG6AGNz/vx5WFtbiy6DiMioKJVKPP3001rbGUJ6sra2hp+fn+gyiIiMikwm07md03FERCQMQ4iIiIRhCBERkTAMISIiEoYhREREwjCEiIhIGIYQEREJwxAiIiJhGEJERCQMQ4h6PWWTUnQJejPGmol04bI91OtZW1hjbOJY0WXo5dTSU6JLIDIIXgkREZEwDCEiIhKGIURERMIwhIiISBiGEBERCdMjQ+jQoUPw9fWFr68vMjMzdbbJy8uDRCJBQEAA/P39ERkZiezs7AceNzs7GxEREfD390dAQAAkEgny8vK6YghERNQOPS6EKisrsW7dOtja2rbZJi0tDYsWLUJpaSlCQ0MRGRmJ6upqxMbGIiEhQWefhIQExMbG4saNG4iMjERoaChKS0uxaNEipKWlddVwiIjoAXrU94RUKhVWrVoFR0dHTJkyBVKpVKuNXC5HQkICHB0dkZWVBTc3NwDA4sWLERERAalUiueffx7+/v7qPkVFRZBKpRg6dCgOHDgABwcHAMDLL7+M8PBwJCQk4LnnnlMfi4iIukePuhJKSUlBQUEB4uPj27wSysrKQmNjI6KiojRCw8HBAQsXLgQAZGRkaPRp/fOiRYvUAQQAbm5umDdvHhobG/Hxxx8bejhEPUKL0rhWVzC2eqlzesyV0OXLl7Fp0yZER0dj5MiRKCgo0Nmudfv48eO19k2YMEGjTXv77Ny5EwUFBVi2bFmnxkDUE/WxtsZXEyaKLqPdJp74SnQJ1I16RAg1NTXhrbfewuDBg/HGG288sG1ZWRkAwMPDQ2vfgAEDYGtri6qqKjQ0NMDGxgb19fW4fv06bG1tMWDAAK0+7u7uAIArV660q1alUgmZTNautmQc/Pz8RJfQIe39HBrj+Pgz1nv0iBDasWMHZDIZ9u7di759+z6wbV1dHQDA3t5e5347OzvU19dDoVDAxsYGCoXige1bt9+9e7ddtVpbWxvlDzWZHlP+HJry2Hqrtv5iIfyeUHFxMZKSkrBgwQKNhwmIiMj0CQ2hpqYmrFixAh4eHli+fHm7+tjZ2QGA+grn1359pdT6v221b93er1+/9hdOREQGIXQ6rr6+Xn0vZsSIETrbrF69GqtXr0Z0dDTefvtteHp64tatW7hy5QqcnJw02lZXV6O+vh6DBg2CjY0NAMDW1hYDBw7E9evXUV1drXVf6OrVqwB032MiIqKuJTSErKysEBERoXNfSUkJSkpKEBAQAE9PT/VUXWBgIIqKipCfn681fXfixAl1m18KDAzEoUOHkJ+fj/Dw8Hb1ISKiric0hPr27Yv169fr3JeYmIiSkhLMnj0bkZGR6u1hYWH45z//ifT0dISFham/K3Tnzh0kJSUBAObMmaNxrDlz5uDQoUPYvXs3goOD1d8Vksvl2Lt3L6ysrBAWFtYVQyQiogfoEU/H6WPIkCFYsWIF4uLiEB4ejpCQEFhaWuLIkSOoqqpCTEyM1hXSM888gwULFuBf//oXQkNDMXXqVPz888/4/PPPcfv2baxZs4arJRARCWB0IQQAEokErq6ukEqlOHjwIFQqFby8vLB8+XLMnj1bZ5/Y2Fj4+PggPT0d+/fvh5mZGYYPH46XX34ZkyZN6uYREBERAJipVCqV6CKMiUwm43cYTNDYxLGiS9DLqaWn9GrPFRNItLZ+dwr/nhAREfVeDCEiIhKGIURERMIwhIiISBiGEBERCcMQIiIiYRhCREQkDEOIiIiEYQgREZEwDCEiIhKGIURERMIwhIiISBiGEBERCcMQIiIiYRhCREQkDEOIiIiEYQgREZEwDCEiIhKGIURERMIwhIiISBiGEBERCcMQIiIiYRhCREQkDEOIiIiEYQgREZEwDCEiIhKGIURERMIwhIiISBiGEBERCcMQIiIiYRhCREQkDEOIiIiEYQgREZEwFqILAID3338fFy5cwJUrV3Dr1i307dsXLi4uCA4ORlRUFJycnLT6FBUVYdeuXSguLsa9e/fg7u6O8PBwSCQSmJub6zxPXl4epFIpSkpK0NLSAm9vb8ybNw+zZ8/u6iESEZEOPeJKKDk5GQ0NDXj22WcRHR2NGTNmwNzcHImJiQgNDUVlZaVG+9zcXPzhD3/Af/7zH3VQ/fzzz4iPj8frr7+u8xxpaWlYtGgRSktLERoaisjISFRXVyM2NhYJCQndMUwiIvqVHnEldPbsWVhbW2tt37JlC3bv3o2kpCS8++67AIC6ujqsWbMGffr0QUpKCkaMGAEAWL58OebPn48jR47g8OHDmD59uvo4crkcCQkJcHR0RFZWFtzc3AAAixcvRkREBKRSKZ5//nn4+/t3/WCJiEitR1wJ6QogAJg2bRoA4OrVq+ptOTk5qK2txfTp09UB1HqM1157DQCwb98+jeNkZWWhsbERUVFR6gACAAcHByxcuBAAkJGRYZjBEBFRu/WIEGrLsWPHAAC+vr7qbQUFBQCA8ePHa7UfOXIkbGxscO7cOTQ2Nrarz4QJEzTaEBFR9+kR03GtPvzwQ9TX10OhUODChQs4e/YsfH198eqrr6rblJWVAQA8PDy0+ltYWMDNzQ2XLl1CRUUFvLy8HtpnwIABsLW1RVVVFRoaGmBjY2P4gRERkU49KoSkUilqamrUfx4/fjw2bNiARx55RL2trq4OAGBvb6/zGHZ2dgCAu3fv6tWnNfweFkJKpRIymawdoyFj4efnJ7qEDmnv59AYx8efsd6jR4XQqVOnAAA1NTU4d+4cNm7ciFmzZiEpKQnDhw8XXN191tbWRvlDTabHlD+Hpjy23qqtv1j0yHtC/fv3x5QpUyCVSnH79m2sXLlSva/1SkehUOjs23rV069fP737tHWlREREXaNHhlArV1dXeHt749KlS6itrQUAeHp6AgCuXLmi1b6pqQlyuRwWFhYYMmSIevuD+lRXV6O+vh6DBg3i/SAiom7Wo0MIuB8SANSrIAQGBgIA8vPztdoWFhaioaEB/v7+sLKyUm9/UJ8TJ05otCEiou4jPITKysp0TpO1tLRgy5YtuHnzJvz9/eHg4AAAeOGFF+Dk5ITDhw/ju+++U7dXKpXYunUrAGDu3LkaxwoLC4OVlRXS09Mhl8vV2+/cuYOkpCQAwJw5cww+NiIiejDhDyZ89dVX2Lx5MwICAuDm5gZHR0fU1NSgsLAQFRUVcHZ2RlxcnLq9nZ0d4uLisGzZMkRHRyMkJAQODg44duwYysrKMHXqVISEhGicY8iQIVixYgXi4uIQHh6OkJAQWFpa4siRI6iqqkJMTAxXSyAiEkB4CD377LMoLy/H2bNnUVJSon5M2sPDAzNnzoREIoGjo6NGn+DgYKSmpmL37t344osvoFQq4e7ujlWrVkEikcDMzEzrPBKJBK6urpBKpTh48CBUKhW8vLywfPlyLmBKRCSImUqlUokuwpjIZDI+PmqCxiaOFV2CXk4tPaVX+68mTOyiSgxv4omvRJdAXaCt353C7wkREVHvxRAiIiJhGEJERCSMXiH0448/qlcXaEtdXR1+/PHHThVFRES9g14hNHnyZCQnJz+wTWpqKiZPntypooiIqHfQK4RUKhX4MB0RERmKwe8J1dTUcA02IiJql4d+WfXgwYMaf7548aLWNgBobm5GZWUlPvnkE/j4+BiuQiIiMlkPDaHY2Fj1CgRmZmY4evQojh49qtWudZrOxsYGS5YsMXCZRERkih4aQvHx8QDuh8z//M//IDg4WOeDB3369IGjoyP8/f013uVDRETUloeG0C/XVcvOzkZwcDBmzZrVpUUREVHvoNcCpqmpqV1VBxER9UJcMYGIiITR+1UOZ86cwYcffohvv/0Wd+/eRUtLi1YbMzMzlJSUGKRAIiIyXXqF0PHjx7F48WI0NzfDxcUFnp6e6tduExER6UuvEEpMTISFhQWSkpIwbty4rqqJiIh6Cb3uCV26dAkhISEMICIiMgi9QsjW1hYODg5dVQsREfUyeoXQmDFjcP78+a6qhYiIehm9QujNN99EeXk5du7cydW0iYio0/R6MGH79u3w9vZGYmIisrKy4OfnB3t7e612ZmZm+Nvf/mawIomIyDTpFULZ2dnqf7927RquXbumsx1DiIiI2kOvENK1ejYREVFH6RVCrq6uXVUHERH1Qlw7joiIhNHrSujHH39sd1sXFxe9iyEiot5FrxAKCgpSv2X1QbiAKRERtYdeITRr1iydIXT37l3IZDL8+OOPGDVqFO8dERFRu+gVQhs2bGhzX0tLC3bu3ImMjAwkJCR0ujAiIjJ9BnswoU+fPliyZAlcXV2xceNGQx2WiIhMmMGfjvP398epU6cMfVgiIjJBBg+hO3fuoKGhwdCHJSIiE2TQEPr666/x+eefY9iwYYY8LBERmSi9HkyIjo7Wub25uRmVlZWorKwEACxevLjzlRERkcnTK4TOnDmjc7uZmRn69euHcePGISYmBmPGjGn3MW/duoXc3FwcP34cpaWluH79OiwtLeHj44OwsDCEh4ejTx/tC7aioiLs2rULxcXFuHfvHtzd3REeHg6JRAJzc3Od58rLy4NUKkVJSQlaWlrg7e2NefPmYfbs2e2ul4iIDEevELp48aLBC8jJycG7774LZ2dnjB49Gi4uLqipqcGXX36J1atXIz8/H1u3btX4flJubi6WLVsGa2trTJs2DQ4ODsjLy0N8fDyKioqwbds2rfOkpaVh3bp1cHR0RGhoKCwtLXHkyBHExsaitLQUK1euNPjYiIjowcxUgt9O980336ChoQHPPfecxhXPjRs3EBkZicrKSmzbtg1Tp04FANTV1WHKlClQKBTYt28fRowYAQBQKpWYP38+zp07h82bN2P69OnqY8nlckybNg22trbIysqCm5sbgPsPUURERKC8vBwZGRnw9/d/aL0ymQx+fn6G/E9APcDYxLGiS9DLqaX6PYH61YSJXVSJ4U088ZXoEqgLtPW7s1MPJtTV1aGyshJ1dXUdPsaYMWMQFBSkNeXm7OyMOXPmANCcBszJyUFtbS2mT5+uDiAAsLa2xmuvvQYA2Ldvn8axsrKy0NjYiKioKHUAAYCDgwMWLlwIAMjIyOjwGIiIqGP0mo4DgKamJkilUmRmZkIul6u3u7m5ITIyEjExMbCw0Puwuov7v+P88h5PQUEBAGD8+PFa7UeOHAkbGxucO3cOjY2NsLKyemifCRMmaLQhIqLuo9eVUGNjI2JiYrBlyxZcu3YNgwcPxpNPPonBgwfj2rVr2LJlCxYsWIDGxsZOF9bU1IRDhw4B0AyPsrIyAICHh4dWHwsLC7i5uaGpqQkVFRXt6jNgwADY2tqiqqqK328iIupmel2y7NmzB2fOnMFzzz2H2NhYjV/q5eXl2LBhA/Ly8rBnzx68+uqrnSps06ZNKC0txcSJEzVCqHXqz97eXmc/Ozs7APcXVdWnT319PRQKBWxsbB5Yl1KphEwma/9AqMcz1nt87f0cGuP4+DPWe+gVQp9++imGDRuGnTt3at3DGTp0KLZv346ZM2fi008/7VQIpaSkQCqV4rHHHsPf//73Dh+nK1hbWxvlDzWZHlP+HJry2Hqrtv5iodd0XHl5OSZMmKDzezvA/UVMJ0yYgPLycv0r/D9paWlYv349vL29kZKSAkdHR439rVc6CoVCZ//Wq55+/frp3aetKyUiIuoaeoWQpaUl6uvrH9imoaGhww8m7NmzB+vWrYOPjw9SUlLg7Oys1cbT0xMAcOXKFa19TU1NkMvlsLCwwJAhQ9rVp7q6GvX19Rg0aNBDp+KIiMiw9AohX19fHDlyBLW1tTr319bW4siRI/jtb3+rdyEffPAB4uPj4efnh+TkZDz66KM62wUGBgIA8vPztfYVFhaioaEB/v7+6ifjHtbnxIkTGm2IiKj76BVCUVFRqK2tRUREBDIzM1FRUYF79+6hoqICWVlZePHFF1FbW4uoqCi9itixYwc2bdqE4cOHY8+ePXjkkUfabPvCCy/AyckJhw8fxnfffaferlQqsXXrVgDA3LlzNfqEhYXBysoK6enpGo+V37lzB0lJSQCg/k4SERF1H73mzUJCQnDx4kV88MEH+Otf/6q1X6VS4U9/+hNCQkLafczs7Gxs27YN5ubm+N3vfofU1FStNq6urggLCwNw//5OXFwcli1bhujoaISEhMDBwQHHjh1DWVkZpk6dqnX+IUOGYMWKFYiLi0N4eDhCQkLUy/ZUVVUhJiamXaslEBGRYel98+aNN95AUFAQDhw4gJKSEtTV1cHOzg6PP/44wsPD9f5l3npl0tzcjOTkZJ1tRo0apQ4hAAgODkZqaip2796NL774AkqlEu7u7li1ahUkEonGOnOtJBIJXF1dIZVKcfDgQahUKnh5eWH58uVcwJSISBDha8cZG64dZ5q4dlzPwbXjTJNB1o7797//jejoaFy/fl3n/uvXr2P+/Pn44osvOlYlERH1KnqF0IEDB6BQKDBw4ECd+wcOHAiFQoHMzEyDFEdERKZNrxD6/vvv8cQTTzywzYgRI/D99993qigiIuod9AqhO3fuPPDxaQBwdHTErVu3OlUUERH1DnqFkJOTE65evfrANlevXtVYMoeIiKgteoXQM888g2PHjuHy5cs691++fBlHjx5FQECAQYojIiLTplcIxcTEoLm5GfPmzUNKSgrKyspQX1+PsrIyJCcnY968eWhpacHLL7/cVfUSEZEJ0evLqk8++STeeecdrF27FvHx8YiPj9fYb25ujnfffRdPPfWUQYskIiLTpPeKCS+++CICAgKwd+9eFBcXQ6FQwN7eHk8//TTmzp0LLy+vrqiTiIhMUIfeueDl5YU1a9YYuhYiIupl9LonREREZEgMISIiEoYhREREwjCEiIhIGIYQEREJwxAiIiJhGEJERCQMQ4iIiIRhCBERkTAMISIiEoYhREREwjCEiIhIGIYQEREJwxAiIiJhGEJERCQMQ4iIiIRhCBERkTAMISIiEoYhREREwjCEiIhIGIYQERmtpp+bRZegN2OsuStZiC6AiKijLCzNsf0vn4ouQy9LNs0QXUKPwishIiIShiFERETCCJ+Oy8nJQWFhIWQyGS5evIiffvoJM2bMwMaNG9vsU1RUhF27dqG4uBj37t2Du7s7wsPDIZFIYG5urrNPXl4epFIpSkpK0NLSAm9vb8ybNw+zZ8/uqqEREdFDCA+hXbt24eLFi7C1tcWgQYPwww8/PLB9bm4uli1bBmtra0ybNg0ODg7Iy8tDfHw8ioqKsG3bNq0+aWlpWLduHRwdHREaGgpLS0scOXIEsbGxKC0txcqVK7tqeERE9ADCQ2jVqlUYNGgQ3N3dcebMGURHR7fZtq6uDmvWrEGfPn2QkpKCESNGAACWL1+O+fPn48iRIzh8+DCmT5+u7iOXy5GQkABHR0dkZWXBzc0NALB48WJERERAKpXi+eefh7+/f9cOlIiItAi/JxQYGAgPDw+YmZk9tG1OTg5qa2sxffp0dQABgLW1NV577TUAwL59+zT6ZGVlobGxEVFRUeoAAgAHBwcsXLgQAJCRkWGIoRARkZ6Eh5A+CgoKAADjx4/X2jdy5EjY2Njg3LlzaGxsbFefCRMmaLQhIqLuJXw6Th9lZWUAAA8PD619FhYWcHNzw6VLl1BRUQEvL6+H9hkwYABsbW1RVVWFhoYG2NjYPLQGpVIJmUzW8UFQj+Pn5ye6hA5p7+fQGMdnymMD2j++3sCoQqiurg4AYG9vr3O/nZ0dAODu3bt69amvr4dCoWhXCFlbWxvtB59Miyl/Dk15bIDpj0+XtoLXqKbjiIjItBhVCLVe6SgUCp37W696+vXrp3eftq6UiIio6xhVCHl6egIArly5orWvqakJcrkcFhYWGDJkSLv6VFdXo76+HoMGDWrXVBwRERmWUYVQYGAgACA/P19rX2FhIRoaGuDv7w8rK6t29Tlx4oRGGyIi6l5GFUIvvPACnJyccPjwYXz33Xfq7UqlElu3bgUAzJ07V6NPWFgYrKyskJ6eDrlcrt5+584dJCUlAQDmzJnTDdUTEdGvCX86Ljc3F7m5uQCAGzduAADOnz+P2NhYAICTk5N6WR07OzvExcVh2bJliI6ORkhICBwcHHDs2DGUlZVh6tSpCAkJ0Tj+kCFDsGLFCsTFxSE8PBwhISHqZXuqqqoQExPD1RKIiAQRHkIymQzZ2dka2yoqKlBRUQEAcHV11VjbLTg4GKmpqdi9eze++OILKJVKuLu7Y9WqVZBIJDpXXpBIJHB1dYVUKsXBgwehUqng5eWF5cuXcwFTIiKBhIfQ0qVLsXTpUr36BAQE4B//+IdefYKCghAUFKRXHyIi6lpGdU+IiIhMC0OIiIiEYQgREZEwDCEiIhKGIURERMIwhIiISBiGEBERCcMQIiIiYRhCREQkDEOIiIiEYQgREZEwDCEiIhKGIURERMIwhIiISBiGEBERCcMQIiIiYRhCREQkDEOIiIiEYQgREZEwDCEiIhKGIURERMIwhIiISBiGEBERCcMQIiIiYRhC9FCqJqXoEvRmjDUT9UYWogugns/Mwhrla0eILkMvQ//6negSiKgdeCVERETCMISIiEgYhhAREQnDECIiImEYQkREJAxDiIiIhGEIERH1UE2NjaJL0Ju+Nfeq7wlVVVVh69atyM/Px+3btzFgwABMnjwZS5YsgYODg+jyiIg0WFhZYf0fIkSXoZe30w7o1b7XhFB5eTnmzJmDmzdvYvLkyXjsscfw7bffIiUlBfn5+di3bx+cnJw6dGzlz82wtjQ3cMVdyxhrJiLT02tC6L333sPNmzexevVqSCQS9fb4+Hjs2bMHW7Zswdq1azt0bGtLcwS8lWKoUrvF2fejRZdARNQ77gmVl5fj5MmTcHV1RVRUlMa+pUuXwtbWFp988gnq6+sFVUhE1Dv1ihA6ffo0AGDcuHHo00dzyHZ2dnjmmWfQ0NCA4uJiEeUREfVavSKEfvjhBwCAh4eHzv3u7u4AgLKysu4qiYiI0EvuCdXV1QEA7O3tde5v3a5QKB56LKVSCZlMprU9LWZkJyrsfrrG8ECR+7umkC6i7/j+GfzPLqqka+g7vgFJu7uoEsPTd2yT/+TdRZV0DX3HF/b2ui6qpGu0NT6lUvfrVXpFCBnS008/LboEIiKT0Sum4+zs7AC0faXTur2tKyUiIuoavSKEHnvsMQDAlStXdO6/evUqAMDT07O7SiIiIvSSEBo9ejQA4OTJk2hpadHYV1dXh6KiItjY2OCpp54SUR4RUa/VK0Jo6NChGDduHK5du4b09HSNfYmJiaivr0doaChsbW0FVUhE1DuZqVQqlegiusOvl+3x8vJCcXExTp8+DQ8PD2RkZHR42R4iIuqYXhNCAFBZWYlt27apFzB1dnZGcHAwFzAlIhKkV4UQERH1LPyeUA9kqq+cyMnJQWFhIWQyGS5evIiffvoJM2bMwMaNG0WX1mm3bt1Cbm4ujh8/jtLSUly/fh2Wlpbw8fFBWFgYwsPDtZaMMjbvv/8+Lly4gCtXruDWrVvo27cvXFxcEBwcjKioKJObzj506BBWrFgBAIiLi0NkZKTgijonKCgI165d07mvf//+OHXqVDdXdB9DqIfpyldOiLZr1y5cvHgRtra2GDRokHo5JVOQk5ODd999F87Ozhg9ejRcXFxQU1ODL7/8EqtXr0Z+fj62bt0KMzMz0aV2WHJyMh5//HE8++yzePTRR9HQ0IDz588jMTERH330Efbv34/BgweLLtMgKisrsW7dOtja2prUwsb29vaYP3++1nahD2WpqEeJiYlR+fj4qFJSUjS2/+1vf1P5+Pio1qxZI6iyzvvmm29UZWVlqpaWFlVBQYHKx8dH9Ze//EV0WQbx9ddfq44ePapqbm7W2F5dXa2aOHGiysfHR5WTkyOoOsO4d++ezu2bN29W+fj4qN55553uLaiLtLS0qObPn6+aPHmyasOGDSofHx/V/v37RZfVaZMmTVJNmjRJdBlajHt+wMSY+isnAgMD4eHhYdRXA20ZM2YMgoKCtKbcnJ2dMWfOHADAmTNnRJRmMNbW1jq3T5s2DcD/f+nb2KWkpKCgoADx8fH82kY3YAj1IHzlhGmysLg/621ubppvsj127BgAwNfXV3AlnXf58mVs2rQJ0dHRGDnSuBYlbo/GxkYcOnQIu3fvRnJyMgoKCtDc3Cy0Jt4T6kHa88qJkydPoqysDGPGjOnGyqijmpqacOjQIQDA+PHjBVdjGB9++CHq6+uhUChw4cIFnD17Fr6+vnj11VdFl9YpTU1NeOuttzB48GC88cYbosvpEjdu3FA/bNHKzc0N8fHxGDVqlJCaGEI9iCFfOUE9w6ZNm1BaWoqJEyeaTAhJpVLU1NSo/zx+/Hhs2LABjzzyiMCqOm/Hjh2QyWTYu3cv+vbtK7ocgwsLC0NAQACGDRuG3/zmN6ioqEBaWhr279+PV155BR999BF++9vfdntdnI4j6iIpKSmQSqV47LHH8Pe//110OQZz6tQpfP/99zh16hS2b9+OiooKzJo1C//9739Fl9ZhxcXFSEpKwoIFC+Dv7y+6nC6xZMkSjBkzBv3794eNjQ18fHywdu1aLFiwAPfu3UNiYqKQuhhCPQhfOWE60tLSsH79enh7eyMlJQWOjo6iSzK4/v37Y8qUKZBKpbh9+zZWrlwpuqQOaWpqwooVK+Dh4YHly5eLLqfbtT4485///EfI+Tkd1+9a3DoAAAfxSURBVIPwlROmYc+ePYiPj4ePjw/27NmDRx99VHRJXcrV1RXe3t6QyWSora01umm5+vp69c/ciBEjdLZZvXo1Vq9ejejoaLz99tvdWF3Xa/3/S9RTtwyhHuTXr5z45RNyfOWEcfjggw+wadMm+Pn5QSqVGt0v5I6qrq4GYJxPAFpZWSEiIkLnvpKSEpSUlCAgIACenp4mOVV3/vx5AMCQIUOEnJ8h1IO0vnLi5MmTSE9Ph0QiUe9rfeXESy+9xO8u9FA7duzAtm3bMHz4cEilUpOagisrK0P//v21poJbWlqwdetW3Lx5E/7+/ka5rFTfvn2xfv16nfsSExNRUlKC2bNnG/WyPZcvX8bgwYO1fnfI5XKsW7cOABAaGiqiNIZQT/POO+9gzpw5iIuLwzfffKP1yonXX39ddIkdlpubi9zcXAD3HxUF7v8tLDY2FgDg5ORktPcVsrOzsW3bNpibm+N3v/sdUlNTtdq4uroiLCxMQHWd99VXX2Hz5s0ICAiAm5sbHB0dUVNTg8LCQlRUVMDZ2RlxcXGiy6Q2fP7555BKpRg5ciRcXFzUT8cdP34cSqUSEydORExMjJDaGEI9zNChQ5GVlaV+5cSJEyfg7OyM6Ohoo1/AVCaTITs7W2NbRUUFKioqANz/JW2sISSXywEAzc3NSE5O1tlm1KhRRhtCzz77LMrLy3H27FmUlJRAoVDAxsYGHh4emDlzJiQSiUld+Zma0aNHo6ysDCUlJSgqKkJDQwPs7e0REBCAmTNnYubMmcJWMuGrHIiISBg+ok1ERMIwhIiISBiGEBERCcMQIiIiYRhCREQkDEOIiIiEYQgREZEwDCEiE3X69Gn4+vpqLdEvkUhM4i2oZBoYQkTUZmARdTUu20PUyyQkJKChoUF0GUQAGEJEvY6Li4voEojUOB1HZCAqlQppaWmYPn06RowYgfHjx2Pt2rVQKBQICgpCUFCQum1iYiJ8fX1x+vRprePI5XL4+vqqVxdvVVZWho0bNyIsLAyBgYF44oknMGnSJKxZswZVVVXtrvPX94RiY2MRHR0NANi+fTt8fX3V/5w+fRoZGRnw9fXF9u3bdR7vxo0bGD58OGbMmNHuGoha8UqIyEDWr1+P1NRUODs746WXXoKFhQWOHj2K4uJiNDY2wsrKqlPH//LLL5GRkYHRo0fjmWeegaWlJS5duoTMzEzk5eUhKysLAwcO1Pu4wcHBAO6/jmLUqFEYNWqUep+rqyueeOIJvP/++zhw4AD+/Oc/a724LisrC01NTXjppZc6NT7qnRhCRAZQVFSE1NRUDB06FJmZmerXGrz++uuIjo7GjRs34Orq2qlzzJw5E3/84x+1wuzkyZN45ZVXsHPnTrz33nt6Hzc4OBj29vbqEFq6dKnOc6enp+PEiROYNGmSertKpUJmZiZsbGwwc+ZM/QdFvR6n44gM4OOPPwYALFq0SOO9OtbW1njjjTcMco6BAwfqvJoaN24cvL29cfLkSYOcR5e5c+cCAD766CON7SdPnoRcLse0adO03rpK1B68EiIygJKSEgDQmMpqFRAQoDWF1REqlQqffPIJsrOzcfHiRdy9exfNzc3q/ZaWlp0+R1uGDRuGkSNH4sSJE6isrMTgwYMBAPv37wfw/yFFpC+GEJEBKBQKAMCjjz6qtc/CwgJOTk6dPkd8fDySk5Ph7OyMcePGYeDAgejbty+A+/dzrl271ulzPMi8efNQWFiIzMxMLFu2DDdu3MCxY8fg5+eHJ598skvPTaaLIURkAK1TUTdv3oStra3GvqamJty6dQuDBg1Sb2t9lfIvr2RatQbaL928eROpqanw8fHBvn37YGdnp7H/s88+6/QYHmbKlCno378/Dhw4gMWLF/OBBDII3hMiMoDHH38cAHDmzBmtfWfPntUKGwcHBwBAZWWlVvsLFy5obauoqEBLSwvGjh2rFUBVVVWQy+Udrh2AerpQVyi2srS0REREBK5fv468vDxkZmbC1taWj2ZTpzCEiAxg9uzZAIDdu3fj9u3b6u1KpRKbN2/Wat86ffXxxx+jqalJvb2yshI7duzQat/6ZN2vA+2nn37C6tWrNY7REa0PU+gKxV966aWXYG5ujrVr10Iul2PGjBlaoUikD07HERlAQEAAJBIJUlNT8fvf/x4vvPCC+ntC/fr1g7Ozs0b7p556CiNHjkRhYSEiIyMRGBiImpoa5OXlYdy4cVph4OzsjOnTp+Pw4cOYNWsWxo4dC4VCga+//hpWVlbw8/ODTCbrcP2enp4YOHAgDh8+DAsLC7i4uMDMzAwzZ87UeLTcxcUFEydOxLFjxwCAU3HUabwSIjKQt99+G2vWrIG9vT0yMjLw2WefYdy4cdizZ4/OR6t37tyJyMhIVFVVITU1FTKZDG+99RbefPNNncdfv349Fi1ahHv37iE9PR0nT57Ec889h4yMjE4/Hm1ubo7t27cjICAAOTk5SExMxNatW3VO84WHhwMAnnjiCQwfPrxT5yUyU6lUKtFFEJm61iV7Wq8gjFliYiK2b9+OuLg4REZGii6HjByvhIio3erq6pCRkQFHR0f8/ve/F10OmQDeEyKihzp+/Dj++9//Ii8vDzU1NVi5ciVsbGxEl0UmgCFERA+Vk5OD7Oxs9O/fHwsXLsQf//hH0SWRieA9ISIiEob3hIiISBiGEBERCcMQIiIiYRhCREQkDEOIiIiE+V9leQOCqtC6DQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aHo-yRQkF5a"
      },
      "source": [
        "#Note that we continue on the code above. So, we have converted the target values.\n",
        "#Looking at figure 6 we can say that the quality is very much skewed. Some classes have a lot of samples, and some don’t have that many. If we do a simple k-fold, we won’t have an equal distribution of targets in every fold. Thus, we choose stratified k-fold in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5AN13jVl_7e"
      },
      "source": [
        "#The rule is simple. If it’s a  standard classification problem, choose stratified k-fold  blindly. \n",
        "\n",
        " \n",
        "\n",
        "#But  what  should  we  do  if  we  have  a  large  amount  of  data?  Suppose  we  have  1 million samples. A 5 fold cross-validation would mean training on 800k samples and validating on 200k. Depending on which algorithm we choose, training and even validation can be very expensive for a dataset which is of this size. In these cases, we can opt for a hold-out based validation. \n",
        "\n",
        " \n",
        "\n",
        "#The process for creating the hold-out remains the same as stratified k-fold. For a dataset which has 1 million samples, we can create ten folds instead of 5 and keep one of those folds as hold-out. This means we will have 100k samples in the hold-out, and we will always calculate loss, accuracy and other metrics on this set and train on 900k samples.\n",
        "\n",
        "#Hold-out  is  also  used  very  frequently  with  time-series  data.  Let’s  assume  the problem we are provided with is predicting sales of a store for 2020, and you are provided all the data from 2015-2019. In this case, you can select all the data for 2019 as a hold-out and train your model on all the data from 2015 to 2018."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo0UMj_nnfBE"
      },
      "source": [
        "#In many cases, we have to deal with small datasets and creating big validation sets means losing a lot of data for the model to learn. In those cases, we can opt for a  type of k-fold cross-validation where k=N, where N is the number of samples in the dataset.  This  means  that  in  all  folds  of  training,  we  will  be  training  on  all  data samples except 1. The number of folds for this type of cross-validation is the same as the number of samples that we have in the dataset.  \n",
        "\n",
        " \n",
        "#One should note that this type of cross-validation can be costly in terms of the time it takes if the model is not fast enough, but since it’s only preferable to use this cross-validation for small datasets, it doesn’t matter much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRjTwFOOoeKB"
      },
      "source": [
        "#Now we can move to regression. The good thing about regression problems is that\n",
        "we can use all the cross-validation techniques mentioned above for regression\n",
        "problems except for stratified k-fold. That is we cannot use stratified k-fold directly,\n",
        "but there are ways to change the problem a bit so that we can use stratified k-fold\n",
        "for regression problems. Mostly, simple k-fold cross-validation works for any\n",
        "regression problem. However, if you see that the distribution of targets is not\n",
        "consistent, you can use stratified k-fold."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_F3IwOUoxP1"
      },
      "source": [
        "#To use stratified k-fold for a regression problem, we have first to divide the target\n",
        "into bins, and then we can use stratified k-fold in the same way as for classification\n",
        "problems. There are several choices for selecting the appropriate number of bins. If\n",
        "you have a lot of samples( > 10k, > 100k), then you don’t need to care about the\n",
        "number of bins. Just divide the data into 10 or 20 bins. If you do not have a lot of\n",
        "samples, you can use a simple rule like Sturge’s Rule to calculate the appropriate\n",
        "number of bins.\n",
        "Sturge’s rule:\n",
        "Number of Bins = 1 + log2(N)\n",
        "Where N is the number of samples you have in your dataset. This function is plotted\n",
        "in Figure 8.\n",
        "Figure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSMhHRs4o7-2"
      },
      "source": [
        "#Let’s make a sample regression dataset and try to apply stratified k-fold as shown\n",
        "in the following python snippet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aCZHBxbjPRT"
      },
      "source": [
        "# stratified-kfold for regression\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "from sklearn import model_selection\n",
        "\n",
        "def create_folds(data):\n",
        "  # we create a new column called kfold and fill it with -1\n",
        "  data[\"kfold\"] = -1\n",
        "  # the next step is to randomize the rows of the data\n",
        "  data = data.sample(frac=1).reset_index(drop=True)\n",
        "  # calculate the number of bins by Sturge's rule\n",
        "  # I take the floor of the value, you can also\n",
        "  # just round it\n",
        "  num_bins = np.floor(1 + np.log2(len(data)))\n",
        "  # bin targets\n",
        "  data.loc[:, \"bins\"] = pd.cut(\n",
        "  data[\"target\"], bins=num_bins, labels=False\n",
        "  )\n",
        "  # initiate the kfold class from model_selection module\n",
        "  kf = model_selection.StratifiedKFold(n_splits=5)\n",
        "  # fill the new kfold column\n",
        "  # note that, instead of targets, we use bins!\n",
        "  for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n",
        "    data.loc[v_, 'kfold'] = f\n",
        "    # drop the bins column\n",
        "    data = data.drop(\"bins\", axis=1)\n",
        "  # return dataframe with folds\n",
        "  return data\n",
        "if __name__ == \"__main__\":\n",
        "  # we create a sample dataset with 15000 samples\n",
        "  # and 100 features and 1 target\n",
        "  X, y = datasets.make_regression(\n",
        "  n_samples=15000, n_features=100, n_targets=1\n",
        "  )\n",
        "  # create a dataframe out of our numpy arrays\n",
        "  df = pd.DataFrame(\n",
        "  X,\n",
        "  columns=[f\"f_{i}\" for i in range(X.shape[1])]\n",
        "  )\n",
        "  df.loc[:, \"target\"] = y\n",
        "  # create folds\n",
        "  df = create_folds(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tfpMisQrOQD"
      },
      "source": [
        "from sklearn import datasets\n",
        "# we create a sample dataset with 15000 samples\n",
        "# and 100 features and 1 target\n",
        "X, y = datasets.make_regression(\n",
        "  n_samples=15000, n_features=100, n_targets=1\n",
        "  )\n",
        "  # create a dataframe out of our numpy arrays\n",
        "df = pd.DataFrame(\n",
        "  X,\n",
        "  columns=[f\"f_{i}\" for i in range(X.shape[1])]\n",
        "  )\n",
        "df.loc[:, \"target\"] = np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNCDQjDrrXs2",
        "outputId": "162c5a0c-bc1c-4770-970c-3e643095e30b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f_0</th>\n",
              "      <th>f_1</th>\n",
              "      <th>f_2</th>\n",
              "      <th>f_3</th>\n",
              "      <th>f_4</th>\n",
              "      <th>f_5</th>\n",
              "      <th>f_6</th>\n",
              "      <th>f_7</th>\n",
              "      <th>f_8</th>\n",
              "      <th>f_9</th>\n",
              "      <th>f_10</th>\n",
              "      <th>f_11</th>\n",
              "      <th>f_12</th>\n",
              "      <th>f_13</th>\n",
              "      <th>f_14</th>\n",
              "      <th>f_15</th>\n",
              "      <th>f_16</th>\n",
              "      <th>f_17</th>\n",
              "      <th>f_18</th>\n",
              "      <th>f_19</th>\n",
              "      <th>f_20</th>\n",
              "      <th>f_21</th>\n",
              "      <th>f_22</th>\n",
              "      <th>f_23</th>\n",
              "      <th>f_24</th>\n",
              "      <th>f_25</th>\n",
              "      <th>f_26</th>\n",
              "      <th>f_27</th>\n",
              "      <th>f_28</th>\n",
              "      <th>f_29</th>\n",
              "      <th>f_30</th>\n",
              "      <th>f_31</th>\n",
              "      <th>f_32</th>\n",
              "      <th>f_33</th>\n",
              "      <th>f_34</th>\n",
              "      <th>f_35</th>\n",
              "      <th>f_36</th>\n",
              "      <th>f_37</th>\n",
              "      <th>f_38</th>\n",
              "      <th>f_39</th>\n",
              "      <th>...</th>\n",
              "      <th>f_61</th>\n",
              "      <th>f_62</th>\n",
              "      <th>f_63</th>\n",
              "      <th>f_64</th>\n",
              "      <th>f_65</th>\n",
              "      <th>f_66</th>\n",
              "      <th>f_67</th>\n",
              "      <th>f_68</th>\n",
              "      <th>f_69</th>\n",
              "      <th>f_70</th>\n",
              "      <th>f_71</th>\n",
              "      <th>f_72</th>\n",
              "      <th>f_73</th>\n",
              "      <th>f_74</th>\n",
              "      <th>f_75</th>\n",
              "      <th>f_76</th>\n",
              "      <th>f_77</th>\n",
              "      <th>f_78</th>\n",
              "      <th>f_79</th>\n",
              "      <th>f_80</th>\n",
              "      <th>f_81</th>\n",
              "      <th>f_82</th>\n",
              "      <th>f_83</th>\n",
              "      <th>f_84</th>\n",
              "      <th>f_85</th>\n",
              "      <th>f_86</th>\n",
              "      <th>f_87</th>\n",
              "      <th>f_88</th>\n",
              "      <th>f_89</th>\n",
              "      <th>f_90</th>\n",
              "      <th>f_91</th>\n",
              "      <th>f_92</th>\n",
              "      <th>f_93</th>\n",
              "      <th>f_94</th>\n",
              "      <th>f_95</th>\n",
              "      <th>f_96</th>\n",
              "      <th>f_97</th>\n",
              "      <th>f_98</th>\n",
              "      <th>f_99</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.464094</td>\n",
              "      <td>0.709491</td>\n",
              "      <td>-2.054099</td>\n",
              "      <td>-0.944574</td>\n",
              "      <td>0.870449</td>\n",
              "      <td>0.090552</td>\n",
              "      <td>0.895403</td>\n",
              "      <td>1.275740</td>\n",
              "      <td>-0.209319</td>\n",
              "      <td>0.952506</td>\n",
              "      <td>1.552691</td>\n",
              "      <td>-0.354468</td>\n",
              "      <td>1.632768</td>\n",
              "      <td>-2.154695</td>\n",
              "      <td>0.379040</td>\n",
              "      <td>-0.063702</td>\n",
              "      <td>-0.302846</td>\n",
              "      <td>-0.327819</td>\n",
              "      <td>-0.624518</td>\n",
              "      <td>-0.598670</td>\n",
              "      <td>-0.722252</td>\n",
              "      <td>0.221149</td>\n",
              "      <td>-0.991672</td>\n",
              "      <td>-0.712171</td>\n",
              "      <td>-0.465023</td>\n",
              "      <td>1.373615</td>\n",
              "      <td>-0.802585</td>\n",
              "      <td>-0.293797</td>\n",
              "      <td>-1.422767</td>\n",
              "      <td>1.814149</td>\n",
              "      <td>-1.075575</td>\n",
              "      <td>0.090813</td>\n",
              "      <td>1.098624</td>\n",
              "      <td>-1.088580</td>\n",
              "      <td>-0.150711</td>\n",
              "      <td>0.709782</td>\n",
              "      <td>-0.045016</td>\n",
              "      <td>-0.530930</td>\n",
              "      <td>-0.555162</td>\n",
              "      <td>-0.318084</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004090</td>\n",
              "      <td>-1.061833</td>\n",
              "      <td>-0.357245</td>\n",
              "      <td>-0.134812</td>\n",
              "      <td>-0.757400</td>\n",
              "      <td>-1.091686</td>\n",
              "      <td>0.155185</td>\n",
              "      <td>-0.793564</td>\n",
              "      <td>-0.359673</td>\n",
              "      <td>1.314810</td>\n",
              "      <td>0.369728</td>\n",
              "      <td>0.262699</td>\n",
              "      <td>2.198443</td>\n",
              "      <td>-0.992477</td>\n",
              "      <td>0.161088</td>\n",
              "      <td>-0.094004</td>\n",
              "      <td>-0.397900</td>\n",
              "      <td>0.142740</td>\n",
              "      <td>-0.595191</td>\n",
              "      <td>-1.258696</td>\n",
              "      <td>-0.222823</td>\n",
              "      <td>0.064831</td>\n",
              "      <td>-1.217372</td>\n",
              "      <td>0.685363</td>\n",
              "      <td>-0.731132</td>\n",
              "      <td>-0.123965</td>\n",
              "      <td>-1.007446</td>\n",
              "      <td>0.286478</td>\n",
              "      <td>-1.198299</td>\n",
              "      <td>-0.325732</td>\n",
              "      <td>1.060370</td>\n",
              "      <td>1.395708</td>\n",
              "      <td>-0.656979</td>\n",
              "      <td>0.496657</td>\n",
              "      <td>-0.707222</td>\n",
              "      <td>1.131116</td>\n",
              "      <td>0.262702</td>\n",
              "      <td>-0.364065</td>\n",
              "      <td>-1.883873</td>\n",
              "      <td>-122.301333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.048285</td>\n",
              "      <td>0.493355</td>\n",
              "      <td>0.502420</td>\n",
              "      <td>0.793302</td>\n",
              "      <td>-0.473695</td>\n",
              "      <td>-1.444286</td>\n",
              "      <td>-2.251987</td>\n",
              "      <td>-0.564807</td>\n",
              "      <td>-0.604647</td>\n",
              "      <td>0.653389</td>\n",
              "      <td>0.666112</td>\n",
              "      <td>0.066359</td>\n",
              "      <td>0.588879</td>\n",
              "      <td>-1.543148</td>\n",
              "      <td>-0.444312</td>\n",
              "      <td>1.233920</td>\n",
              "      <td>0.572427</td>\n",
              "      <td>-0.210645</td>\n",
              "      <td>2.080004</td>\n",
              "      <td>-0.637511</td>\n",
              "      <td>-0.079178</td>\n",
              "      <td>1.750777</td>\n",
              "      <td>-0.883928</td>\n",
              "      <td>1.340708</td>\n",
              "      <td>1.562441</td>\n",
              "      <td>1.632206</td>\n",
              "      <td>-0.765785</td>\n",
              "      <td>-2.006947</td>\n",
              "      <td>1.051170</td>\n",
              "      <td>1.132981</td>\n",
              "      <td>0.115894</td>\n",
              "      <td>-0.064618</td>\n",
              "      <td>-0.254337</td>\n",
              "      <td>-2.349168</td>\n",
              "      <td>-0.030532</td>\n",
              "      <td>3.113260</td>\n",
              "      <td>-0.536025</td>\n",
              "      <td>-0.896544</td>\n",
              "      <td>0.799158</td>\n",
              "      <td>0.255625</td>\n",
              "      <td>...</td>\n",
              "      <td>1.343337</td>\n",
              "      <td>1.838339</td>\n",
              "      <td>-1.078754</td>\n",
              "      <td>-1.398824</td>\n",
              "      <td>1.955873</td>\n",
              "      <td>0.956246</td>\n",
              "      <td>1.151652</td>\n",
              "      <td>0.297026</td>\n",
              "      <td>1.239645</td>\n",
              "      <td>-1.540468</td>\n",
              "      <td>0.308906</td>\n",
              "      <td>-0.431609</td>\n",
              "      <td>-0.300472</td>\n",
              "      <td>-0.451161</td>\n",
              "      <td>-0.659919</td>\n",
              "      <td>0.469477</td>\n",
              "      <td>0.062033</td>\n",
              "      <td>-1.659795</td>\n",
              "      <td>-1.874371</td>\n",
              "      <td>0.913445</td>\n",
              "      <td>-0.808320</td>\n",
              "      <td>1.732371</td>\n",
              "      <td>0.043591</td>\n",
              "      <td>0.566802</td>\n",
              "      <td>0.068453</td>\n",
              "      <td>-0.095018</td>\n",
              "      <td>0.465620</td>\n",
              "      <td>0.012577</td>\n",
              "      <td>-0.263639</td>\n",
              "      <td>-0.228348</td>\n",
              "      <td>0.091919</td>\n",
              "      <td>-1.073425</td>\n",
              "      <td>-0.779322</td>\n",
              "      <td>0.282733</td>\n",
              "      <td>-0.568742</td>\n",
              "      <td>1.268473</td>\n",
              "      <td>-0.290083</td>\n",
              "      <td>1.091785</td>\n",
              "      <td>-1.550019</td>\n",
              "      <td>176.153125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.391104</td>\n",
              "      <td>0.681290</td>\n",
              "      <td>0.490506</td>\n",
              "      <td>0.676052</td>\n",
              "      <td>0.402841</td>\n",
              "      <td>-0.016840</td>\n",
              "      <td>-0.917892</td>\n",
              "      <td>-0.167514</td>\n",
              "      <td>1.347119</td>\n",
              "      <td>0.816968</td>\n",
              "      <td>1.393229</td>\n",
              "      <td>0.017734</td>\n",
              "      <td>2.651090</td>\n",
              "      <td>-1.365601</td>\n",
              "      <td>1.187361</td>\n",
              "      <td>-0.674484</td>\n",
              "      <td>-0.687129</td>\n",
              "      <td>-0.464736</td>\n",
              "      <td>0.749492</td>\n",
              "      <td>1.249619</td>\n",
              "      <td>-0.067370</td>\n",
              "      <td>-0.071682</td>\n",
              "      <td>-0.485829</td>\n",
              "      <td>1.800882</td>\n",
              "      <td>0.614962</td>\n",
              "      <td>0.403652</td>\n",
              "      <td>1.808181</td>\n",
              "      <td>0.786147</td>\n",
              "      <td>-0.286649</td>\n",
              "      <td>-0.901450</td>\n",
              "      <td>0.116837</td>\n",
              "      <td>1.706143</td>\n",
              "      <td>-1.309684</td>\n",
              "      <td>-0.094999</td>\n",
              "      <td>0.613604</td>\n",
              "      <td>-0.564455</td>\n",
              "      <td>-0.971979</td>\n",
              "      <td>0.423302</td>\n",
              "      <td>-1.088586</td>\n",
              "      <td>0.462172</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.148162</td>\n",
              "      <td>0.570216</td>\n",
              "      <td>0.503088</td>\n",
              "      <td>-0.302073</td>\n",
              "      <td>-0.546844</td>\n",
              "      <td>0.229679</td>\n",
              "      <td>0.193383</td>\n",
              "      <td>1.070814</td>\n",
              "      <td>3.000255</td>\n",
              "      <td>-1.034228</td>\n",
              "      <td>0.758716</td>\n",
              "      <td>1.535114</td>\n",
              "      <td>1.564546</td>\n",
              "      <td>-1.404508</td>\n",
              "      <td>-0.032718</td>\n",
              "      <td>0.961480</td>\n",
              "      <td>0.925260</td>\n",
              "      <td>0.363097</td>\n",
              "      <td>-0.670305</td>\n",
              "      <td>0.375398</td>\n",
              "      <td>2.219655</td>\n",
              "      <td>-1.267025</td>\n",
              "      <td>-0.562530</td>\n",
              "      <td>-0.410211</td>\n",
              "      <td>-1.417569</td>\n",
              "      <td>-1.036486</td>\n",
              "      <td>-0.055885</td>\n",
              "      <td>0.207085</td>\n",
              "      <td>1.192735</td>\n",
              "      <td>0.019331</td>\n",
              "      <td>1.981386</td>\n",
              "      <td>1.144589</td>\n",
              "      <td>0.381848</td>\n",
              "      <td>1.294177</td>\n",
              "      <td>0.369602</td>\n",
              "      <td>-1.487163</td>\n",
              "      <td>0.804857</td>\n",
              "      <td>0.903420</td>\n",
              "      <td>1.499470</td>\n",
              "      <td>-68.065152</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.719885</td>\n",
              "      <td>-0.889897</td>\n",
              "      <td>0.058613</td>\n",
              "      <td>-0.399093</td>\n",
              "      <td>0.753669</td>\n",
              "      <td>-0.199855</td>\n",
              "      <td>-1.008447</td>\n",
              "      <td>-1.744641</td>\n",
              "      <td>0.181095</td>\n",
              "      <td>-0.075441</td>\n",
              "      <td>1.096473</td>\n",
              "      <td>0.782867</td>\n",
              "      <td>-0.627074</td>\n",
              "      <td>0.523761</td>\n",
              "      <td>0.073936</td>\n",
              "      <td>0.151389</td>\n",
              "      <td>-0.470097</td>\n",
              "      <td>-1.733421</td>\n",
              "      <td>-1.317414</td>\n",
              "      <td>0.081911</td>\n",
              "      <td>0.675981</td>\n",
              "      <td>-1.398310</td>\n",
              "      <td>-1.002599</td>\n",
              "      <td>-1.204504</td>\n",
              "      <td>1.559354</td>\n",
              "      <td>-0.591018</td>\n",
              "      <td>2.297427</td>\n",
              "      <td>0.040422</td>\n",
              "      <td>-1.336666</td>\n",
              "      <td>-0.378264</td>\n",
              "      <td>-0.020471</td>\n",
              "      <td>-1.305801</td>\n",
              "      <td>0.804193</td>\n",
              "      <td>-0.082371</td>\n",
              "      <td>0.175748</td>\n",
              "      <td>-0.517907</td>\n",
              "      <td>0.966972</td>\n",
              "      <td>-0.822497</td>\n",
              "      <td>-0.223144</td>\n",
              "      <td>-0.969422</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.587095</td>\n",
              "      <td>-1.051829</td>\n",
              "      <td>0.548410</td>\n",
              "      <td>-0.694777</td>\n",
              "      <td>0.687213</td>\n",
              "      <td>0.771571</td>\n",
              "      <td>0.566063</td>\n",
              "      <td>-0.494580</td>\n",
              "      <td>-1.410308</td>\n",
              "      <td>0.001335</td>\n",
              "      <td>0.427368</td>\n",
              "      <td>-0.747377</td>\n",
              "      <td>0.129508</td>\n",
              "      <td>-0.688339</td>\n",
              "      <td>1.581951</td>\n",
              "      <td>0.717288</td>\n",
              "      <td>-1.581983</td>\n",
              "      <td>-1.712351</td>\n",
              "      <td>0.359528</td>\n",
              "      <td>-0.731421</td>\n",
              "      <td>0.288311</td>\n",
              "      <td>0.672756</td>\n",
              "      <td>-0.467097</td>\n",
              "      <td>-0.549905</td>\n",
              "      <td>0.316208</td>\n",
              "      <td>0.178741</td>\n",
              "      <td>1.396324</td>\n",
              "      <td>0.801009</td>\n",
              "      <td>-1.617191</td>\n",
              "      <td>0.142715</td>\n",
              "      <td>0.056705</td>\n",
              "      <td>-0.182896</td>\n",
              "      <td>-0.528584</td>\n",
              "      <td>-0.725880</td>\n",
              "      <td>0.442389</td>\n",
              "      <td>0.970580</td>\n",
              "      <td>1.654307</td>\n",
              "      <td>1.044164</td>\n",
              "      <td>-1.362483</td>\n",
              "      <td>-71.334371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.144181</td>\n",
              "      <td>-0.319032</td>\n",
              "      <td>0.348945</td>\n",
              "      <td>-0.114184</td>\n",
              "      <td>-0.682886</td>\n",
              "      <td>0.250030</td>\n",
              "      <td>-0.488626</td>\n",
              "      <td>1.250046</td>\n",
              "      <td>1.375095</td>\n",
              "      <td>1.010553</td>\n",
              "      <td>0.292285</td>\n",
              "      <td>-0.094708</td>\n",
              "      <td>-1.210721</td>\n",
              "      <td>-2.189818</td>\n",
              "      <td>-0.212161</td>\n",
              "      <td>0.580827</td>\n",
              "      <td>0.353397</td>\n",
              "      <td>0.668362</td>\n",
              "      <td>0.252390</td>\n",
              "      <td>0.550159</td>\n",
              "      <td>-1.228381</td>\n",
              "      <td>-0.339964</td>\n",
              "      <td>1.141003</td>\n",
              "      <td>1.684626</td>\n",
              "      <td>1.369355</td>\n",
              "      <td>-1.052544</td>\n",
              "      <td>0.864944</td>\n",
              "      <td>0.468058</td>\n",
              "      <td>0.266212</td>\n",
              "      <td>-0.552052</td>\n",
              "      <td>1.045455</td>\n",
              "      <td>-1.391131</td>\n",
              "      <td>-1.672114</td>\n",
              "      <td>-1.027490</td>\n",
              "      <td>0.425736</td>\n",
              "      <td>0.358787</td>\n",
              "      <td>0.723373</td>\n",
              "      <td>0.244412</td>\n",
              "      <td>0.278188</td>\n",
              "      <td>-0.102053</td>\n",
              "      <td>...</td>\n",
              "      <td>-1.069598</td>\n",
              "      <td>0.761401</td>\n",
              "      <td>-0.302439</td>\n",
              "      <td>-1.089970</td>\n",
              "      <td>-0.874629</td>\n",
              "      <td>-0.221619</td>\n",
              "      <td>-0.138332</td>\n",
              "      <td>-1.484082</td>\n",
              "      <td>-0.563871</td>\n",
              "      <td>0.143567</td>\n",
              "      <td>0.167575</td>\n",
              "      <td>0.417327</td>\n",
              "      <td>0.445875</td>\n",
              "      <td>-0.818144</td>\n",
              "      <td>0.035537</td>\n",
              "      <td>-0.932450</td>\n",
              "      <td>0.090955</td>\n",
              "      <td>1.713935</td>\n",
              "      <td>0.111902</td>\n",
              "      <td>-0.830804</td>\n",
              "      <td>-0.353344</td>\n",
              "      <td>0.529086</td>\n",
              "      <td>0.896109</td>\n",
              "      <td>1.043990</td>\n",
              "      <td>-1.390438</td>\n",
              "      <td>-0.239256</td>\n",
              "      <td>-2.156437</td>\n",
              "      <td>0.062166</td>\n",
              "      <td>-0.833536</td>\n",
              "      <td>-0.047061</td>\n",
              "      <td>0.102890</td>\n",
              "      <td>-0.530058</td>\n",
              "      <td>-2.472791</td>\n",
              "      <td>-0.202770</td>\n",
              "      <td>1.249588</td>\n",
              "      <td>1.206175</td>\n",
              "      <td>-0.118433</td>\n",
              "      <td>1.006130</td>\n",
              "      <td>-0.730983</td>\n",
              "      <td>113.100727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14995</th>\n",
              "      <td>1.545114</td>\n",
              "      <td>-0.167122</td>\n",
              "      <td>-1.367842</td>\n",
              "      <td>-0.673855</td>\n",
              "      <td>0.178848</td>\n",
              "      <td>2.206777</td>\n",
              "      <td>0.244497</td>\n",
              "      <td>-1.155167</td>\n",
              "      <td>-1.704361</td>\n",
              "      <td>1.224313</td>\n",
              "      <td>-0.217844</td>\n",
              "      <td>-1.116583</td>\n",
              "      <td>0.623339</td>\n",
              "      <td>-0.962096</td>\n",
              "      <td>-1.218290</td>\n",
              "      <td>0.596774</td>\n",
              "      <td>0.246616</td>\n",
              "      <td>1.382255</td>\n",
              "      <td>1.014601</td>\n",
              "      <td>-0.812442</td>\n",
              "      <td>-1.783169</td>\n",
              "      <td>0.058534</td>\n",
              "      <td>-1.509516</td>\n",
              "      <td>0.022897</td>\n",
              "      <td>0.174287</td>\n",
              "      <td>-0.336951</td>\n",
              "      <td>0.166739</td>\n",
              "      <td>0.021835</td>\n",
              "      <td>-0.556025</td>\n",
              "      <td>0.086050</td>\n",
              "      <td>-1.038358</td>\n",
              "      <td>-2.530225</td>\n",
              "      <td>1.411084</td>\n",
              "      <td>-0.094539</td>\n",
              "      <td>-0.728083</td>\n",
              "      <td>-1.497200</td>\n",
              "      <td>0.449866</td>\n",
              "      <td>1.182965</td>\n",
              "      <td>-0.782986</td>\n",
              "      <td>-0.634949</td>\n",
              "      <td>...</td>\n",
              "      <td>1.150915</td>\n",
              "      <td>1.398881</td>\n",
              "      <td>-0.678083</td>\n",
              "      <td>0.746945</td>\n",
              "      <td>-0.777826</td>\n",
              "      <td>0.509954</td>\n",
              "      <td>0.152189</td>\n",
              "      <td>-1.432918</td>\n",
              "      <td>-0.063957</td>\n",
              "      <td>-1.608370</td>\n",
              "      <td>-1.349103</td>\n",
              "      <td>1.362261</td>\n",
              "      <td>1.910221</td>\n",
              "      <td>-0.611304</td>\n",
              "      <td>-0.294880</td>\n",
              "      <td>0.025684</td>\n",
              "      <td>-0.391341</td>\n",
              "      <td>0.263925</td>\n",
              "      <td>0.497980</td>\n",
              "      <td>-2.053597</td>\n",
              "      <td>0.350908</td>\n",
              "      <td>-0.292344</td>\n",
              "      <td>1.451974</td>\n",
              "      <td>0.147775</td>\n",
              "      <td>0.312538</td>\n",
              "      <td>0.872372</td>\n",
              "      <td>0.199461</td>\n",
              "      <td>-0.422645</td>\n",
              "      <td>-1.415882</td>\n",
              "      <td>-0.345844</td>\n",
              "      <td>-1.006833</td>\n",
              "      <td>-0.301356</td>\n",
              "      <td>-0.127034</td>\n",
              "      <td>0.153282</td>\n",
              "      <td>-0.824268</td>\n",
              "      <td>-1.196392</td>\n",
              "      <td>-2.608612</td>\n",
              "      <td>0.299099</td>\n",
              "      <td>0.724160</td>\n",
              "      <td>-137.033373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14996</th>\n",
              "      <td>1.163516</td>\n",
              "      <td>-0.166921</td>\n",
              "      <td>-0.276958</td>\n",
              "      <td>1.756970</td>\n",
              "      <td>0.243474</td>\n",
              "      <td>-0.286666</td>\n",
              "      <td>1.110022</td>\n",
              "      <td>-1.218195</td>\n",
              "      <td>0.692561</td>\n",
              "      <td>0.623567</td>\n",
              "      <td>-0.441170</td>\n",
              "      <td>0.642048</td>\n",
              "      <td>0.344966</td>\n",
              "      <td>-0.669358</td>\n",
              "      <td>0.146449</td>\n",
              "      <td>0.716909</td>\n",
              "      <td>0.127046</td>\n",
              "      <td>0.814435</td>\n",
              "      <td>-0.296196</td>\n",
              "      <td>2.017077</td>\n",
              "      <td>0.720872</td>\n",
              "      <td>-0.959372</td>\n",
              "      <td>-2.043443</td>\n",
              "      <td>-0.040644</td>\n",
              "      <td>-0.941572</td>\n",
              "      <td>0.154517</td>\n",
              "      <td>-0.366313</td>\n",
              "      <td>-0.417611</td>\n",
              "      <td>0.820438</td>\n",
              "      <td>0.701136</td>\n",
              "      <td>-0.948167</td>\n",
              "      <td>-1.827302</td>\n",
              "      <td>-0.231488</td>\n",
              "      <td>0.455176</td>\n",
              "      <td>0.142046</td>\n",
              "      <td>-1.043011</td>\n",
              "      <td>0.056278</td>\n",
              "      <td>-0.365349</td>\n",
              "      <td>0.885716</td>\n",
              "      <td>-1.948366</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.243440</td>\n",
              "      <td>0.470649</td>\n",
              "      <td>0.850258</td>\n",
              "      <td>0.562836</td>\n",
              "      <td>1.384557</td>\n",
              "      <td>-1.117088</td>\n",
              "      <td>-1.226499</td>\n",
              "      <td>-0.750229</td>\n",
              "      <td>-0.058317</td>\n",
              "      <td>0.186255</td>\n",
              "      <td>-1.034166</td>\n",
              "      <td>-0.738177</td>\n",
              "      <td>0.269844</td>\n",
              "      <td>1.641138</td>\n",
              "      <td>-0.156426</td>\n",
              "      <td>1.103855</td>\n",
              "      <td>1.312231</td>\n",
              "      <td>0.765476</td>\n",
              "      <td>2.580312</td>\n",
              "      <td>-0.985113</td>\n",
              "      <td>1.523020</td>\n",
              "      <td>-1.344391</td>\n",
              "      <td>0.629626</td>\n",
              "      <td>0.717585</td>\n",
              "      <td>-1.270978</td>\n",
              "      <td>-0.827769</td>\n",
              "      <td>-0.007042</td>\n",
              "      <td>0.325521</td>\n",
              "      <td>0.756363</td>\n",
              "      <td>-1.621526</td>\n",
              "      <td>1.474190</td>\n",
              "      <td>-1.431877</td>\n",
              "      <td>-0.139012</td>\n",
              "      <td>-1.004274</td>\n",
              "      <td>-0.974729</td>\n",
              "      <td>0.036024</td>\n",
              "      <td>0.047508</td>\n",
              "      <td>0.006563</td>\n",
              "      <td>-0.865625</td>\n",
              "      <td>-40.816411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14997</th>\n",
              "      <td>-1.033274</td>\n",
              "      <td>-0.435623</td>\n",
              "      <td>0.853572</td>\n",
              "      <td>1.432407</td>\n",
              "      <td>0.513440</td>\n",
              "      <td>-0.132013</td>\n",
              "      <td>0.359568</td>\n",
              "      <td>1.482623</td>\n",
              "      <td>0.563352</td>\n",
              "      <td>-0.752640</td>\n",
              "      <td>-0.890584</td>\n",
              "      <td>0.571377</td>\n",
              "      <td>0.493287</td>\n",
              "      <td>2.343939</td>\n",
              "      <td>-0.139023</td>\n",
              "      <td>0.487771</td>\n",
              "      <td>0.020718</td>\n",
              "      <td>0.468156</td>\n",
              "      <td>0.277303</td>\n",
              "      <td>2.583147</td>\n",
              "      <td>2.262418</td>\n",
              "      <td>-0.164864</td>\n",
              "      <td>-0.171376</td>\n",
              "      <td>1.089398</td>\n",
              "      <td>0.849872</td>\n",
              "      <td>-0.175210</td>\n",
              "      <td>0.133909</td>\n",
              "      <td>-0.067360</td>\n",
              "      <td>0.126319</td>\n",
              "      <td>-0.139243</td>\n",
              "      <td>-0.689473</td>\n",
              "      <td>1.574436</td>\n",
              "      <td>-0.506444</td>\n",
              "      <td>0.801242</td>\n",
              "      <td>-0.052975</td>\n",
              "      <td>-1.678809</td>\n",
              "      <td>-1.113424</td>\n",
              "      <td>1.577680</td>\n",
              "      <td>-0.134068</td>\n",
              "      <td>-0.876387</td>\n",
              "      <td>...</td>\n",
              "      <td>0.766017</td>\n",
              "      <td>0.328752</td>\n",
              "      <td>-0.289896</td>\n",
              "      <td>-0.695213</td>\n",
              "      <td>0.284232</td>\n",
              "      <td>-0.411008</td>\n",
              "      <td>-0.490671</td>\n",
              "      <td>0.284149</td>\n",
              "      <td>0.946619</td>\n",
              "      <td>-1.045193</td>\n",
              "      <td>1.415749</td>\n",
              "      <td>-0.009919</td>\n",
              "      <td>0.659791</td>\n",
              "      <td>-0.539024</td>\n",
              "      <td>-0.339412</td>\n",
              "      <td>-0.632053</td>\n",
              "      <td>0.185367</td>\n",
              "      <td>1.413021</td>\n",
              "      <td>-1.498033</td>\n",
              "      <td>-0.935386</td>\n",
              "      <td>0.085556</td>\n",
              "      <td>-0.294534</td>\n",
              "      <td>1.145761</td>\n",
              "      <td>-0.424243</td>\n",
              "      <td>0.014264</td>\n",
              "      <td>0.733201</td>\n",
              "      <td>1.824631</td>\n",
              "      <td>-0.041434</td>\n",
              "      <td>-0.854563</td>\n",
              "      <td>-0.908474</td>\n",
              "      <td>1.411313</td>\n",
              "      <td>-1.312830</td>\n",
              "      <td>0.219216</td>\n",
              "      <td>-0.637770</td>\n",
              "      <td>-1.205861</td>\n",
              "      <td>-1.268636</td>\n",
              "      <td>1.780151</td>\n",
              "      <td>-1.239915</td>\n",
              "      <td>1.897995</td>\n",
              "      <td>14.992331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14998</th>\n",
              "      <td>1.259606</td>\n",
              "      <td>1.448206</td>\n",
              "      <td>0.811341</td>\n",
              "      <td>1.022531</td>\n",
              "      <td>0.428322</td>\n",
              "      <td>-1.162456</td>\n",
              "      <td>1.276411</td>\n",
              "      <td>-0.152711</td>\n",
              "      <td>-0.395997</td>\n",
              "      <td>-0.181918</td>\n",
              "      <td>-0.341960</td>\n",
              "      <td>-0.760812</td>\n",
              "      <td>0.646627</td>\n",
              "      <td>-1.714064</td>\n",
              "      <td>-1.398055</td>\n",
              "      <td>1.458554</td>\n",
              "      <td>-0.770586</td>\n",
              "      <td>0.541755</td>\n",
              "      <td>-0.158525</td>\n",
              "      <td>-0.620635</td>\n",
              "      <td>1.855892</td>\n",
              "      <td>-0.846785</td>\n",
              "      <td>0.678048</td>\n",
              "      <td>0.015501</td>\n",
              "      <td>-0.074209</td>\n",
              "      <td>1.330166</td>\n",
              "      <td>-0.051602</td>\n",
              "      <td>-0.857914</td>\n",
              "      <td>0.190565</td>\n",
              "      <td>0.602166</td>\n",
              "      <td>-0.067264</td>\n",
              "      <td>0.470888</td>\n",
              "      <td>0.782349</td>\n",
              "      <td>0.302400</td>\n",
              "      <td>0.351353</td>\n",
              "      <td>-2.065372</td>\n",
              "      <td>-0.343593</td>\n",
              "      <td>-0.812172</td>\n",
              "      <td>-0.677642</td>\n",
              "      <td>-0.352727</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.800508</td>\n",
              "      <td>0.759756</td>\n",
              "      <td>1.748599</td>\n",
              "      <td>0.276593</td>\n",
              "      <td>-0.036808</td>\n",
              "      <td>0.185975</td>\n",
              "      <td>0.611435</td>\n",
              "      <td>-0.973588</td>\n",
              "      <td>-0.462930</td>\n",
              "      <td>-0.805485</td>\n",
              "      <td>0.606302</td>\n",
              "      <td>0.069062</td>\n",
              "      <td>0.085991</td>\n",
              "      <td>-0.862176</td>\n",
              "      <td>-0.891958</td>\n",
              "      <td>0.253522</td>\n",
              "      <td>-0.531128</td>\n",
              "      <td>-2.221376</td>\n",
              "      <td>0.510346</td>\n",
              "      <td>-1.003147</td>\n",
              "      <td>1.126642</td>\n",
              "      <td>-0.010658</td>\n",
              "      <td>-1.199161</td>\n",
              "      <td>0.657451</td>\n",
              "      <td>-0.100792</td>\n",
              "      <td>0.222487</td>\n",
              "      <td>-1.358077</td>\n",
              "      <td>1.648059</td>\n",
              "      <td>0.528499</td>\n",
              "      <td>1.539199</td>\n",
              "      <td>1.567961</td>\n",
              "      <td>-1.794899</td>\n",
              "      <td>0.878275</td>\n",
              "      <td>0.558930</td>\n",
              "      <td>-0.190316</td>\n",
              "      <td>0.493027</td>\n",
              "      <td>-0.137877</td>\n",
              "      <td>-0.046186</td>\n",
              "      <td>0.703378</td>\n",
              "      <td>189.993695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14999</th>\n",
              "      <td>0.646354</td>\n",
              "      <td>-1.150759</td>\n",
              "      <td>-0.782070</td>\n",
              "      <td>0.958161</td>\n",
              "      <td>-0.131790</td>\n",
              "      <td>-0.594893</td>\n",
              "      <td>0.917584</td>\n",
              "      <td>1.253354</td>\n",
              "      <td>1.758362</td>\n",
              "      <td>-0.687988</td>\n",
              "      <td>-0.086333</td>\n",
              "      <td>-0.062785</td>\n",
              "      <td>1.440211</td>\n",
              "      <td>0.276512</td>\n",
              "      <td>-0.522454</td>\n",
              "      <td>-0.349524</td>\n",
              "      <td>-0.190902</td>\n",
              "      <td>0.164012</td>\n",
              "      <td>-1.326302</td>\n",
              "      <td>0.381237</td>\n",
              "      <td>-0.562355</td>\n",
              "      <td>-1.062620</td>\n",
              "      <td>0.504032</td>\n",
              "      <td>-0.402480</td>\n",
              "      <td>0.732181</td>\n",
              "      <td>-1.073406</td>\n",
              "      <td>-1.149304</td>\n",
              "      <td>-0.491826</td>\n",
              "      <td>-0.702183</td>\n",
              "      <td>1.797133</td>\n",
              "      <td>-1.969432</td>\n",
              "      <td>1.383986</td>\n",
              "      <td>1.529361</td>\n",
              "      <td>2.005019</td>\n",
              "      <td>-0.041085</td>\n",
              "      <td>-1.379975</td>\n",
              "      <td>0.162023</td>\n",
              "      <td>-0.248657</td>\n",
              "      <td>-0.196309</td>\n",
              "      <td>0.429640</td>\n",
              "      <td>...</td>\n",
              "      <td>0.677118</td>\n",
              "      <td>0.442751</td>\n",
              "      <td>0.785748</td>\n",
              "      <td>-2.472052</td>\n",
              "      <td>0.088822</td>\n",
              "      <td>0.254515</td>\n",
              "      <td>-1.725288</td>\n",
              "      <td>-0.154023</td>\n",
              "      <td>-0.362389</td>\n",
              "      <td>-0.023564</td>\n",
              "      <td>-2.368821</td>\n",
              "      <td>3.229775</td>\n",
              "      <td>-0.030923</td>\n",
              "      <td>0.956017</td>\n",
              "      <td>-0.151769</td>\n",
              "      <td>-1.416129</td>\n",
              "      <td>-0.173153</td>\n",
              "      <td>1.043534</td>\n",
              "      <td>-0.454917</td>\n",
              "      <td>-0.274631</td>\n",
              "      <td>0.375069</td>\n",
              "      <td>-0.630376</td>\n",
              "      <td>-1.479524</td>\n",
              "      <td>-0.417380</td>\n",
              "      <td>-0.618551</td>\n",
              "      <td>0.121428</td>\n",
              "      <td>-0.662447</td>\n",
              "      <td>-1.267673</td>\n",
              "      <td>-0.285459</td>\n",
              "      <td>-0.073321</td>\n",
              "      <td>-0.345907</td>\n",
              "      <td>0.262940</td>\n",
              "      <td>1.525330</td>\n",
              "      <td>-0.967360</td>\n",
              "      <td>-0.934249</td>\n",
              "      <td>0.122279</td>\n",
              "      <td>-0.252035</td>\n",
              "      <td>-0.372530</td>\n",
              "      <td>-1.126846</td>\n",
              "      <td>81.004495</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>15000 rows × 101 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            f_0       f_1       f_2  ...      f_98      f_99      target\n",
              "0      1.464094  0.709491 -2.054099  ... -0.364065 -1.883873 -122.301333\n",
              "1      0.048285  0.493355  0.502420  ...  1.091785 -1.550019  176.153125\n",
              "2     -0.391104  0.681290  0.490506  ...  0.903420  1.499470  -68.065152\n",
              "3     -0.719885 -0.889897  0.058613  ...  1.044164 -1.362483  -71.334371\n",
              "4     -1.144181 -0.319032  0.348945  ...  1.006130 -0.730983  113.100727\n",
              "...         ...       ...       ...  ...       ...       ...         ...\n",
              "14995  1.545114 -0.167122 -1.367842  ...  0.299099  0.724160 -137.033373\n",
              "14996  1.163516 -0.166921 -0.276958  ...  0.006563 -0.865625  -40.816411\n",
              "14997 -1.033274 -0.435623  0.853572  ... -1.239915  1.897995   14.992331\n",
              "14998  1.259606  1.448206  0.811341  ... -0.046186  0.703378  189.993695\n",
              "14999  0.646354 -1.150759 -0.782070  ... -0.372530 -1.126846   81.004495\n",
              "\n",
              "[15000 rows x 101 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLfUfscVrmE-"
      },
      "source": [
        "import numpy as np\n",
        "data = df.copy()\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "  # calculate the number of bins by Sturge's rule\n",
        "  # I take the floor of the value, you can also\n",
        "  # just round it\n",
        "num_bins = np.floor(1 + np.log2(len(data)))\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNnTqjBIr1vz",
        "outputId": "8063bf33-0329-48c7-a285-1cfd61711f12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.log2(len(data)),np.floor(1 + np.log2(len(data)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(13.872674880270605, 14.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHNoJBfBuNtK",
        "outputId": "926161f3-1b5a-4c1f-baaf-858759dd3a62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "data[\"target\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        -12.633692\n",
              "1        -77.060051\n",
              "2       -103.868103\n",
              "3         33.073009\n",
              "4        110.247254\n",
              "            ...    \n",
              "14995    -88.733920\n",
              "14996    -64.968666\n",
              "14997     81.333266\n",
              "14998   -187.129530\n",
              "14999    -73.433552\n",
              "Name: target, Length: 15000, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfP0ZI9OvIt1"
      },
      "source": [
        "pd.cut()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTY8Z4dOu_bw",
        "outputId": "cafe53f8-7189-43c4-be41-d9a23ddb04b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], (0.994, 3.0]]\n",
              "Categories (3, interval[float64]): [(0.994, 3.0] < (3.0, 5.0] < (5.0, 7.0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLMZwEDuvPGX",
        "outputId": "96f8a757-119e-49c3-d918-39fbc80cdcb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3,labels=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 1, 1, 2, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkbkT1j_u8gh"
      },
      "source": [
        "pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3) ... # doctest: +ELLIPSIS [(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ... Categories (3, interval[float64]): [(0.994, 3.0] < (3.0, 5.0] ...\n",
        "\n",
        "pd.cut(np.array([1, 7, 5, 4, 6, 3]), 3, retbins=True) ... # doctest: +ELLIPSIS ([(0.994, 3.0], (5.0, 7.0], (3.0, 5.0], (3.0, 5.0], (5.0, 7.0], ... Categories (3, interval[float64]): [(0.994, 3.0] < (3.0, 5.0] ... array([0.994, 3. , 5. , 7. ]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXH3xNKNuhKv"
      },
      "source": [
        "pd.cut()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWgyqkQFuKQU",
        "outputId": "12a7f7f6-8fa2-4daa-900c-b03e45983f3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        }
      },
      "source": [
        "pd.cut(data[\"target\"], bins=num_bins, labels=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\u001b[0m in \u001b[0;36mlinspace\u001b[0;34m(start, stop, num, endpoint, retstep, dtype, axis)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-7ed6558b0edb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_bins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/tile.py\u001b[0m in \u001b[0;36mcut\u001b[0;34m(x, bins, right, labels, retbins, precision, include_lowest, duplicates)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# adjust end points after binning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m             \u001b[0madj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m  \u001b[0;31m# 0.1% of the range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mlinspace\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\u001b[0m in \u001b[0;36mlinspace\u001b[0;34m(start, stop, num, endpoint, retstep, dtype, axis)\u001b[0m\n\u001b[1;32m    119\u001b[0m         raise TypeError(\n\u001b[1;32m    120\u001b[0m             \u001b[0;34m\"object of type {} cannot be safely interpreted as an integer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 .format(type(num)))\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nu2ezUnsOXP",
        "outputId": "efdff42f-8b9d-436b-e1f3-3d345bd9d07c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        }
      },
      "source": [
        " # bin targets\n",
        "data.loc[:, \"bins\"] = pd.cut(data[\"target\"], bins=num_bins, labels=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\u001b[0m in \u001b[0;36mlinspace\u001b[0;34m(start, stop, num, endpoint, retstep, dtype, axis)\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-3fa981b036b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# bin targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"bins\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"target\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_bins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/tile.py\u001b[0m in \u001b[0;36mcut\u001b[0;34m(x, bins, right, labels, retbins, precision, include_lowest, duplicates)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# adjust end points after binning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             \u001b[0mbins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m             \u001b[0madj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m  \u001b[0;31m# 0.1% of the range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mlinspace\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/function_base.py\u001b[0m in \u001b[0;36mlinspace\u001b[0;34m(start, stop, num, endpoint, retstep, dtype, axis)\u001b[0m\n\u001b[1;32m    119\u001b[0m         raise TypeError(\n\u001b[1;32m    120\u001b[0m             \u001b[0;34m\"object of type {} cannot be safely interpreted as an integer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 .format(type(num)))\n\u001b[0m\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type <class 'numpy.float64'> cannot be safely interpreted as an integer."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJTI8niKO1bk"
      },
      "source": [
        "#Cross-validation is the first and most essential step when it comes to building\n",
        "machine learning models. If you want to do feature engineering, split your data first.\n",
        "If you're going to build models, split your data first. If you have a good crossvalidation\n",
        "scheme in which validation data is representative of training and realworld\n",
        "data, you will be able to build a good machine learning model which is highly\n",
        "generalizable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SpKNAZNRD19"
      },
      "source": [
        "#The types of cross-validation presented in this chapter can be applied to almost any\n",
        "machine learning problem. Still, you must keep in mind that cross-validation also\n",
        "depends a lot on the data and you might need to adopt new forms of cross-validation\n",
        "depending on your problem and data.\n",
        "#For example, let’s say we have a problem in which we would like to build a model to detect skin cancer from skin images of patients. Our task is to build a binary classifier which takes an input image and predicts the probability for it being benign or malignant.\n",
        "#In these kinds of datasets, you might have multiple images for the same patient in\n",
        "the training dataset. So, to build a good cross-validation system here, you must have\n",
        "stratified k-folds, but you must also make sure that patients in training data do not\n",
        "appear in validation data. Fortunately, scikit-learn offers a type of cross-validation\n",
        "known as GroupKFold. Here the patients can be considered as groups. But\n",
        "unfortunately, there is no way to combine GroupKFold with StratifiedKFold in\n",
        "scikit-learn. So you need to do that yourself. I’ll leave it as an exercise for the reader."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lebUMSIMSy-b"
      },
      "source": [
        "#Evaluation metrics\n",
        "#Knowing about how the aforementioned metrics work is not the only thing we have to understand. We must also know when to use which metrics, and that depends on what kind of data and targets you have. I think it’s more about the targets and less about the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHhhZmEksgd4",
        "outputId": "ed5a3c59-0910-4fbb-a80b-f0ae7b5efd1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "pwd"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/gdrive/My Drive/notes'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5aVIqgnW6-4"
      },
      "source": [
        "#When we have an equal number of positive and negative samples in a binary\n",
        "classification metric, we generally use accuracy, precision, recall and f1.\n",
        "#Accuracy: It is one of the most straightforward metrics used in machine learning.\n",
        "#It defines how accurate your model is. For the problem described above, if you build\n",
        "a model that classifies 90 images accurately, your accuracy is 90% or 0.90. If only\n",
        "83 images are classified correctly, the accuracy of your model is 83% or 0.83.\n",
        "Simple.\n",
        "#Python code for calculating accuracy is also quite simple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNWY09pMSxOj"
      },
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Function to calculate accuracy\n",
        "  :param y_true: list of true values\n",
        "  :param y_pred: list of predicted values\n",
        "  :return: accuracy score\n",
        "  \"\"\"\n",
        "  # initialize a simple counter for correct predictions\n",
        "  correct_counter = 0\n",
        "  # loop over all elements of y_true\n",
        "  # and y_pred \"together\"\n",
        "  for yt, yp in zip(y_true, y_pred):\n",
        "    if yt == yp:\n",
        "      # if prediction is equal to truth, increase the counter\n",
        "      correct_counter += 1\n",
        "  # return accuracy\n",
        "  # which is correct predictions over the number of samples\n",
        "  return correct_counter / len(y_true)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMIsORVzX3fn"
      },
      "source": [
        "#We can also calculate accuracy using scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsHERSQbXbdZ",
        "outputId": "1d093635-d381-45ea-ad2c-60841ad47d0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn import metrics\n",
        "l1 = [0,1,1,1,0,0,0,1]\n",
        "l2 = [0,1,0,1,0,1,0,0]\n",
        "metrics.accuracy_score(l1, l2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwXcKKZEX8fN",
        "outputId": "c506a142-6089-415e-b185-e9c11677dca2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy(l1, l2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.625"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCPKJx2QYlpH"
      },
      "source": [
        "#Now, let’s say we change the dataset a bit such that there are 180 chest x-ray images which do not have pneumothorax and only 20 with pneumothorax. Even in this case, we will create the training and validation sets with the same ratio of positive to negative (pneumothorax to non- pneumothorax) targets. In each set, we have 90 non- pneumothorax and 10 pneumothorax images. If you say that all images in the validation set are non-pneumothorax, what would your accuracy be? Let’s see; you classified 90% of the images correctly. So, your accuracy is 90%.\n",
        "#But look at it one more time.\n",
        "#You didn’t even build a model and got an accuracy of 90%. That seems kind of useless. If we look carefully, we will see that the dataset is skewed, i.e., the number of samples in one class outnumber the number of samples in other class by a lot. In these kinds of cases, it is not advisable to use accuracy as an evaluation metric as it is not representative of the data. So, you might get high accuracy, but your model will probably not perform that well when it comes to real-world samples, and you won’t be able to explain to your managers why.\n",
        "\n",
        "#In these cases, it’s better to look at other metrics such as precision.\n",
        "#Before learning about precision, we need to know a few terms. Here we have\n",
        "assumed that chest x-ray images with pneumothorax are positive class (1) and\n",
        "without pneumothorax are negative class (0).\n",
        "#True positive (TP): Given an image, if your model predicts the image has\n",
        "pneumothorax, and the actual target for that image has pneumothorax, it is\n",
        "considered a true positive.\n",
        "#True negative (TN): Given an image, if your model predicts that the image does not\n",
        "have pneumothorax and the actual target says that it is a non-pneumothorax image,\n",
        "it is considered a true negative.\n",
        "#In simple words, if your model correctly predicts positive class, it is true positive, and if your model accurately predicts negative class, it is a true negative.\n",
        "\n",
        "#False positive (FP): Given an image, if your model predicts pneumothorax and the actual target for that image is non- pneumothorax, it a false positive.\n",
        "\n",
        "\n",
        "#False negative (FN): Given an image, if your model predicts non-pneumothorax\n",
        "and the actual target for that image is pneumothorax, it is a false negative.\n",
        "#In simple words, if your model incorrectly (or falsely) predicts positive class, it is a false positive. If your model incorrectly (or falsely) predicts negative class, it is a false negative.\n",
        "#Let’s look at implementations of these, one at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcLNN_nJX_sy"
      },
      "source": [
        "def true_positive(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Function to calculate True Positives\n",
        "  :param y_true: list of true values\n",
        "  :param y_pred: list of predicted values\n",
        "  :return: number of true positives\n",
        "  \"\"\"\n",
        "  # initialize\n",
        "  tp = 0\n",
        "  for yt, yp in zip(y_true, y_pred):\n",
        "    if yt == 1 and yp == 1:\n",
        "      tp += 1\n",
        "  return tp\n",
        "\n",
        "def true_negative(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Function to calculate True Negatives\n",
        "  :param y_true: list of true values\n",
        "  :param y_pred: list of predicted values\n",
        "  :return: number of true negatives\n",
        "  \"\"\"\n",
        "  # initialize\n",
        "  tn = 0\n",
        "  for yt, yp in zip(y_true, y_pred):\n",
        "      if yt == 0 and yp == 0:\n",
        "        tn += 1\n",
        "  return tn\n",
        "\n",
        "def false_positive(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Function to calculate False Positives\n",
        "  :param y_true: list of true values\n",
        "  :param y_pred: list of predicted values\n",
        "  :return: number of false positives\n",
        "  \"\"\"\n",
        "  # initialize\n",
        "  35\n",
        "  fp = 0\n",
        "  for yt, yp in zip(y_true, y_pred):\n",
        "      if yt == 0 and yp == 1:\n",
        "        fp += 1\n",
        "  return fp\n",
        "\n",
        "def false_negative(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Function to calculate False Negatives\n",
        "  :param y_true: list of true values\n",
        "  :param y_pred: list of predicted values\n",
        "  :return: number of false negatives\n",
        "  \"\"\"\n",
        "  # initialize\n",
        "  fn = 0\n",
        "  for yt, yp in zip(y_true, y_pred):\n",
        "      if yt == 1 and yp == 0:\n",
        "        fn += 1\n",
        "  return fn"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6ikRgfEakqM"
      },
      "source": [
        "#The way I have implemented these here is quite simple and works only for binary classification. Let’s check these functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLxL1-Vlaabi"
      },
      "source": [
        "l1 = [0,1,1,1,0,0,0,1]\n",
        "l2 = [0,1,0,1,0,1,0,0]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQb1U3ZoaptJ",
        "outputId": "cc013d8b-a0b4-476a-a4f6-11f7c9243f30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "true_positive(l1, l2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiS5tFR9arQ_",
        "outputId": "b6010af4-5fe4-491e-a2f5-239c3133bd5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "false_positive(l1, l2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0x3EGifaumU",
        "outputId": "c15a9923-c4b3-4615-d348-f396bb39f094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "false_negative(l1, l2)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAqZzNZVawJf",
        "outputId": "f8f5b0ae-fa92-4201-de28-0e77ecfe3f48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "true_negative(l1, l2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUeYoWKua7d5"
      },
      "source": [
        "#If we have to define accuracy using the terms described above, we can write:\n",
        "#Accuracy Score = (TP + TN) / (TP + TN + FP + FN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khsQGVTLbH3x"
      },
      "source": [
        "#We can now quickly implement accuracy score using TP, TN, FP and FN in python.\n",
        "#Let’s call it accuracy_v2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqCDxRVXa1XY"
      },
      "source": [
        "def accuracy_v2(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Function to calculate accuracy using tp/tn/fp/fn\n",
        "  :param y_true: list of true values\n",
        "  :param y_pred: list of predicted values\n",
        "  :return: accuracy score\n",
        "  \"\"\"\n",
        "  tp = true_positive(y_true, y_pred)\n",
        "  fp = false_positive(y_true, y_pred)\n",
        "  fn = false_negative(y_true, y_pred)\n",
        "  tn = true_negative(y_true, y_pred)\n",
        "  accuracy_score = (tp + tn) / (tp + tn + fp + fn)\n",
        "  return accuracy_score"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T78kyoV3bQJ0",
        "outputId": "006945b6-359b-4720-c179-6dd88dade2b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accuracy(l1, l2),accuracy_v2(l1,l2),metrics.accuracy_score(l1, l2)\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.625, 0.625, 0.625)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODfhsyOYb8l1"
      },
      "source": [
        "#Impliment other metrics\n",
        "#Precison = TP/(TP+FP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Srr5UADbZNz"
      },
      "source": [
        "def precision(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Function to calculate precision\n",
        "  :param y_true: list of true values\n",
        "  :param y_pred: list of predicted values\n",
        "  :return: precision score\n",
        "  \"\"\"\n",
        "  tp = true_positive(y_true, y_pred)\n",
        "  fp = false_positive(y_true, y_pred)\n",
        "  precision = tp / (tp + fp)\n",
        "  return precision"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KENDY3aqcND-",
        "outputId": "99bb1324-6324-480f-e180-97eefc66a625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "precision(l1, l2)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6666666666666666"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxKW_GmAcTRz"
      },
      "source": [
        "#we come to recall. Recall is defined as:\n",
        "#Recall = TP / (TP + FN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDA3tUgbcQol"
      },
      "source": [
        "def recall(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Function to calculate recall\n",
        "  :param y_true: list of true values\n",
        "  :param y_pred: list of predicted values\n",
        "  :return: recall score\n",
        "  \"\"\"\n",
        "  tp = true_positive(y_true, y_pred)\n",
        "  fn = false_negative(y_true, y_pred)\n",
        "  recall = tp / (tp + fn)\n",
        "  return recall"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0GcM0WBcYPj",
        "outputId": "5d2fcfc6-4779-41bd-cd1b-ec4174271418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "recall(l1, l2)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjezpDXXcxtQ"
      },
      "source": [
        "#For a “good” model, our precision and recall values should be high. \n",
        "#We see that in the above example, the precison value is NOT quite high. However, reall is very low!\n",
        "#Our model produces quite a lot of **false positives** but less false negatives. \n",
        "#Fewer false negatives are good in this type of problem because you don’t want to say that patients do not have pneumothorax when they do. That is going to be more harmful. But we do have a lot of false positives, and that’s not good either.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I0ktdmLdm2v"
      },
      "source": [
        "# Most of the models predict a probability, and when we predict, we usually choose this threshold to be 0.5. This threshold is not always ideal, and depending on this threshold, your value of precision and recall can change drastically. If for every threshold we choose, we calculate the precision and recall values, we can create a plot between these sets of values. This plot or curve is known as the precision-recall curve.\n",
        "#Before looking into the precision-recall curve, let’s assume two lists."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99D1CBb4cbHj"
      },
      "source": [
        "y_true = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0,1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
        "y_pred = [0.02638412, 0.11114267, 0.31620708,\n",
        " 0.0490937, 0.0191491, 0.17554844,\n",
        " 0.15952202, 0.03819563, 0.11639273,\n",
        " 0.079377, 0.08584789, 0.39095342,\n",
        " 0.27259048, 0.03447096, 0.04644807,\n",
        " 0.03543574, 0.18521942, 0.05934905,\n",
        " 0.61977213, 0.33056815]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRmyjoO-eZmI"
      },
      "source": [
        "# So, y_true is our targets, and y_pred is the probability values for a sample being assigned a value of 1. So, now, we look at probabilities in prediction instead of the predicted value (which is most of the time calculated with a threshold at 0.5)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0t4FbRU0eF1k",
        "outputId": "40ade053-46aa-4ec0-9b14-d879c7420f51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        }
      },
      "source": [
        "# NOTE: this code is written in a jupyter notebook\n",
        "# import scikit-learn tree and metrics\n",
        "from sklearn import tree\n",
        "from sklearn import metrics\n",
        "# import matplotlib and seaborn\n",
        "# for plotting\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "# this is our global size of label text\n",
        "# on the plots\n",
        "matplotlib.rc('xtick', labelsize=20)\n",
        "matplotlib.rc('ytick', labelsize=20)\n",
        "# This line ensures that the plot is displayed\n",
        "# inside the notebook\n",
        "%matplotlib inline\n",
        "precisions = []\n",
        "recalls = []\n",
        "# how we assumed these thresholds is a long story\n",
        "thresholds = [0.0490937 , 0.05934905, 0.079377,\n",
        "0.08584789, 0.11114267, 0.11639273,\n",
        "0.15952202, 0.17554844, 0.18521942,\n",
        "0.27259048, 0.31620708, 0.33056815,\n",
        "0.39095342, 0.61977213]\n",
        "# for every threshold, calculate predictions in binary\n",
        "# and append calculated precisions and recalls\n",
        "# to their respective lists\n",
        "for i in thresholds:\n",
        "  temp_prediction = [1 if x >= i else 0 for x in y_pred]\n",
        "  p = precision(y_true, temp_prediction)\n",
        "  r = recall(y_true, temp_prediction)\n",
        "  precisions.append(p)\n",
        "  recalls.append(r)\n",
        "\n",
        "#Now, we can plot these values of precisions and recalls.\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.plot(recalls, precisions)\n",
        "plt.xlabel('Recall', fontsize=15)\n",
        "plt.ylabel('Precision', fontsize=15)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Precision')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXWR9nXmfzF4"
      },
      "source": [
        "#This precision-recall curve looks very different from what you might have seen on the internet. It’s because we had only 20 samples, and only 3 of them were positive samples. But there’s nothing to worry. It’s the same old precision-recall curve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVVOJU1egZTW"
      },
      "source": [
        "#Both precision and recall range from 0 to 1 and a value closer to 1 is better.\n",
        "#F1 score is a metric that combines both precision and recall. It is defined as a simple weighted average (harmonic mean) of precision and recall. If we denote precisionusing P and recall using R, we can represent \n",
        "#the F1 score as: F1 = 2PR / (P + R)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1R7vDMMfLPZ"
      },
      "source": [
        "def f1(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Function to calculate f1 score\n",
        "  :param y_true: list of true values\n",
        "  :param y_pred: list of predicted values\n",
        "  :return: f1 score\n",
        "  \"\"\"\n",
        "  p = precision(y_true, y_pred)\n",
        "  r = recall(y_true, y_pred)\n",
        "  score = 2 * p * r / (p + r)\n",
        "  return score"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDJPcyo-g4QZ"
      },
      "source": [
        "#Let’s see the results of this and compare it with scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YNdxsfqg0SN",
        "outputId": "dd1583b4-d115-4d53-b0db-f02ca6fcba21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn import metrics\n",
        "y_true = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0,1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
        "y_pred = [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
        "f1(y_true, y_pred),metrics.f1_score(y_true,y_pred)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5714285714285715, 0.5714285714285715)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XxDD4i5haG9"
      },
      "source": [
        "#Instead of looking at precision and recall individually, you can also just look at F1 score. Same as for precision, recall and accuracy, F1 score also ranges from 0 to 1, and a perfect prediction model has an F1 of 1. When dealing with datasets that have skewed targets, we should look at F1 (or precision and recall) instead of accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIajKadPiSIn"
      },
      "source": [
        "#The first one is TPR or True Positive Rate, which is the same as recall.\n",
        "#TPR = TP / (TP + FN)\n",
        "#Even though it is same as recall, we will make a python function for it for further use with this name.\n",
        "#TPR or recall is also known as sensitivity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mza6GtkrhHdG"
      },
      "source": [
        "def tpr(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Function to calculate tpr\n",
        "  :param y_true: list of true values\n",
        "  :param y_pred: list of predicted values\n",
        "  :return: tpr/recall\n",
        "  \"\"\"\n",
        "  return recall(y_true, y_pred)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxKXl__tig7A"
      },
      "source": [
        "#And FPR or False Positive Rate, which is defined as:\n",
        "#FPR = FP / (TN + FP)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9X1Sx6HiijD"
      },
      "source": [
        "def fpr(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Function to calculate fpr\n",
        "  :param y_true: list of true values\n",
        "  :param y_pred: list of predicted values\n",
        "  :return: fpr\n",
        "  \"\"\"\n",
        "  fp = false_positive(y_true, y_pred)\n",
        "  tn = true_negative(y_true, y_pred)\n",
        "  return fp / (tn + fp)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeQDxmGAi3q0"
      },
      "source": [
        "#And 1 - FPR is known as specificity or True Negative Rate or TNR.\n",
        "#These are a lot of terms, but the most important ones out of these are only TPR and FPR."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGsq1Jn-jK0_"
      },
      "source": [
        "#Let’s calculate only two values, though: TPR and FPR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM6skQFDipKp"
      },
      "source": [
        "# empty lists to store tpr\n",
        "# and fpr values\n",
        "tpr_list = []\n",
        "fpr_list = []\n",
        "# actual targets\n",
        "y_true = [0, 0, 0, 0, 1, 0, 1,\n",
        "0, 0, 1, 0, 1, 0, 0, 1]\n",
        "# predicted probabilities of a sample being 1\n",
        "y_pred = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05,\n",
        "0.9, 0.5, 0.3, 0.66, 0.3, 0.2,\n",
        "0.85, 0.15, 0.99]\n",
        "# handmade thresholds\n",
        "thresholds = [0, 0.1, 0.2, 0.3, 0.4, 0.5,\n",
        "0.6, 0.7, 0.8, 0.85, 0.9, 0.99, 1.0]\n",
        "# loop over all thresholds\n",
        "for thresh in thresholds:\n",
        "  # calculate predictions for a given threshold\n",
        "  temp_pred = [1 if x >= thresh else 0 for x in y_pred]\n",
        "  # calculate tpr\n",
        "  temp_tpr = tpr(y_true, temp_pred)\n",
        "  # calculate fpr\n",
        "  temp_fpr = fpr(y_true, temp_pred)\n",
        "  # append tpr and fpr to lists\n",
        "  tpr_list.append(temp_tpr)\n",
        "  fpr_list.append(temp_fpr)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf8S1Lq-ji6y",
        "outputId": "373f8085-a117-4d8b-f006-ca832f10c2cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "source": [
        "plt.figure(figsize=(7, 7))\n",
        "plt.fill_between(fpr_list, tpr_list, alpha=0.4)\n",
        "plt.plot(fpr_list, tpr_list, lw=3)\n",
        "plt.xlim(0, 1.0)\n",
        "plt.ylim(0, 1.0)\n",
        "plt.xlabel('FPR', fontsize=15)\n",
        "plt.ylabel('TPR', fontsize=15)\n",
        "plt.show()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIJWpqGskFq9"
      },
      "source": [
        "#This curve is also known as the Receiver Operating Characteristic (ROC). And if we calculate the area under this ROC curve, we are calculating another metric which is used very often when you have a dataset which has skewed binary targets.\n",
        "#This metric is known as the Area Under ROC Curve or Area Under Curve or just simply AUC. There are many ways to calculate the area under the ROC curve.\n",
        "#or this particular purpose, we will stick to the fantastic implementation by scikitlearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGdfuo3kjyUQ",
        "outputId": "8fcaa193-d7d3-4c75-f9f4-317ae4a0239e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn import metrics\n",
        "metrics.roc_auc_score(y_true, y_pred)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8300000000000001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXKGmNJxpK_0"
      },
      "source": [
        "#But what does AUC say about our model?\n",
        "##Now you get an AUC of 0.83 when you build a model to detect pneumothorax from chest x-ray images. This means that if you select a random image from your dataset with pneumothorax (positive sample) and another random image without pneumothorax (negative sample), then the pneumothorax image will rank higher than a non-pneumothorax image with a probability of 0.83."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCHN_hW7p0dh"
      },
      "source": [
        "#AUC is a widely used metric for skewed binary classification tasks in the industry,and a metric everyone should know about. Once you understand the idea behind AUC, as explained in the paragraphs above, it is also easy to explain it to nontechnical people who would probably be assessing your models in the industry.\n",
        "#Another important metric you should learn after learning AUC is log loss. In case of a binary classification problem, we define log loss as:\n",
        "#Log Loss = - 1.0 * ( target * log(prediction) + (1 - target) * log(1 - prediction) )\n",
        "#Where target is either 0 or 1 and prediction is a probability of a sample belongingto class 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8Vxc9llquL3"
      },
      "source": [
        "#For multiple samples in the dataset, the log-loss over all samples is a mere average of all individual log losses. \n",
        "#One thing to remember is that log loss penalizes quite high for an incorrect or a far-off prediction, i.e. log loss punishes you for being very sure and very wrong."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SS7_YhT0keMX"
      },
      "source": [
        "import numpy as np\n",
        "def log_loss(y_true, y_proba):\n",
        "  \"\"\"\n",
        "  Function to calculate log_loss\n",
        "  :param y_true: list of true values\n",
        "  :param y_proba: list of probabilities for 1\n",
        "  :return: overall log loss\n",
        "  \"\"\"\n",
        "  # define an epsilon value\n",
        "  # this can also be an input\n",
        "  # this value is used to clip probabilities\n",
        "  epsilon = 1e-15\n",
        "  # initialize empty list to store\n",
        "  # individual losses\n",
        "  loss = []\n",
        "  # loop over all true and predicted probability values\n",
        "  for yt, yp in zip(y_true, y_proba):\n",
        "    # adjust probability\n",
        "    # 0 gets converted to 1e-15\n",
        "    # 1 gets converted to 1-1e-15\n",
        "    # Why? Think about it!\n",
        "    yp = np.clip(yp, epsilon, 1 - epsilon)\n",
        "    # calculate loss for one sample\n",
        "    temp_loss = - 1.0 * (\n",
        "    yt * np.log(yp)\n",
        "    + (1 - yt) * np.log(1 - yp)\n",
        "    )\n",
        "    # add to loss list\n",
        "    loss.append(temp_loss)\n",
        "    # return mean loss over all samples\n",
        "  return np.mean(loss)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE4PHOYbryRU"
      },
      "source": [
        "#Let’s test our implementation and compare with scikit-learn:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSi5P6KZr4Bj",
        "outputId": "e16e6d17-008d-4b42-9f12-aa093916f3d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_true = [0, 0, 0, 0, 1, 0, 1,0, 0, 1, 0, 1, 0, 0, 1]\n",
        "y_proba = [0.1, 0.3, 0.2, 0.6, 0.8, 0.05,0.9, 0.5, 0.3, 0.66, 0.3, 0.2,0.85, 0.15, 0.99]\n",
        "log_loss(y_true, y_proba),metrics.log_loss(y_true, y_proba)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.49882711861432294, 0.49882711861432294)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXfcuf6lsXAE"
      },
      "source": [
        "#Implementation of log loss is easy.\n",
        "#Interpretation may seem a bit difficult. You must remember that log loss penalizes a lot more than other metrics.\n",
        "#For example, if you are 51% sure about a sample belonging to class 1, log losswould be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNZRgmVXselE",
        "outputId": "9f13c951-5926-4f69-9f49-959d4ec79d2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "-1.0 *(1 * np.log(0.51) + (1-1) * np.log(1 -0.51))  #= 0.67"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6733445532637656"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqB3Q0bttZCs"
      },
      "source": [
        "#And if you are 49% sure for a sample belonging to class 0, log loss would be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRlE1HyItcSw",
        "outputId": "ef71beba-24cc-49be-93e2-cb0db133f6d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "-1.0 *(0 * np.log(0.49) + (1-0) * np.log(1 -0.49))  #= 0.67"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6733445532637656"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yJsRS1uugvJ"
      },
      "source": [
        "#So, even though we can choose a cut off at 0.5 and get perfect predictions, we willstill have a very high log loss. So, when dealing with log loss, you need to be very careful; any non-confident prediction will have a very high log loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuS9V6YjoHEg"
      },
      "source": [
        "#Most of the time, the top-left value on ROC curve should give you a quite good threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQr1b-7tvt7-"
      },
      "source": [
        "#This seems complicated but is easy to understand by python implementations. Let’ssee how macro-averaged precision is implemented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMKYNuaWomzN"
      },
      "source": [
        "import numpy as np\n",
        "def macro_precision(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Function to calculate macro averaged precision\n",
        "  :param y_true: list of true values\n",
        "  :param y_proba: list of predicted values\n",
        "  :return: macro precision score\n",
        "  \"\"\"\n",
        "  # find the number of classes by taking\n",
        "  # length of unique values in true list\n",
        "  num_classes = len(np.unique(y_true))\n",
        "  # initialize precision to 0\n",
        "  precision = 0\n",
        "  # loop over all classes\n",
        "  for class_ in range(num_classes):\n",
        "    # all classes except current are considered negative\n",
        "    temp_true = [1 if p == class_ else 0 for p in y_true]\n",
        "    temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
        "    # calculate true positive for current class\n",
        "    tp = true_positive(temp_true, temp_pred)\n",
        "    # calculate false positive for current class\n",
        "    fp = false_positive(temp_true, temp_pred)\n",
        "    # calculate precision for current class\n",
        "    temp_precision = tp / (tp + fp)\n",
        "    # keep adding precision for all classes\n",
        "    precision += temp_precision\n",
        "  # calculate and return average precision over all classes\n",
        "  precision /= num_classes\n",
        "  return precision"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6iIp-gWwizz"
      },
      "source": [
        "#You will notice that it wasn’t so difficult. Similarly, we have micro-averaged precision score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEneI070weWH"
      },
      "source": [
        "def micro_precision(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Function to calculate micro averaged precision\n",
        "  :param y_true: list of true values\n",
        "  :param y_proba: list of predicted values\n",
        "  :return: micro precision score\n",
        "  \"\"\"\n",
        "  # find the number of classes by taking\n",
        "  # length of unique values in true list\n",
        "  num_classes = len(np.unique(y_true))\n",
        "  # initialize tp and fp to 0\n",
        "  tp = 0\n",
        "  fp = 0\n",
        "  # loop over all classes\n",
        "  for class_ in range(num_classes):\n",
        "    # all classes except current are considered negative\n",
        "    temp_true = [1 if p == class_ else 0 for p in y_true]\n",
        "    temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
        "    # calculate true positive for current class\n",
        "    # and update overall tp\n",
        "    tp += true_positive(temp_true, temp_pred)\n",
        "    # calculate false positive for current class\n",
        "    # and update overall tp\n",
        "    fp += false_positive(temp_true, temp_pred)\n",
        "  # calculate and return overall precision\n",
        "  precision = tp / (tp + fp)\n",
        "  return precision"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUIGci02yKQL"
      },
      "source": [
        "#This isn’t difficult, either. Then what is? Nothing. Machine learning is easy.\n",
        "#Now, let’s look at the implementation of weighted precision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CN4vlQbTyDQH"
      },
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "def weighted_precision(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Function to calculate weighted averaged precision\n",
        "  :param y_true: list of true values\n",
        "  :param y_proba: list of predicted values\n",
        "  :return: weighted precision score\n",
        "  \"\"\"\n",
        "  # find the number of classes by taking\n",
        "  # length of unique values in true list\n",
        "  num_classes = len(np.unique(y_true))\n",
        "  # create class:sample count dictionary\n",
        "  # it looks something like this:\n",
        "  # {0: 20, 1:15, 2:21}\n",
        "  class_counts = Counter(y_true)\n",
        "  # initialize precision to 0\n",
        "  precision = 0\n",
        "  # loop over all classes\n",
        "  for class_ in range(num_classes):\n",
        "    # all classes except current are considered negative\n",
        "    temp_true = [1 if p == class_ else 0 for p in y_true]\n",
        "    temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
        "    # calculate tp and fp for class\n",
        "    tp = true_positive(temp_true, temp_pred)\n",
        "    fp = false_positive(temp_true, temp_pred)\n",
        "    # calculate precision of class\n",
        "    temp_precision = tp / (tp + fp)\n",
        "    # multiply precision with count of samples in class\n",
        "    weighted_precision = class_counts[class_] * temp_precision\n",
        "    # add to overall precision\n",
        "    precision += weighted_precision\n",
        "  # calculate overall precision by dividing by\n",
        "  # total number of samples\n",
        "  overall_precision = precision / len(y_true)\n",
        "  return overall_precision"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C19oRg_tyJn8"
      },
      "source": [
        "#Let’s compare our implementations with scikit-learn to know if we implemented it right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc1OiIHpzDum"
      },
      "source": [
        "from sklearn import metrics\n",
        "y_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n",
        "y_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUEf-244zRBL",
        "outputId": "75563f21-6e94-4ca0-e528-7bd59d57755b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "macro_precision(y_true, y_pred),metrics.precision_score(y_true, y_pred, average=\"macro\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.3611111111111111, 0.3611111111111111)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgnNCSlMzUsr",
        "outputId": "6d21c323-5d93-4335-893d-a3e3ec617e06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "micro_precision(y_true, y_pred),metrics.precision_score(y_true, y_pred, average=\"micro\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.4444444444444444, 0.4444444444444444)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5H-2J_DZzZA7",
        "outputId": "3003dad2-236e-4a88-a6e9-fc0e16d0096e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "weighted_precision(y_true, y_pred),metrics.precision_score(y_true, y_pred, average=\"weighted\")"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.39814814814814814, 0.39814814814814814)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIHiD8VNzcrZ",
        "outputId": "8253bb9c-4a14-49fa-d76e-c61e0ea15771",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "class_counts = Counter(y_true)\n",
        "class_counts"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({0: 3, 1: 2, 2: 4})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOT-o709zm3g"
      },
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "def weighted_f1(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Function to calculate weighted f1 score\n",
        "  :param y_true: list of true values\n",
        "  :param y_proba: list of predicted values\n",
        "  :return: weighted f1 score\n",
        "  \"\"\"\n",
        "  # find the number of classes by taking\n",
        "  # length of unique values in true list\n",
        "  num_classes = len(np.unique(y_true))\n",
        "  # create class:sample count dictionary\n",
        "  # it looks something like this:\n",
        "  # {0: 20, 1:15, 2:21}\n",
        "  class_counts = Counter(y_true)\n",
        "  # initialize f1 to 0\n",
        "  f1 = 0\n",
        "  # loop over all classes\n",
        "  for class_ in range(num_classes):\n",
        "    # all classes except current are considered negative\n",
        "    temp_true = [1 if p == class_ else 0 for p in y_true]\n",
        "    temp_pred = [1 if p == class_ else 0 for p in y_pred]\n",
        "    # calculate precision and recall for class\n",
        "    p = precision(temp_true, temp_pred)\n",
        "    r = recall(temp_true, temp_pred)\n",
        "    # calculate f1 of class\n",
        "    if p + r != 0:\n",
        "      temp_f1 = 2 * p * r / (p + r)\n",
        "    else:\n",
        "      temp_f1 = 0\n",
        "    # multiply f1 with count of samples in class\n",
        "    weighted_f1 = class_counts[class_] * temp_f1\n",
        "    # add to f1 precision\n",
        "    f1 += weighted_f1\n",
        "  # calculate overall F1 by dividing by\n",
        "  # total number of samples\n",
        "  overall_f1 = f1 / len(y_true)\n",
        "  return overall_f1"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1jasCUp1B4g",
        "outputId": "901e80d0-d83a-4dca-ed0e-f3f10edd5984",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "weighted_f1(y_true, y_pred),metrics.f1_score(y_true, y_pred, average=\"weighted\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.41269841269841273, 0.41269841269841273)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GgtQQ5w01lgu"
      },
      "source": [
        "def f1(y_true, y_pred):\n",
        "  \"\"\"\n",
        "  Function to calculate f1 score\n",
        "  :param y_true: list of true values\n",
        "  :param y_pred: list of predicted values\n",
        "  :return: f1 score\n",
        "  \"\"\"\n",
        "  p = precision(y_true, y_pred)\n",
        "  r = recall(y_true, y_pred)\n",
        "  if p + r != 0:\n",
        "    score = 2 * p * r / (p + r)\n",
        "  else:\n",
        "    score = 0\n",
        "  return score"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JL0hN-qU11d2",
        "outputId": "de57d414-48c2-4d76-b9c5-2222061af256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "f1(y_true, y_pred)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-ff40971e720d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-64-b9b299598654>\u001b[0m in \u001b[0;36mf1\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf1\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-1009491ea755>\u001b[0m in \u001b[0;36mprecision\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_positive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfalse_positive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLOTLdL71OJ0",
        "outputId": "9709a87c-4bf5-4f9f-94c9-ba840ae8762a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        }
      },
      "source": [
        "f1(y_true, y_pred),metrics.f1_score(y_true, y_pred)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-17f8977ff759>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-64-b9b299598654>\u001b[0m in \u001b[0;36mf1\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf1\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \"\"\"\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m   \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-1009491ea755>\u001b[0m in \u001b[0;36mprecision\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mtp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrue_positive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfalse_positive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-zBP0EL1ZdA",
        "outputId": "8944e9b4-4ad6-40b4-87f8-38c03a148824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import metrics\n",
        "# some targets\n",
        "y_true = [0, 1, 2, 0, 1, 2, 0, 2, 2]\n",
        "#some predictions\n",
        "y_pred = [0, 2, 1, 0, 2, 1, 0, 0, 2]\n",
        "# get confusion matrix from sklearn\n",
        "cm = metrics.confusion_matrix(y_true, y_pred)\n",
        "# plot using matplotlib and seaborn\n",
        "plt.figure(figsize=(10, 10))\n",
        "cmap = sns.cubehelix_palette(50, hue=0.05, rot=0, light=0.9, dark=0,\n",
        "as_cmap=True)\n",
        "sns.set(font_scale=2.5)\n",
        "sns.heatmap(cm, annot=True, cmap=cmap, cbar=False)\n",
        "plt.ylabel('Actual Labels', fontsize=20)\n",
        "plt.xlabel('Predicted Labels', fontsize=20)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 48.5, 'Predicted Labels')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfGDAOfodLEv"
      },
      "source": [
        "#Writing good, understandable code is an essential quality one can have, and many data scientists ignore it. \n",
        "#If you work on a project that others can understand and use without consulting you, you save their time and your own time and can invest that time to improve your project or work on a new one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJTiIibz27-s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}